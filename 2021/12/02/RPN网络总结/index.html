<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>RPN网络总结 | twn29004&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="常见的RPN网络 Faster-RCNN中的RPN网络  在Backbone生成的特征图中，使用大小为\(3\times 3\)的卷积处理特征图，针对每一个中心点生成一个256维的向量。特征图可以理解为原图的一种缩小版。 使用\(3\times3\)的卷积处理特征图可以对应到原图中的一个区域。然后RPN网络的目的就是在原图中的各个区域放置anchors。然后根据提取的特征判断这些anchor">
<meta property="og:type" content="article">
<meta property="og:title" content="RPN网络总结">
<meta property="og:url" content="http://twn29004.top/2021/12/02/RPN%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="twn29004&#39;s Blog">
<meta property="og:description" content="常见的RPN网络 Faster-RCNN中的RPN网络  在Backbone生成的特征图中，使用大小为\(3\times 3\)的卷积处理特征图，针对每一个中心点生成一个256维的向量。特征图可以理解为原图的一种缩小版。 使用\(3\times3\)的卷积处理特征图可以对应到原图中的一个区域。然后RPN网络的目的就是在原图中的各个区域放置anchors。然后根据提取的特征判断这些anchor">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202111281938250.png">
<meta property="og:image" content="https://pic1.zhimg.com/v2-e3108dc5cdd76b871e21a4cb64001b5c_r.jpg">
<meta property="og:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021017505.png">
<meta property="og:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202112020959428.png">
<meta property="og:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021029478.png">
<meta property="og:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021000221.png">
<meta property="og:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021050611.png">
<meta property="og:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021046903.png">
<meta property="article:published_time" content="2021-12-01T16:00:00.000Z">
<meta property="article:modified_time" content="2021-12-02T13:24:46.940Z">
<meta property="article:author" content="twn29004">
<meta property="article:tag" content="3D目标检测">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/twn29004/myimg/raw/master/imgs/202111281938250.png">
  
    <link rel="alternate" href="/atom.xml" title="twn29004&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">twn29004&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://twn29004.top"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-RPN网络总结" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/12/02/RPN%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/" class="article-date">
  <time datetime="2021-12-01T16:00:00.000Z" itemprop="datePublished">2021-12-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      RPN网络总结
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="常见的rpn网络">常见的RPN网络</h1>
<h2 id="faster-rcnn中的rpn网络">Faster-RCNN中的RPN网络</h2>
<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111281938250.png" alt="image-20211128193818170" /></p>
<p>在Backbone生成的特征图中，使用大小为<span class="math inline">\(3\times 3\)</span>的卷积处理特征图，针对每一个中心点生成一个256维的向量。特征图可以理解为原图的一种缩小版。 使用<span class="math inline">\(3\times3\)</span>的卷积处理特征图可以对应到原图中的一个区域。然后RPN网络的目的就是在原图中的各个区域放置anchors。然后根据提取的特征判断这些anchor是否合理以及之后对这些anchor进行调整。</p>
<p>针对特征图中的每一个中心点，使用<span class="math inline">\(3\times3\)</span>的卷积来提取特征图的特征，生成一个256-D的向量。这个256-D的向量可以理解为对应于原始图像中的某一个区域。然后使用一些全连接层来判断这些anchor是背景还是前景，以及这些anchor距离目标中心点的偏移。</p>
<p>上述步骤处理完成之后，我们就在提取的特征图上的每一个点都获得了一组anchor,及其是否是前景点还是背景点，以及其相对Ground truth的偏移。然后我们需要proposal layer网络来生成proposal了。其基本步骤如下：</p>
<ol type="1">
<li>生成anchors. 根据前面的RPN网络计算出的偏移量和原始的anchors,生成最终anchors所在的位置</li>
<li>根据前面的对于anchor是前景点还是背景点的判断的输出，根据置信度排序，选取前N个作为关注的anchor.</li>
<li>对于超出图像边界的anchors进行处理</li>
<li>剔除尺寸较小的anchors</li>
<li>对剩余的positive anchor进行NMS</li>
<li>剩余的区域输出作为下个阶段的输入</li>
</ol>
<p>上述proposal生成了一系列大小不同的anchor。这些anchor都是对应原图的不同大小的区域。由于神经网络只能处理固定大小的输入，因此，在将不同大小的anchor映射回backbone生成的特征图之后，将所对应区域的不同大小的特征图划分为相同大小的网格，然后对这些网格进行pooling操作。这样就使得网络能够有固定大小的输入了。</p>
<p><img src="https://pic1.zhimg.com/v2-e3108dc5cdd76b871e21a4cb64001b5c_r.jpg" alt="preview" /></p>
<h2 id="second网络中的rpn网络">SECOND网络中的RPN网络</h2>
<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021017505.png" alt="Second-RPN" /></p>
<p>SECOND的RPN网络与Faster-RCNN中的类似，不同的是，SECOND中并没有第二阶段，这个RPN网络是用于从backbone提取的特征图中生成边界框的。</p>
<p>由于三维的卷积神经网络太过耗时，所以作者在使用稀疏卷积神经网络提取三维点云的特征之后，将其压缩到了俯视图的特征图上，然后再特征图的各个位置计算该区域所属的类别以及回归分支计算的值。</p>
<p>神经网络总是要找到一个目标去优化的，但是就目前来说，我们还不知道我们优化的目标是什么，所以我们需要找到一个基准值去优化。所以需要根据anchor找到一个基准值作为优化的目标。这里采用的是以IoU为基准来进行判断。也就是说当我的anchor与基准值的IoU大于某个阈值的时候,我就需要关注这个anchor预测的输出。小于某个阈值的时候，我们也需要关注其预测的输出，我们将其分类背景，那么我们在训练的时候也希望他是朝着背景的方向去优化的。</p>
<p>这个就是OpenPCDet中的TargetAssign. 将anchor与gt绑定之后，我们还需要gt的其他信息来帮助我们优化我们的目标。</p>
<h2 id="pointrcnn网络中的rpn网络">PointRCNN网络中的RPN网络</h2>
<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112020959428.png" alt="image-20211202095917332" /></p>
<p><strong>该方法生成的区域提议达到了很高的recall。</strong>其实原因很好理解，根据每一个前景点生成，只要能够找到目标的前景点，大概率也能计算出其对应的三维边界框。使用点云分割网络对采样的点云进行分割。然后根据分割出的前景点生成边界框。文中的意思的是针对每一个前景点都预测一个三维边界框。这样的话就会产生大量的重复的三维边界框，因此作者使用了NMS去除边界框。作者选择在俯视图上IoU阈值为0.85,来选择300个高质量的porposal来进行第二阶段的refine。选择在俯视图上的原因是二维IoU的计算要简单快速的多，此外，因为重复的很多，所以作者选择了一个很高的阈值来减少proposal的数量。</p>
<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021029478.png" alt="image-20211202102931430" /></p>
<p>从上图中可以看出，在IoU为0.5时,其recall能达到98.21。这已经达到了相当高的水准。但是当IoU为0.7时，其下降到了82.29，这也在一定程度上反映了针对其实各个前景点生成的三维边界框的质量并不是很高。分析其原因是因为不同的前景点通常位于目标不同的位置，根据其预测回归三维边界框可能比较困难。</p>
<p>上述方法存在的弊端就一方面需要消耗大量的资源来计算三维边界框。该网络是一个二阶段的网络，但是作者并没有同时训练这两个网络，分析其原因是因为计算资源消耗太大了。一方面是分割网络的资源消耗，另一方面的proposal生成的消耗。</p>
<h2 id="votenet网络中的rpn网络">VoteNet网络中的RPN网络</h2>
<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021000221.png" alt="VoteNet-RPN" /></p>
<p>VoteNet的基本流程就是首先在原始场景中进行采样，提取特征。然后根据提取的特征对目标的中心点进行投票。这里投票的意思就是根据原始点云的位置和特征，估算目标中心点的位置，计算该点云到中心点的偏移。原始点云加上这个偏移就可以向目标中心点靠拢。然后在根据投票结果做一个聚类。文中描述的聚类的话<strong>是在Votes中进行FPS采样</strong>，采样<span class="math inline">\(K\)</span>个点，然后将这<span class="math inline">\(K\)</span>个点周围的其他点聚合到一个集合中就形成了一个聚类。因为投票之后目标的点靠的更近了。聚类后的结果即为网络的proposal。然后根据这些proposal生成三维边界框。</p>
<p>VoteNet网络的RPN感觉比PointRCNN的要好一些，同样是根据点来生成proposal。VoteNet中借用了投票和聚类的方法来生成proposal。此外，再该方法中，不仅利用了前景点云，还在一定程度上利用了周围的背景点云。可能还在一定程度上减少了计算开销。</p>
<h2 id="dssd网络中的rpn网络">3DSSD网络中的RPN网络</h2>
<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021050611.png" alt="3DSSD-RPN" /></p>
<p>3DSSD中的RPN网络部分和VoteNet中的基本一致。不同的是3DSSD中引入了一种混合采样的方法。传统的FPS采样是在欧几里得空间进行的。作者在文中分析了这个方法的弊端。因为点云场景中大部分的点云都是背景点云。FPS采样虽然可以让采样的点云均匀的近似均匀的分布在整个点云空间，但是其采样到的点云大部分都是背景点云。这不利于我们的特征提取。因此作者引入了一种混合采样的方式，就是既在欧几里得空间应用FPS采样，又在语义特征空间应用FPS采样。然后将两种采样的距离结合起来。</p>
<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021046903.png" alt="image-20211202104607856" /></p>
<p>上表中，D-FPS表示的是欧几里得空间的FPS,F-FPS表示的是语义特征空间的FPS。从上表中可以看出，引入F-FPS确实能在一定程度上提高算法采样到前景点的比例。</p>
<p>此外，3DSSD采用的是anchor-free的方法生成三维框，其根据每一个候选人点生成一个三维框，</p>
<p>一个疑问<strong>高维空间中的距离度量将会失效，那再高维的语义特征空间中，FPS为什么还会有效呢？？？,难道是这里的语义特征的维度很小，这里需要到代码中求证</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/t949500898/article/details/107433419">高维空间欧氏距离与余弦相似度失效</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://twn29004.top/2021/12/02/RPN%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/" data-id="cl2d63r8o0015msxchvcsbkhy" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">3D目标检测</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/12/07/Pointformer/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          3D ObJect Detection with Pointformer
        
      </div>
    </a>
  
  
    <a href="/2021/11/29/CT3D%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">CT3D论文</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/">参考链接</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/">知识分享</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/2D-Object-Detection/" rel="tag">2D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2D-Object-Detection/" rel="tag">2D Object-Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Object-Detect-Graph-Network/" rel="tag">3D Object Detect, Graph Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Object-Detection/" rel="tag">3D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">3D目标检测</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context-information-self-Attenion/" rel="tag">Context information, self-Attenion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fusion/" rel="tag">Fusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Instance-Segment/" rel="tag">Instance Segment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Object-Detection/" rel="tag">Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Object-Detection-2D-Object-Detection/" rel="tag">Object Detection, 2D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Segment/" rel="tag">Segment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E5%B7%A7/" rel="tag">技巧</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2/" rel="tag">点云分割</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/2D-Object-Detection/" style="font-size: 15px;">2D Object Detection</a> <a href="/tags/2D-Object-Detection/" style="font-size: 10px;">2D Object-Detection</a> <a href="/tags/3D-Object-Detect-Graph-Network/" style="font-size: 10px;">3D Object Detect, Graph Network</a> <a href="/tags/3D-Object-Detection/" style="font-size: 20px;">3D Object Detection</a> <a href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">3D目标检测</a> <a href="/tags/Context-information-self-Attenion/" style="font-size: 10px;">Context information, self-Attenion</a> <a href="/tags/Fusion/" style="font-size: 10px;">Fusion</a> <a href="/tags/Instance-Segment/" style="font-size: 10px;">Instance Segment</a> <a href="/tags/Object-Detection/" style="font-size: 20px;">Object Detection</a> <a href="/tags/Object-Detection-2D-Object-Detection/" style="font-size: 10px;">Object Detection, 2D Object Detection</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/Segment/" style="font-size: 10px;">Segment</a> <a href="/tags/%E6%8A%80%E5%B7%A7/" style="font-size: 10px;">技巧</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2/" style="font-size: 10px;">点云分割</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/02/27/mmvirtualpioint/">Multimodal Virtual Point 3D Detection论文解读</a>
          </li>
        
          <li>
            <a href="/2022/02/26/2-17%E8%B0%83%E7%A0%94/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/01/14/Faster-RCNN/">Faster-RCNN论文解读</a>
          </li>
        
          <li>
            <a href="/2022/01/13/3D_IoU_Net/">3D IoU-Net论文解读</a>
          </li>
        
          <li>
            <a href="/2022/01/12/YOLOF/">YOLOF论文解读</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 twn29004<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>
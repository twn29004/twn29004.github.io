<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Relation Network for Object Network解读 | twn29004&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="代码链接 paper链接 个人博客 问题 本文首先提出了一个目前在目标检测领域的一个共识就是: 丰富的上下文信息和目标之间的关系能够更好的帮助网络去检测目标。但是现有的网络都没有很好的利用这个信息，或者说现有的卷积神经网络都不能很好的处理这些问题。原因是一个场景中往往有很多类别的目标，且类别的数目是不一定的，而且目标的数目也是不一定的。因此，按照之前的CNN网络很难对这些目标之间的关系">
<meta property="og:type" content="article">
<meta property="og:title" content="Relation Network for Object Network解读">
<meta property="og:url" content="http://twn29004.top/2022/01/07/RelationNetworkforObjectDetection/index.html">
<meta property="og:site_name" content="twn29004&#39;s Blog">
<meta property="og:description" content="代码链接 paper链接 个人博客 问题 本文首先提出了一个目前在目标检测领域的一个共识就是: 丰富的上下文信息和目标之间的关系能够更好的帮助网络去检测目标。但是现有的网络都没有很好的利用这个信息，或者说现有的卷积神经网络都不能很好的处理这些问题。原因是一个场景中往往有很多类别的目标，且类别的数目是不一定的，而且目标的数目也是不一定的。因此，按照之前的CNN网络很难对这些目标之间的关系">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071647839.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071654018.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071656601.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071914128.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071915082.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071916354.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071927059.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071941581.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071946722.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071948077.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201072004552.png">
<meta property="og:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201072017793.png">
<meta property="article:published_time" content="2022-01-06T16:00:00.000Z">
<meta property="article:modified_time" content="2022-01-08T08:44:36.523Z">
<meta property="article:author" content="twn29004">
<meta property="article:tag" content="2D Object Detection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071647839.png">
  
    <link rel="alternate" href="/atom.xml" title="twn29004&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">twn29004&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://twn29004.top"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-RelationNetworkforObjectDetection" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/07/RelationNetworkforObjectDetection/" class="article-date">
  <time datetime="2022-01-06T16:00:00.000Z" itemprop="datePublished">2022-01-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Relation Network for Object Network解读
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071647839.png" alt="image-20220107164710361" /></p>
<p><a target="_blank" rel="noopener" href="https://github.com/msracver/Relation-Networks-for-Object-Detection">代码链接</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.11575">paper链接</a></p>
<p><a href="https://twn29004.top/">个人博客</a></p>
<h2 id="问题">问题</h2>
<p>本文首先提出了一个目前在目标检测领域的一个共识就是: 丰富的上下文信息和目标之间的关系能够更好的帮助网络去检测目标。但是现有的网络都没有很好的利用这个信息，或者说现有的卷积神经网络都不能很好的处理这些问题。原因是一个场景中往往有很多类别的目标，且类别的数目是不一定的，而且目标的数目也是不一定的。因此，按照之前的CNN网络很难对这些目标之间的关系进行建模。作者受到在NLP领域中非常成功的Transformer的影响，提出了一种自适应的注意力网络用于目标检测领域。作者把这个模块称为目标关系模块。作者将这个关系模块应用于两个任务，一个是<span class="math inline">\(instance-recognition\)</span>和<span class="math inline">\(Duplicate-remova\)</span>阶段。其中前一个阶段就是在提取proposal的特征之后，在对RoI进行调整和rescore之前。后者应用于消除重复的预测阶段，是取代了NMS，因为NMS不是一个端到端的结构。具体应用阶段如下图所示:</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071654018.png" alt="image-20220107165406903" /></p>
<h2 id="解决方法">解决方法</h2>
<h3 id="relation-module的结构">Relation Module的结构</h3>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071656601.png" alt="image-20220107165648497" /></p>
<p>Relation Module的结构如上图所示。下面分别介绍<span class="math inline">\(W_V,W_Q,W_V, \varepsilon_G\)</span>的计算方法，以及<span class="math inline">\(f_A^n,f_G^m,f_G^n\)</span>的具体含义。</p>
<p>在开始介绍本文所提的Realtion Module之前，作者首先回顾了self-attention的计算方法。<br />
<span class="math display">\[
v^{out}=softmax(\frac{qK^t}{\sqrt{d_k}})V
\]</span><br />
首先介绍<span class="math inline">\(f_A^n\)</span>，其表示对是<span class="math inline">\(n^{th}\)</span>这个RoI的外观特征(Apperance Feature)。<span class="math inline">\(F_G^n\)</span>表示的是<span class="math inline">\(n^{th}\)</span>这个RoI的集合特征，其实就是表示简单的四维边界框。这样，对于一个大小为<span class="math inline">\(N\)</span>的目标集合，其特征可以表示为{(<span class="math inline">\(f^n_A, f^n_G\)</span>)}。第<span class="math inline">\(n^{th}\)</span>个目标关于其他所有目标的关系的计算方法如下:<br />
<span class="math display">\[
f_{R}(n)=\sum_{m}w^{mn}\cdot (W_V \cdot f_A^m)
\]</span><br />
这里的<span class="math inline">\(w^{mn}\)</span>的作用和self-attention中的前半部分基本类似。但是还是存在一些区别。其具体做法如下:<br />
<span class="math display">\[
w^{mn} = \frac{w_G^{mn}\cdot w_{A}^{mn}}{\sum_{k}w_G^{kn}\cdot exp(w_A^{kn})}
\]</span></p>
<p><span class="math display">\[
w_A^{mn} = \frac{dot(W_Kf_A^m, W_Qf_A^n)}{\sqrt{d_K}}
\]</span><br />
上述式子的作用是将<span class="math inline">\(f_A^m\)</span>和<span class="math inline">\(f_A^n\)</span>投影到相同的子空间，然后计算其相关性。其中子空间的维度为<span class="math inline">\(d_K\)</span></p>
<p><span class="math display">\[
w_G^{mn}=max(0， W_G \cdot \varepsilon_G(f_G^m, f_G^n))
\]</span><br />
<span class="math inline">\(\varepsilon_G\)</span>表示的是将<span class="math inline">\((f_G^m，f_G^n)\)</span>嵌入到一个高维空间中，同时为了保证一些旋转不变性和尺度不变性，作者并没有采用原始的描述bbox的参数，而是做了一下变化:<br />
<span class="math display">\[
(log(\frac{|x_m-x_n|}{w_m}), log(\frac{|y_m - y_n|}{h_m}),log(\frac{w_n}{w_m}), log(\frac{h_n}{h_m}))^T
\]</span><br />
这个几何特征嵌入的空间的维度为<span class="math inline">\(d_g\)</span>。其实这里可以将<span class="math inline">\(w_G^{mn}\)</span>理解为公式(3)中的<span class="math inline">\(w_A^{mn}\)</span>的权重。就是在attention的过程中，除了要考虑语义特征之外，对于那些在几何特征上关联性不大的proposal，我们也是不考虑的。换句话说就是在式子(2)中，作者同时考虑了proposal之间的几何和语义关联性。</p>
<p>此外，作者还引入了类似于multi-head的attention设计。其中head的数目用<span class="math inline">\(N_r\)</span>表示。然后将这些head的输出进行拼接之后与原始的语义特征进行拼接。即<br />
<span class="math display">\[
f^n_A=f_A^n+Concat(f_R^1(n),...,f_R^{N_r}(n))
\]</span><br />
上述结构就是作者设计的relation module的主要结构。</p>
<p>正文开头叙述的，作者在目标检测中的两个阶段使用了realtion module.下面将介绍其在不同部分的使用。</p>
<h3 id="rm在instance-recognition中的使用">RM在Instance Recognition中的使用</h3>
<p>原始的Instance Recognition的使用的以下结构:</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071914128.png" alt="image-20220107191420021" /></p>
<p>作者对此提出了一个增强版的检测头，其在两个FC层之间增加了Realtion Module子网络。其结构如下:</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071915082.png" alt="image-20220107191545020" /></p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071916354.png" alt="image-20220107191602270" /></p>
<h3 id="rm在duplicate-removal中的应用">RM在Duplicate Removal中的应用</h3>
<p>在本节中，作者首先分析了NMS的缺陷。NMS虽然简单，但是由于其采用的是贪心的策略，因此其可能得到的是次优解。此外，目标之间的关系还能对于去除重复有一定的帮助。对此，作者尝试抛弃NMS来做重复预测的去除。也就是NMS-Free的方法。作者将这个重复去除看作一个二分类问题，对于每一个基准框，只有一个被检测的目标能够与之匹配。这样的话可以将这个重复去除问题看做事一个二分类问题。与基准框绑定的预测框为correct，其余的为duplicate的预测。这个预测是能够通过网络来进行的。这个网络的输入就是各个预测的目标。每个目标有一个1-24-D的语义特征，分类分数<span class="math inline">\(s_0\)</span>和预测得三维边界框。网络的得输出事一个二值分类分数<span class="math inline">\(s_1 \in [0,1]\)</span>，1表示的是correct，0表示得duplicate。最终边界框得分数为<span class="math inline">\(s_0 \cdot s_1\)</span>。也就是说，一个好的预测，<span class="math inline">\(s_0, s_1\)</span>都要大才行。这部分的子网络如下图所示:</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071927059.png" alt="image-20220107192723917" /></p>
<p>RM模块是这部分网络得核心模块。对于RM中得几何特征，作者首先将<span class="math inline">\(s_0\)</span>做了一个rank embed，就是使用排名来取代原始得<span class="math inline">\(s_0\)</span>，然后将其嵌入到高维空间，为重得维度为128-D,然后是这个RoI对应得1024-D语义特征也将其转化为128-D得特征，然后将这两个特征进行相加，作为RM模块中得语义特征，bbox作为几何特征得输入。然后将输入经过线性层和softMax之后作为<span class="math inline">\(s_1\)</span>输出，<span class="math inline">\(s_1\)</span>得含义其实就是这个RoI是否被选择与GT绑定。这样就得到了最终得预测输出。</p>
<p>但是这存在一个问题就是，这个输出得标签怎么来呢，由于预测得RoI每次都不同，且数据集中并没有关于RoI的label,这样的话网络是没办法训练的。针对这个问题，作者提出给定一个在基准框和预测的bbox之间的IoU阈值。对于匹配同一个基准狂且大于阈值的预测的bbox，选择分数最大的作为correct的bbox,其余的为duplicate的预测。这样的话就获得了bbox的标签，文中使用的是交叉熵损失来对部分网络进行训练。</p>
<p>文中还发现了不同的预测通常会获得不同的效果，当阈值为0.5时，<span class="math inline">\(mAP_{0.5}\)</span>获得最大，当阈值为0.75时，<span class="math inline">\(mAP_{0.75}\)</span>最大。因此，为了能够在COCO数据集中获得最好的性能，作者设计了多个阈值来获得一个比较均衡的性能。</p>
<h2 id="解决效果">解决效果</h2>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071941581.png" alt="image-20220107194151453" /></p>
<p>上图中，对于每个精度组合，第一个表示的时2fc head和softNMS组合得到的结果，第二个精度表示的是2fc+RM head+SoftNMS获得的结果，第三个结果表示的是2fc+RM head + e2e得到的结果。</p>
<p>从上述结果中可以看出，作者提出的RM在不同阶段均有很大的提升。</p>
<h2 id="消融实验">消融实验</h2>
<h3 id="在instance-recognition部分的消融实验">在Instance Recognition部分的消融实验</h3>
<p>为了验证所提方法的有效性，作者对进行了丰富的消融实验。对于Instance Recognition部分，作者对是否使用几何特征，以及mutli-head中的head的个数和relation-module的个数进行了探索。其结果如下:</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071946722.png" alt="image-20220107194624603" /></p>
<p>对于几何特征，none的意思是不使用几何特征,unary的意思是将几何特征转化到和语义特征一致的维度，相加，其他操作与none一致。</p>
<p>此外，作者还验证了上述提升并不是因为网络层数加深，参数增多带来的。</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201071948077.png" alt="image-20220107194808989" /></p>
<p>从上述结果中可以看出，使用更多参数的FC (a)VS(b)可以获得0.1mAP的提升。但是增加一层FC之后， (a)VS(c)之后，模型的性能有所下降，分析原因可能是因为层数增多之后模型有所下降。为了能够使得模型能够被更简单的训练，作者在FC层之间加入了残差层(a) VS (d)，加入残差层之后模型性能得到了0.3mAP的提升。此外，作者引入了一个2048-D的全局上下文向量。(a) VS (e)也获得了0.3mAP的提升。(a) VS (f)表示的是作者引入了RM模块对于检测性能的影响。此外，作者还观察到将RoI扩大两倍之后得到的聚合的特征与原始1024-D的语义特征拼接之后，模型的检测性能也得到了一定的提升。即(a) VS (g)。检测精度从29.6提升到了30.4。作者还在此基础上增加了RM模块。即(g) VS (h)。作者还进行了(i)和(j)实验发现增加更多的残差曾和RM层对于模型的性能并没有显著的影响。</p>
<h3 id="在duplicate-removal部分的消融实验">在Duplicate Removal部分的消融实验</h3>
<p>首先也是对于这部分的RM网络的输入特征的一些实验，实验结果如下图所示:</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201072004552.png" alt="image-20220107200459387" /></p>
<p>其中，rank <span class="math inline">\(f_R\)</span>中none表示的是不适用<span class="math inline">\(s_0\)</span>,<span class="math inline">\(s_0\)</span>表示的不进行rand embeding。几何bbox中none表示的是不适用几何box，unary表示的是含义与前文相似，将其融入到语义特征中。</p>
<p>此外，作者还将本文所提方法与NMS进行了比较.</p>
<p><img src="https://blog-image-twn29004.oss-cn-chengdu.aliyuncs.com/blog-img/202201072017793.png" alt="image-20220107201712700" /></p>
<p>从上表的结果中可以看出,本文所提方法所获得的性能在不同程度上超过了NMS和softNMS。其中e2e表示的同时训练前面提到的两个网络。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://twn29004.top/2022/01/07/RelationNetworkforObjectDetection/" data-id="cl2d63r8t001dmsxccjle3wf3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/2D-Object-Detection/" rel="tag">2D Object Detection</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/01/09/pytorchwithcuda/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Pytorch,CUDA联合编程
        
      </div>
    </a>
  
  
    <a href="/2022/01/02/PartA2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">PartA2论文解读</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/">参考链接</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/">知识分享</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/2D-Object-Detection/" rel="tag">2D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2D-Object-Detection/" rel="tag">2D Object-Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Object-Detect-Graph-Network/" rel="tag">3D Object Detect, Graph Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Object-Detection/" rel="tag">3D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">3D目标检测</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context-information-self-Attenion/" rel="tag">Context information, self-Attenion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fusion/" rel="tag">Fusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Instance-Segment/" rel="tag">Instance Segment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Object-Detection/" rel="tag">Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Object-Detection-2D-Object-Detection/" rel="tag">Object Detection, 2D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Segment/" rel="tag">Segment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E5%B7%A7/" rel="tag">技巧</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2/" rel="tag">点云分割</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/2D-Object-Detection/" style="font-size: 15px;">2D Object Detection</a> <a href="/tags/2D-Object-Detection/" style="font-size: 10px;">2D Object-Detection</a> <a href="/tags/3D-Object-Detect-Graph-Network/" style="font-size: 10px;">3D Object Detect, Graph Network</a> <a href="/tags/3D-Object-Detection/" style="font-size: 20px;">3D Object Detection</a> <a href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">3D目标检测</a> <a href="/tags/Context-information-self-Attenion/" style="font-size: 10px;">Context information, self-Attenion</a> <a href="/tags/Fusion/" style="font-size: 10px;">Fusion</a> <a href="/tags/Instance-Segment/" style="font-size: 10px;">Instance Segment</a> <a href="/tags/Object-Detection/" style="font-size: 20px;">Object Detection</a> <a href="/tags/Object-Detection-2D-Object-Detection/" style="font-size: 10px;">Object Detection, 2D Object Detection</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/Segment/" style="font-size: 10px;">Segment</a> <a href="/tags/%E6%8A%80%E5%B7%A7/" style="font-size: 10px;">技巧</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2/" style="font-size: 10px;">点云分割</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/02/27/mmvirtualpioint/">Multimodal Virtual Point 3D Detection论文解读</a>
          </li>
        
          <li>
            <a href="/2022/02/26/2-17%E8%B0%83%E7%A0%94/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/01/14/Faster-RCNN/">Faster-RCNN论文解读</a>
          </li>
        
          <li>
            <a href="/2022/01/13/3D_IoU_Net/">3D IoU-Net论文解读</a>
          </li>
        
          <li>
            <a href="/2022/01/12/YOLOF/">YOLOF论文解读</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 twn29004<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>
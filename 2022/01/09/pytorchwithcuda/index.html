<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Pytorch,CUDA联合编程 | twn29004&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Pytorch和CUDA联合编程的基本步骤 参考链接-w3school 参考链接-CSDN 本文代码-github 背景 目前PyTorch已经提供了丰富的接口可以直接调用，但是仍存在一些高度自定义的操作无法使用PyToch或者Python高效的完成，因此PyTorch还提供了使用C++和CUDA编程的扩展接口。C++扩展主要有两种形式，一种是使用setuptools提前构建，也可以通过">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch,CUDA联合编程">
<meta property="og:url" content="http://twn29004.top/2022/01/09/pytorchwithcuda/index.html">
<meta property="og:site_name" content="twn29004&#39;s Blog">
<meta property="og:description" content="Pytorch和CUDA联合编程的基本步骤 参考链接-w3school 参考链接-CSDN 本文代码-github 背景 目前PyTorch已经提供了丰富的接口可以直接调用，但是仍存在一些高度自定义的操作无法使用PyToch或者Python高效的完成，因此PyTorch还提供了使用C++和CUDA编程的扩展接口。C++扩展主要有两种形式，一种是使用setuptools提前构建，也可以通过">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-01-08T16:00:00.000Z">
<meta property="article:modified_time" content="2022-01-11T13:06:56.945Z">
<meta property="article:author" content="twn29004">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="twn29004&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">twn29004&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://twn29004.top"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-pytorchwithcuda" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/09/pytorchwithcuda/" class="article-date">
  <time datetime="2022-01-08T16:00:00.000Z" itemprop="datePublished">2022-01-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/">知识分享</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Pytorch,CUDA联合编程
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="pytorch和cuda联合编程的基本步骤">Pytorch和CUDA联合编程的基本步骤</h1>
<p><a target="_blank" rel="noopener" href="https://www.w3cschool.cn/pytorch/pytorch-r9fw3btc.html">参考链接-w3school</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wqwqqwqw1231/article/details/106902235">参考链接-CSDN</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/twn29004/PytorchAndCUDA">本文代码-github</a></p>
<h2 id="背景">背景</h2>
<p>目前PyTorch已经提供了丰富的接口可以直接调用，但是仍存在一些高度自定义的操作无法使用PyToch或者Python高效的完成，因此PyTorch还提供了使用C++和CUDA编程的扩展接口。C++扩展主要有两种形式，一种是使用setuptools提前构建，也可以通过torch.utils.cpp_extension.load()在运行时构建。下面仅介绍第一种方法，第二种方法之后再学习。</p>
<h2 id="基本步骤">基本步骤</h2>
<p>Pytorch,CUDA,C++联合编程的一般步骤如下:</p>
<ol type="1">
<li>首先需要定义一个C++文件，该文件声明了CUDA文件中定义的函数，还需要进行一些检查，并最终将其调用转发给<code>.cu</code>文件。此外，该文件还需要声明将在Python中调用的函数，并使用pybind11绑定到python。OpenPCDet将上述步骤划分为以下几个步骤:
<ol type="1">
<li><p>首先定义一个头文件，该头文件<code>.h</code>中包含了<code>.cu</code>文件中定义的函数和<code>.cpp</code>文件中定义的函数.</p></li>
<li><p>然后定义<code>.cpp</code>文件，其中函数的作用是负责进行一些检查和调用<code>.cu</code>文件中定义的函数</p></li>
<li><p><code>.cu</code>文件是负责执行具体的CUDA编程的操作</p></li>
<li><p>api文件是将<code>.cpp</code>文件中定义的函数和PYBIND11绑定，以便Python调用</p></li>
</ol></li>
<li>在<code>setup.py</code>文件中声明将要编译的模块名称，源文件路径等。</li>
<li>使用import导入声明的模块，使用Python实现其前向和反向传播的计算。</li>
</ol>
<h2 id="举例说明">举例说明</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── ball_query_src</span><br><span class="line">│   ├── api.cpp</span><br><span class="line">│   ├── ball_query.cpp</span><br><span class="line">│   ├── ball_query_cuda.cu</span><br><span class="line">│   ├── ball_query_cuda.h</span><br><span class="line">│   └── cuda_utils.h</span><br><span class="line">├── setup.py</span><br><span class="line">└── test_ball_query.py</span><br></pre></td></tr></table></figure>
<p>项目的目录如上图所示，其中<code>api.cpp</code>文件是将<code>ball_query.cpp</code>声明的函数使用PYBIND11与python进行绑定。其中<code>api.cpp</code>的内容如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/serialize/tensor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;ball_query_cuda.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    <span class="comment">// 第一个参数表示的是在python中调用的名称，第二个参数是对应的cpp函数，第三个参数对应的是这个函数的说明</span></span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;ball_query_wrapper&quot;</span>, &amp;ball_query_wrapper_fast, <span class="string">&quot;ball_query_wrapper_fast&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里，作者为了使得代码的结构更加清晰，其在<code>ball_query.h</code>文件中分别声明了两个函数，一个是在C++中被调用的函数，另一个是在CUDA中实现的函数。其具体内容如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> _BALL_QUERY_GPU_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _BALL_QUERY_GPU_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/serialize/tensor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime_api.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 与pybind11绑定的函数，其主要作用是调用下面的cuda函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ball_query_wrapper_fast</span><span class="params">(<span class="keyword">int</span> b, <span class="keyword">int</span> n, <span class="keyword">int</span> m, <span class="keyword">float</span> radius, <span class="keyword">int</span> nsample, </span></span></span><br><span class="line"><span class="params"><span class="function">	at::Tensor new_xyz_tensor, at::Tensor xyz_tensor, at::Tensor idx_tensor)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CUDA文件中的函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ball_query_kernel_launcher_fast</span><span class="params">(<span class="keyword">int</span> b, <span class="keyword">int</span> n, <span class="keyword">int</span> m, <span class="keyword">float</span> radius, <span class="keyword">int</span> nsample, </span></span></span><br><span class="line"><span class="params"><span class="function">	<span class="keyword">const</span> <span class="keyword">float</span> *xyz, <span class="keyword">const</span> <span class="keyword">float</span> *new_xyz, <span class="keyword">int</span> *idx)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>在<code>.h</code>文件中声明了上述两个文件之后，再分别再<code>ball_query.cpp</code>和<code>ball_query_cuda.cu</code>文件中完成这两个函数的具体实现。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/serialize/tensor.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;THC/THC.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime_api.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;ball_query_cuda.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> THCState *state;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义检查数据类型的宏</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA(x) do &#123; \</span></span><br><span class="line"><span class="meta">	  <span class="meta-keyword">if</span> (!x.type().is_cuda()) &#123; \</span></span><br><span class="line"><span class="meta">		      fprintf(stderr, <span class="meta-string">&quot;%s must be CUDA tensor at %s:%d\n&quot;</span>, #x, __FILE__, __LINE__); \</span></span><br><span class="line"><span class="meta">		      exit(-1); \</span></span><br><span class="line"><span class="meta">		    &#125; \</span></span><br><span class="line"><span class="meta">&#125; while (0)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CONTIGUOUS(x) do &#123; \</span></span><br><span class="line"><span class="meta">	  <span class="meta-keyword">if</span> (!x.is_contiguous()) &#123; \</span></span><br><span class="line"><span class="meta">		      fprintf(stderr, <span class="meta-string">&quot;%s must be contiguous tensor at %s:%d\n&quot;</span>, #x, __FILE__, __LINE__); \</span></span><br><span class="line"><span class="meta">		      exit(-1); \</span></span><br><span class="line"><span class="meta">		    &#125; \</span></span><br><span class="line"><span class="meta">&#125; while (0)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_INPUT(x) CHECK_CUDA(x);CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 完成输入数据类型的检查，同时调用cu文件中定义的函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ball_query_wrapper_fast</span><span class="params">(<span class="keyword">int</span> b, <span class="keyword">int</span> n, <span class="keyword">int</span> m, <span class="keyword">float</span> radius, <span class="keyword">int</span> nsample, </span></span></span><br><span class="line"><span class="params"><span class="function">    at::Tensor new_xyz_tensor, at::Tensor xyz_tensor, at::Tensor idx_tensor)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(new_xyz_tensor);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(xyz_tensor);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> *new_xyz = new_xyz_tensor.data&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> *xyz = xyz_tensor.data&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">int</span> *idx = idx_tensor.data&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">ball_query_kernel_launcher_fast</span>(b, n, m, radius, nsample, new_xyz, xyz, idx);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>ball_query_cuda.cu</code>实现</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;ball_query_cuda.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;cuda_utils.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">ball_query_kernel_fast</span><span class="params">(<span class="keyword">int</span> b, <span class="keyword">int</span> n, <span class="keyword">int</span> m, <span class="keyword">float</span> radius, <span class="keyword">int</span> nsample, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">float</span> *__restrict__ new_xyz, <span class="keyword">const</span> <span class="keyword">float</span> *__restrict__ xyz, <span class="keyword">int</span> *__restrict__ idx)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// new_xyz: (B, M, 3)</span></span><br><span class="line">    <span class="comment">// xyz: (B, N, 3)</span></span><br><span class="line">    <span class="comment">// output:</span></span><br><span class="line">    <span class="comment">//      idx: (B, M, nsample)</span></span><br><span class="line">    <span class="keyword">int</span> bs_idx = blockIdx.y;</span><br><span class="line">    <span class="keyword">int</span> pt_idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (bs_idx &gt;= b || pt_idx &gt;= m) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    new_xyz += bs_idx * m * <span class="number">3</span> + pt_idx * <span class="number">3</span>;</span><br><span class="line">    xyz += bs_idx * n * <span class="number">3</span>;</span><br><span class="line">    idx += bs_idx * m * nsample + pt_idx * nsample;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> radius2 = radius * radius;</span><br><span class="line">    <span class="keyword">float</span> new_x = new_xyz[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">float</span> new_y = new_xyz[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">float</span> new_z = new_xyz[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; n; ++k) &#123;</span><br><span class="line">        <span class="keyword">float</span> x = xyz[k * <span class="number">3</span> + <span class="number">0</span>];</span><br><span class="line">        <span class="keyword">float</span> y = xyz[k * <span class="number">3</span> + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">float</span> z = xyz[k * <span class="number">3</span> + <span class="number">2</span>];</span><br><span class="line">        <span class="keyword">float</span> d2 = (new_x - x) * (new_x - x) + (new_y - y) * (new_y - y) + (new_z - z) * (new_z - z);</span><br><span class="line">        <span class="keyword">if</span> (d2 &lt; radius2)&#123;</span><br><span class="line">            <span class="keyword">if</span> (cnt == <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> l = <span class="number">0</span>; l &lt; nsample; ++l) &#123;</span><br><span class="line">                    idx[l] = k;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            idx[cnt] = k;</span><br><span class="line">            ++cnt;</span><br><span class="line">            <span class="keyword">if</span> (cnt &gt;= nsample) <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ball_query_kernel_launcher_fast</span><span class="params">(<span class="keyword">int</span> b, <span class="keyword">int</span> n, <span class="keyword">int</span> m, <span class="keyword">float</span> radius, <span class="keyword">int</span> nsample, \</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">float</span> *new_xyz, <span class="keyword">const</span> <span class="keyword">float</span> *xyz, <span class="keyword">int</span> *idx)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// new_xyz: (B, M, 3)</span></span><br><span class="line">    <span class="comment">// xyz: (B, N, 3)</span></span><br><span class="line">    <span class="comment">// output:</span></span><br><span class="line">    <span class="comment">//      idx: (B, M, nsample)</span></span><br><span class="line"></span><br><span class="line">    cudaError_t err;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blocks</span><span class="params">(DIVUP(m, THREADS_PER_BLOCK), b)</span></span>;  <span class="comment">// blockIdx.x(col), blockIdx.y(row)</span></span><br><span class="line">    <span class="function">dim3 <span class="title">threads</span><span class="params">(THREADS_PER_BLOCK)</span></span>;</span><br><span class="line"></span><br><span class="line">    ball_query_kernel_fast&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(b, n, m, radius, nsample, new_xyz, xyz, idx);</span><br><span class="line">    <span class="comment">// cudaDeviceSynchronize();  // for using printf in kernel function</span></span><br><span class="line">    err = <span class="built_in">cudaGetLastError</span>();</span><br><span class="line">    <span class="keyword">if</span> (cudaSuccess != err) &#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(stderr, <span class="string">&quot;CUDA kernel failed : %s\n&quot;</span>, <span class="built_in">cudaGetErrorString</span>(err));</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至此，ball_query的核心功能已经完成，然后我们需要使用<code>setup.py</code>文件来编译上述文件。<code>setup.py</code>文件的具体实现如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> find_packages, setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> BuildExtension, CUDAExtension</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_cuda_ext</span>(<span class="params">name, module, sources</span>):</span></span><br><span class="line">    cuda_ext = CUDAExtension(</span><br><span class="line">        name=<span class="string">&#x27;%s.%s&#x27;</span> % (module, name),</span><br><span class="line">        sources=[os.path.join(module.split(<span class="string">&#x27;.&#x27;</span>)[-<span class="number">1</span>], src) <span class="keyword">for</span> src <span class="keyword">in</span> sources]</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>([os.path.join(*module.split(<span class="string">&#x27;.&#x27;</span>), src) <span class="keyword">for</span> src <span class="keyword">in</span> sources])</span><br><span class="line">    <span class="keyword">return</span> cuda_ext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;ballquery&#x27;</span>,</span><br><span class="line">        packages=find_packages(),</span><br><span class="line">        ext_modules=[</span><br><span class="line">            CUDAExtension(<span class="string">&#x27;ball_query_cuda&#x27;</span>,[</span><br><span class="line">                <span class="string">&#x27;ball_query_src/api.cpp&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;ball_query_src/ball_query.cpp&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;ball_query_src/ball_query_cuda.cu&#x27;</span>,  </span><br><span class="line">            ])</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;</span><br><span class="line">            <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>至此，我们已经生成了上述代码的链接库，但是如果需要将其嵌入到神经网络中，还需要定义其前向传播和反向传播方法。这里我们在<code>test_ball_query.py</code>文件中完成其前向传播和反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function, Variable</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ball_query_cuda</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义该方法的前向传播和反向传播方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BallQuery</span>(<span class="params">Function</span>):</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, radius: <span class="built_in">float</span>, nsample: <span class="built_in">int</span>, xyz: torch.Tensor, new_xyz: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param ctx:</span></span><br><span class="line"><span class="string">        :param radius: float, radius of the balls</span></span><br><span class="line"><span class="string">        :param nsample: int, maximum number of features in the balls</span></span><br><span class="line"><span class="string">        :param xyz: (B, N, 3) xyz coordinates of the features</span></span><br><span class="line"><span class="string">        :param new_xyz: (B, npoint, 3) centers of the ball query</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">            idx: (B, npoint, nsample) tensor with the indicies of the features that form the query balls</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> new_xyz.is_contiguous()</span><br><span class="line">        <span class="keyword">assert</span> xyz.is_contiguous()</span><br><span class="line"></span><br><span class="line">        B, N, _ = xyz.size()</span><br><span class="line">        npoint = new_xyz.size(<span class="number">1</span>)</span><br><span class="line">        idx = torch.cuda.IntTensor(B, npoint, nsample).zero_()</span><br><span class="line">        </span><br><span class="line">        ball_query_cuda.ball_query_wrapper(B, N, npoint, radius, nsample, new_xyz, xyz, idx)</span><br><span class="line">        <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, a=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ball_query = BallQuery.apply</span><br><span class="line"></span><br><span class="line">xyz = torch.randn(<span class="number">2</span>, <span class="number">128</span>, <span class="number">3</span>).cuda()</span><br><span class="line">new_xyz = xyz</span><br><span class="line"></span><br><span class="line">result = ball_query(<span class="number">0.8</span>, <span class="number">3</span>, xyz, new_xyz)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.shape)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://twn29004.top/2022/01/09/pytorchwithcuda/" data-id="cl2d63r91001qmsxcbb83csdn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/01/09/GSPN/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          GSPN论文解读
        
      </div>
    </a>
  
  
    <a href="/2022/01/07/RelationNetworkforObjectDetection/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Relation Network for Object Network解读</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/">参考链接</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/">知识分享</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/2D-Object-Detection/" rel="tag">2D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/2D-Object-Detection/" rel="tag">2D Object-Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Object-Detect-Graph-Network/" rel="tag">3D Object Detect, Graph Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Object-Detection/" rel="tag">3D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">3D目标检测</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context-information-self-Attenion/" rel="tag">Context information, self-Attenion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fusion/" rel="tag">Fusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Instance-Segment/" rel="tag">Instance Segment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Object-Detection/" rel="tag">Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Object-Detection-2D-Object-Detection/" rel="tag">Object Detection, 2D Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pytorch/" rel="tag">Pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Segment/" rel="tag">Segment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E5%B7%A7/" rel="tag">技巧</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2/" rel="tag">点云分割</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/2D-Object-Detection/" style="font-size: 15px;">2D Object Detection</a> <a href="/tags/2D-Object-Detection/" style="font-size: 10px;">2D Object-Detection</a> <a href="/tags/3D-Object-Detect-Graph-Network/" style="font-size: 10px;">3D Object Detect, Graph Network</a> <a href="/tags/3D-Object-Detection/" style="font-size: 20px;">3D Object Detection</a> <a href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">3D目标检测</a> <a href="/tags/Context-information-self-Attenion/" style="font-size: 10px;">Context information, self-Attenion</a> <a href="/tags/Fusion/" style="font-size: 10px;">Fusion</a> <a href="/tags/Instance-Segment/" style="font-size: 10px;">Instance Segment</a> <a href="/tags/Object-Detection/" style="font-size: 20px;">Object Detection</a> <a href="/tags/Object-Detection-2D-Object-Detection/" style="font-size: 10px;">Object Detection, 2D Object Detection</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/Segment/" style="font-size: 10px;">Segment</a> <a href="/tags/%E6%8A%80%E5%B7%A7/" style="font-size: 10px;">技巧</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2/" style="font-size: 10px;">点云分割</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/02/27/mmvirtualpioint/">Multimodal Virtual Point 3D Detection论文解读</a>
          </li>
        
          <li>
            <a href="/2022/02/26/2-17%E8%B0%83%E7%A0%94/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/01/14/Faster-RCNN/">Faster-RCNN论文解读</a>
          </li>
        
          <li>
            <a href="/2022/01/13/3D_IoU_Net/">3D IoU-Net论文解读</a>
          </li>
        
          <li>
            <a href="/2022/01/12/YOLOF/">YOLOF论文解读</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 twn29004<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>
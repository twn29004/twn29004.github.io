<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CT3D论文</title>
      <link href="/2021/11/27/CT3D%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
      <url>/2021/11/27/CT3D%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292051455.png" alt="image-20211129205132392" /></p><p><a href="https://github.com/hlsheng1/CT3D">代码链接</a></p><p><a href="https://arxiv.org/abs/2108.10723">paper链接</a></p><h2 id="论文总结">论文总结</h2><p>本文提出了一种目前二阶段的目标检测算法不能很好的提取proposal中的特征。本文提出了一种基于通道层面的self-attention结构来提高网络对于proposal中点的特征的提取能力。</p><p>下面简单介绍一下网络的处理流程:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292056595.png" alt="image-20211129205620537" /></p><p>与传统的二阶段目标检测器一样，首先使用一个backbone提取点样场景的特征，然后使用RPN网络生成proposal。注意，这里生成的proposal一个三维的边界框。文中为了能够更好的提取其周围点云的特征，将这个三维边界框转化成了一个不限制高度的圆柱体。圆柱体的直径是底边对角线长度的一定倍数。然后在生成的圆柱体中采样256个点，根据这些点来提取proposal的特征。文中使用了各个点点对原始的三维边界框(就是proposal提出的)的八个角点来计算相对位置作为点的特征的一部分。proposal中的原始特征可以表示为<span class="math inline">\([\bigtriangleup p_i^c, \bigtriangleup p_i^1, ..., \bigtriangleup p_i^8, f_i^r]\)</span>,其中<span class="math inline">\(\bigtriangleup p_i^c\)</span>表示的相对三维边界框中心点的相对位置，<span class="math inline">\(\bigtriangleup p_i^1\)</span>表示的相对边界框角点的位置。 <span class="math inline">\(f_i^r\)</span>表示的是点的原始特征。</p><p>然后作者使用了一个self-attention网络来提取proposal中的点的特征。</p><p>但是在decoder部分，作者并没有使用transformer中的标准的decoder。不使用标准的decoder主要有以下两点原因:</p><ol type="1"><li>如果使用transformer中的decoder网络，需要<span class="math inline">\(M\)</span>个quert嵌入,这将消耗大量的内存</li><li>另一方面，标准的transformer中的decoder网络是需要生成<span class="math inline">\(M\)</span>个单词或者句子，而我们只需要一个proposal的特征的表示。</li></ol><p>此外，作者还分析了使用一个decoder来处理上述问题时还存在一个原始的transformer中的decoder仅关注了全局的信息，而忽略了各个通道的信息，作者的想法是在3D检测中，各个通道都是非常重要的，不同的通道之间通常意味着不同的几何关系。因此我们要重视不同通道的。所以设计了一个通道层面的re-weigthing网络。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292126841.png" alt="image-20211129212652791" /></p><p>a)是原始的transformer中的decoder计算权重的方式。b)是作者提出的在通道层面赋予每个点不同权重的方式。其中a)主要关注的是全局信息，b)主要关注的是局部信息，因此c)将两者结合了起来。</p><p>然后将生成的特征用于计算置信度和进一步调整三维边界框。</p><p>实验结果:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292132776.png" alt="image-20211129213223726" /></p><p>从表中可以看出，作者的网络在KITTI的test数据集上还是取得了不错的效果。超越了PV-RCNN和Voxel-RCNN。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292137720.png" alt="image-20211129213725664" /></p><p>该表展示了作者设计的第二阶段网络对于模型性能提升的影响。从表中可以看出，作者设计的二阶段的网络对模型的性能的提升非常大。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292137177.png" alt="image-20211129213743112" /></p><p>第二三行展示了作者提出的proposal-to-point的效果的对比，从表中结果可以看出，这部分其实对于网络性能的影响非常小。仅0.17%左右。第1行和第3行展示了self-attention中的encoder的作用，该方法提升了2.2%，是一个非常大的提升。此外，作者设计的channel-wise的decoder也在一定程度上提高了模型的精度。</p><h2 id="可用知识点">可用知识点</h2><p>可以使用self-attention中的decoder来提取点云的特征。从文中的结果看出，该部分网络对于模型性能的提升是巨大的。此外，在提取proposal的时候，可以在一定程度上扩大一点proposal的范围。还有就是关于文中的这个decoder结构，我们在使用一些经典的模型的时候，可能需要根据使用环境多一些思考。</p><h2 id="常用句式">常用句式</h2><p>[挖坑]</p><h2 id="代码解读">代码解读</h2>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPVNet解读</title>
      <link href="/2021/11/27/RPVNet/"/>
      <url>/2021/11/27/RPVNet/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161253759.png" alt="image-20211116125355498" /></p><p><a href="">代码链接</a></p><p><a href="https://arxiv.org/abs/2103.12978">RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation (arxiv.org)</a></p><h2 id="论文总结">论文总结</h2><p>本文实现了一种大场景下的点云语义分割网络，并在SemanticKITTI数据集上实现了1st的结果。其主要通过一种融合的方法来实现。不同于以往的使用基于点，基于体素和基于深度图(Range Image)的方法.但是这些方法都有一定的缺点。之前融合的方法也仅是融合其中的两类。本文提出了一种融合三类数据的网络。网络的主要结构如下:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161300855.png" alt="image-20211116130031567" /></p><p>网络主要分为三个分支，使用了经典的Encoder和Decoder的结构。三个分支分别是基于体素的，基于点云的和基于深度图的。分别使用对应的网络提取各个分支的特征，然后作者使用了点这个分支来作为中间节点，来融合不同来源的数据。不同特征进行融合的算法如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161329277.png" alt="image-20211116132950759" /></p><p>将其他特征映射到基于点的表示的话，可以使用最近邻插值或者三线性插值或者双线性插值的方法。以基于深度图到基于点的表示为例:通过计算该点在深度图上的周围的四个</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161348473.png" alt="image-20211116134822778" style="zoom:80%;" /></p><p>其中，<span class="math inline">\(F_R（u）\)</span>表示下标为<span class="math inline">\(u\)</span>的深度图的特征。<span class="math inline">\(\phi(u,j)\)</span>表示的是对应的线性权重的计算方法。<span class="math inline">\(\delta(j)\)</span>表示的是<span class="math inline">\(j\)</span>周围的四个邻居。</p><p>在讲其他表示的特征投影到点上之后，作者提出了一个GFM(Gated Fusion Module)来对这些点表示的特征进行融合。GFM的结构如图2中的蓝色部分所示，然后就得到了一个基于点的融合后的特征，然后讲融合后的特征在转换到其他表示的形式下。</p><p>从点到其他两种表达形式的话，作者使用的是平均的方法。以体素为例子，将属于一个体素的所有点的特征进行平均，然后将平均后的特征作为这个体素的特征。具体做法图下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427140.png" alt="image-20211116142742978" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427766.png" alt="image-20211116142730453" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161423649.png" alt="image-20211116142351294" style="zoom:80%;" /></p><p>消融实验部分</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161441821.png" alt="image-20211116144154823" /></p><p>Table4 表示了不同的视角对于实验结果的影响。R表示深度图，P表示点，V表示体素以及不同分辨率的体素。</p><p>Table5 展示了不同融合方法的性能差异。其中Addition表示将特征进行相加，Concatnation表示进行拼接，Gated Fusion表示使用了作者设计的网络来进行融合。</p><p>Table6 展示了集成模型与本文模型的性能差异</p><p>Table7 展示了不同分支对于模型性能的影响。</p><p><span class="math display">\[d = \sqrt{a^2 + b^2}\]</span><br /><span class="math inline">\(d\)</span>是欧式距离</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">s = <span class="string">&quot;hello, world&quot;</span>;</span><br><span class="line">cout&lt;&lt;s&lt;&lt;endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Segment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯学习</title>
      <link href="/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="贝叶斯理论">贝叶斯理论</h2><h3 id="计算后验概率">计算后验概率</h3><p><span class="math display">\[P(h|D)=\frac{P(D|h)p(h)}{p(D)}\]</span></p><p>其中，<span class="math inline">\(P(D|h)\)</span>是根据数据观察到的条件概率，<span class="math inline">\(p(h)\)</span>是先验概率。我们要做的就是在假设空间中找到具有最大后验概率的假设。那么就有:</p><p><span class="math display">\[h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\\=arg max_{h \in H}P(D|h)p(h)\]</span></p><p>如果每个假设出现的概率相等，等上述问题退化为<span class="math inline">\(h_{ML}=argmax_{h \in H}P(D|h)\)</span>,这个就是极大似然假设。上面那个是极大后验假设。</p><p>使用极大似然假设进行癌症诊断的例子：</p><ol type="1"><li><p>两个假设：1) 病人有癌症，2) 病人没有癌症</p></li><li><p>两种可能的测试输出: 1) positive,记为+ 2) negative,记为-</p></li><li><p>先验知识：P(cancer) = 0.008 P(not cancer)=0.992 P(+|cancer)=0.98 P(- |cancer)=0.02</p><p>P(- |not cancer) = 0.97 P(+|not cancer) = 0.03</p></li><li><p>分别计算病人检测结果为阳性，其患癌症和不患癌症哪个概率大<br /><span class="math display">\[P(cancer|+)=\frac{P(+|cancer)*P(cancer)}{P(+)} \\P(not cancer | +) = \frac{P(+ | not cancer)*P(not cancer)}{P(+)}\]</span><br />从上述式子可以看出，只用比较两个式子的分子部分就可以对比谁的概率大。</p></li></ol><h2 id="极大似然假设和极大后验概率假设的区别">极大似然假设和极大后验概率假设的区别</h2><ol type="1"><li><p>极大后验概率假设：<br /><span class="math display">\[h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\\=arg max_{h \in H}P(D|h)p(h)\]</span></p></li><li><p>极大似然假设</p><p>当假设空间中每个假设出现的概率都相同时，我们就可以不再考虑极大后验概率中的<span class="math inline">\(P(h)\)</span>，仅考虑数据<span class="math inline">\(D\)</span>对于假设<span class="math inline">\(h\)</span>的似然值<span class="math inline">\(P(D|h)\)</span>。则上述问题退化为:<br /><span class="math display">\[h_{ML}=argmax_{h\in H}P(D|h)\]</span></p></li></ol><h2 id="一般概率准则">一般概率准则</h2><ol type="1"><li><p>求和准则： 如果两个事件不是互相独立的，则A或B发生的概率可以使用一下方法进行计算:</p><p><span class="math inline">\(P(A \or B) = P(A) + P(B) - P(A \and B)\)</span></p></li><li><p>相乘准则：</p><ol type="1"><li><p>如果两个事件不是独立的，则：<span class="math inline">\(P(A \and B)=P(A | B)*P(B) =P(B|A)*P(A)\)</span></p></li><li><p>如果两个事件是独立的，则: <span class="math inline">\(P(A \and B)=P(A)*P(B)\)</span></p></li><li><p>当且仅当:<br /><span class="math display">\[P(A|B)=P(A) or \\P(B|A) = P(B) or \\P(AB)=P(A)P(B)\]</span><br />我们可以说事件A和事件B是独立的</p></li><li><p>全概率准则:</p><p>如果<span class="math inline">\(A_1...A_n\)</span>是互斥的，独立的事件，即<span class="math inline">\(P(B) = \sum_{1}^{n}P(B|A_i)P(A_i)\)</span></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211006281.png" alt="image-20211121100603229" /></p><p>上表中，第三列是各个年龄段患病的概率，最后一列是各个年龄段所占的比例。则求整个疾病的患病概率<span class="math inline">\(P(D)\)</span>。</p></li></ol></li></ol><h2 id="最小描述长度">最小描述长度</h2><p>最小描述长度(Minimum Description Length Principle MDL)</p><ol type="1"><li><p>基本思想：</p><ol type="1"><li>对于一个给定的假设集合和数据集，我们需要在假设集中找到或者合并一些使得数据被压缩的最小的假设。</li></ol></li><li><p>最小描述长度原理:</p><p><span class="math inline">\(h_{MDL}=arg \min_{h \in H} L_{C_1}(h)+L_{C_{2}}(D|h)\)</span>,其中<span class="math inline">\(L_{c_1}(h)\)</span>表示的是模型的复杂度，其具体表示描述整个模型所需的最小比特, <span class="math inline">\(L_{C_2}(D|h)\)</span>表示的是描述使用假设编码后的最小长度。在这里可以代表模型的错误率。所以最小描述长度就是在模型复杂度和模型性能之间做一个权衡。</p></li></ol><h2 id="贝叶斯最优分类器">贝叶斯最优分类器</h2><ol type="1"><li><p>核心思想：</p><p><a href="https://blog.csdn.net/lsjseu/article/details/12285407">贝叶斯最优分类器</a></p><p><strong>一般来说，新实例的最可能的分类可以通过合并所有假设的预测得到。用后验概率来加权</strong>。如果新样例可能分类可取集合<span class="math inline">\(V\)</span>的任一值<span class="math inline">\(v_j\)</span>，那么概率<span class="math inline">\(P(v_j|D)\)</span>表示新实例<span class="math inline">\(v_j\)</span>被正确分类的概率，其值为:<br /><span class="math display">\[P(v_i|D)=\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)\]</span><br />新实例的最优分类为使<span class="math inline">\(P(v_i|D)\)</span>最大的<span class="math inline">\(v_i\)</span>的值。因此贝叶斯最优分类器有：<br /><span class="math display">\[arg\max_{v_i \in V}\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)\]</span></p></li><li><p>贝叶斯最优分类器:</p><ol type="1"><li>该方法最大化了新实例被分类正确的可能性</li><li>在使用相同的假设空间和先验知识的条件下，没有其他的模型能够超过该方法</li><li>该方法所作的决策不能对应到假设空间中某个具体的假设上</li></ol></li><li><p>贝叶斯最优分类器存在的问题：需要计算所有可能的模型和假设，当假设空间很大时，这个计算是耗时的。因此可以使用Gibbs算法来解决</p></li></ol><h2 id="吉布斯算法">吉布斯算法</h2><p>为了解决贝叶斯最优分类器难以计算的问题，提出了吉布斯算法。吉布斯算法的基本思想就是:</p><p>根据后验概率在假设空间的分布，随机的从假设空间选取一个假设来对新实例进行分类，在具体的条件下，该方法的分类错误率小于两倍的贝叶斯最优分类器的错误率。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211323660.png" alt="image-20211121132357605" /></p><p>但是，通常从假设空间采样是困难的，因为：</p><ol type="1"><li><span class="math inline">\(P(h|D)\)</span>难以计算</li><li>对于没有参数的分类器例如SVM，<span class="math inline">\(P(h|D)\)</span>不可能计算</li><li><span class="math inline">\(P(h|D)\)</span>在假设空间很大时非常的小。</li></ol><p>为了解决上述问题，提出了Bagging 分类器算法。通过将从假设空间采样转化为从训练数据中进行采样。</p><h2 id="bagging-分类器算法">Bagging 分类器算法</h2><ol type="1"><li><p>Boostrap采样</p><p>对于给定的训练数据D,其中包含m个训练样本，从D中有放回的随机采样n个样本构成<span class="math inline">\(D^i\)</span>数据集。<span class="math inline">\(D^i\)</span>理论上有0.37的数据没有被采样到。</p></li><li><p>Bagging算法</p><p>构建<span class="math inline">\(k\)</span>个<span class="math inline">\(D^i\)</span>，对灭一个<span class="math inline">\(D^i\)</span>分别构建一个分类器<span class="math inline">\(h^i\)</span>,在对新实例分类时，所有的分类器以相同的权重进行投票:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211337106.png" alt="image-20211121133725061" /></p></li></ol><h2 id="偏差方差分解"><strong>偏差方差分解</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/38853908">参考链接</a></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212130947.png" alt="image-20211121213036863" style="zoom:50%;" /></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212141086.png" alt="image-20211121214117003" /></p><p>注意借助预测值的期望这个值来使得方差和偏差能够联系起来。此外，还要知道偏差和方差的计算方法，这样才能刚好的往这个方向去凑:<br /><span class="math display">\[var=E_{D}[g^{(D)}(x) - E_{D}[g^{(D)}(x)]] \\bias = E_{D}[y - E_{D}[g^{(D)}(x)]]  \\\sigma^2 = (y - y_D)^2\]</span><br />注意上述公式中的第二个关于偏差的计算中，式子中的<span class="math inline">\(y\)</span>不是表示数据标签，而是训练数据的真实标签，因为训练所用的标签和真实的标签可能存在一定的误差。其中<span class="math inline">\(y_D\)</span>表示的是数据在训练时的标签。</p><p>偏差方差分解的核心原理就是利用已知的方差偏差的计算公式不断的去凑。第一次用训练结果的期望去凑，第二次用训练时的标签<span class="math inline">\(y_D\)</span>去凑。</p><p>方差描述的是不同训练集训练出的模型的差距。</p><p>偏差描述的是所有可能的训练数据集训练的所有模型的输出的平均值与真实模型之间输出值之间的差距。</p><h2 id="朴素贝叶斯分类器--可能会出计算题">朴素贝叶斯分类器--可能会出计算题</h2><p>假设训练集中的一个实例可以划分为有n个属性的组合。<span class="math inline">\(&lt;a_1,a_2,...a_n&gt;\)</span>，同时，类别空间<span class="math inline">\(V\)</span>是一个有限的集合。</p><p>使用贝叶斯方法对新实例进行分类:<br /><span class="math display">\[v_{MAP}=arg \max_{v_J \in V}P(v_j|a_1,a_2,...a_n) \\v_{MAP}=arg \max_{v_J \in V]}\frac{P(a_1,a_2,..a_n|v_j)P(v_j)}{P(a_1,a_2,..a_n)}\]</span><br />如何计算<span class="math inline">\(P(v_j)\)</span>和<span class="math inline">\(P(a_1,a_2,...a_n|v_j)\)</span>：</p><ol type="1"><li><span class="math inline">\(P(v_j)\)</span>是这一类别在训练数据中出现的概率</li><li><strong><span class="math inline">\(P(a_1, a_2, ...,a_n|v_j)\)</span>不太可信，除非我们有一个非常非常大的训练样本。</strong></li></ol><p>为了解决<span class="math inline">\(P(a_1,a_2, ..a_n|v_j)\)</span>难以直接计算的问题，提出了贝叶斯假设。其核心内容就是所有的属性值都是相互独立的。所以有:<br /><span class="math display">\[P(a_1,a_2,...a_n|v_j)=\prod_{i}P(a_i|v_i)\]</span><br /><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211352341.png" alt="image-20211121135228260" /></p><p>计算的例子。</p><h2 id="贝叶斯信念网络----计算题">贝叶斯信念网络----计算题</h2><p>贝叶斯最优分类器难以计算，朴素贝叶斯要求各个属性相互独立，条件太过严格。贝叶斯信念网络提出了一种折衷的方法。</p><h3 id="条件独立">条件独立</h3><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211708912.png" alt="image-20211121170821847" /></p><p>在给定<span class="math inline">\(z\)</span>的请款下，对于任意的<span class="math inline">\(x\)</span>，都有<span class="math inline">\(P(X=x_i|Y=y_j,Z=z_k)=P(x=x_i|Z=z_k)\)</span>或者说<span class="math inline">\(P(X|Y,Z)=P(X|Z)\)</span>。我们就称<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>关于<span class="math inline">\(Z\)</span>是条件独立的。</p><p>这样的话，使用条件独立可以将朴素贝叶斯调整为:<br /><span class="math display">\[P(X,Y|Z)=P(X|Y,Z)*P(Y|Z)=P(X|Z)*P(Y|Z) \]</span><br />上述式子在另一个角度证明了朴素贝叶斯。因为朴素贝叶斯中<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>是相互独立的。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211900632.png" alt="image-20211121190026571" /></p><p>计算下列事件发生的概率:</p><ol type="1"><li><span class="math inline">\(P(W|S)\)</span></li><li><span class="math inline">\(P(S|W)\)</span></li><li><span class="math inline">\(P(S|R,W)\)</span></li></ol><p><span class="math display">\[\begin{equation}\begin{aligned}P(W|S) &amp;= \frac{P(W,S)}{P(S)} \\&amp;= \frac{P(W,S|R)*P(R) +P(W,S|\neg R)*P(\neg R)}{P(S)} \\&amp;= \frac{P(W,S,R) + P(W,S,\neg R)}{P(S)} \\&amp;= \frac{P(W,S,R)}{P(R,S)}*\frac{P(R,S)}{R(S)} + \frac{P(W,\neg R,S)}{P(\neg R, S)}*\frac{P(\neg R,S)}{P(S)} \\  &amp;= P(W|R,S)*P(R|S) + P(W | \neg R,S)*P(\neg R | S) \\&amp;= P(W|R,S)*P(R) + P(W|\neg R, S)*P(\neg R) \\\end{aligned}\end{equation}\]</span></p><p>第三行变换的依据是根据已知条件凑出来的。最后一行的变化是因为<span class="math inline">\(R\)</span>和<span class="math inline">\(S\)</span>是独立的。<br /><span class="math display">\[P(S|W) = \frac{P(W|S)*P(S)}{P(W)} \\\begin{equation}\begin{aligned}P(W) &amp;= P(W|R,S)*P(R , S) + P(W|R, \neg S) *P(R, \neg S) \\&amp;+ P(W|\neg R, S) * P(\neg R. S) + P(W|\neg R, \neg S) *P(\neg R, \neg S) \end{aligned}\end{equation} \\\\P(R,S) = P(R)*P(S)\]</span></p><p><span class="math display">\[\begin{equation}\begin{aligned}P(S|R,W) &amp;= \frac{P(R,S,W)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(R,S)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(R)*P(S)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(S)}{P(W|R)}\end{aligned}\end{equation} \\\]</span></p><p><span class="math display">\[\begin{equation}\begin{aligned}P(W|R) &amp;= \frac{P(W,R)}{P(R)} \\&amp;= \frac{P(W.R|S) * P(S) + P(W,R|\neg S)*P(\neg S)}{P(R)} \\&amp;= \frac{P(W,R,S) + P(W,R,\neg S)}{P(R)} \\&amp;= \frac{P(W, R,S)}{P(R,S)}*\frac{P(R,S)}{P(R)} + \frac{P(W, R,\neg S)}{P(R,\neg S)}*\frac{P(R,\neg S)}{P(R)} \\&amp;= P(W|R,S)*P(S|R) + P(W|R, \neg S)*P(\neg S|R) \\&amp;= P(W|R,S)*P(S) + P(W|R, \neg S)*P(\neg S)\end{aligned}\end{equation}\]</span></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212023894.png" alt="image-20211121202358797" /></p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 知识分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于服务器GPU使用的一些注意事项</title>
      <link href="/2021/12/20/GPU-tips/"/>
      <url>/2021/12/20/GPU-tips/</url>
      
        <content type="html"><![CDATA[<h1 id="关于gpu">关于GPU</h1><h2 id="nvidia-smi命令">nvidia-smi命令</h2><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112201907358.png" alt="image-20211220190754247" /></p><p>1表示的是显存的占用率，可以理解为电脑内存，手机运行内存的占用率。2表示的是显卡利用率，可以理解为电脑CPU的占用率。3表示的是占用显卡的进程的相关信息。</p><h2 id="关于显卡利用率的问题">关于显卡利用率的问题</h2><p>有时候会遇到程序占着显存，但是显卡利用率为0的情况。这个时候可以从两个方面进行分析:</p><ol type="1"><li><p>查看进程是否挂起。查看方法<code>ls -l \proc\PID</code>。举个例子，查看上图中PID为41732的程序的相关信息.使用以下命令<code>ls -l \proc\41732</code></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112201913046.png" alt="image-20211220191300993" /></p><p>可以通过cwd和exe查看相关进程的信息。如果出现该进程deleted的情况，说明该进程被挂起，可以私信对应的同学询问具体情况。</p><p><strong>tips: 在终止程序运行时，因使用快捷键<code>ctrl + C</code>。不是使用<code>ctrl+Z</code>。后者是将程序挂起。不清楚的同学可参考<a href="https://blog.csdn.net/mylizh/article/details/38385739">参考链接</a></strong></p></li><li><p><code>nvidia-smi</code>命令<strong>没有显示进程占用显存</strong>，但是<strong>显存占用率依然很高，且显卡利用率为0</strong>。这个时候可能是出现显存泄露的问题。处理方法可参考<a href="https://zhuanlan.zhihu.com/p/375330192">参考链接</a></p></li><li><p>不是上述两种情况，即<strong>程序正常再跑，但是显卡利用率很低，且出现一会高一会低的情况</strong>，显存也正常。这个时候可能是因为数据集加载过程中num_worker设置较小，GPU大量时间空闲的原因或者其他的一些原因。具体可参考<a href="https://zhuanlan.zhihu.com/p/410244780">参考链接</a></p></li></ol><h2 id="关于cuda的问题">关于CUDA的问题</h2><p>同一个服务器可以安装多个版本的CUDA。但是同一个用户下，同一时间只能使用同一个版本的CUDA。不同用户可以在同一时间使用不同版本的CUDA。下面主要介绍两个方面的内容，一个是CUDA版本的切换和不同CUDA版本的安装。</p><h3 id="cuda版本切换">CUDA版本切换</h3><p>CUDA版本的切换可以分为两种方法，第一种是通过修改软连接(软连接可以理解为快捷方式，具体内容可自行百度)的方法，该方法在全局范围内都会生效。换句话说就是你改了，其他正在使用这台服务器的同学的CUDA版本也改了。另一种方法是只在当前终端生效，就是如果你和服务器断开链接，你的修改就失效了，此外，由于只在当前终端生效，也不会影响其他同学的使用。</p><h4 id="修改软连接">修改软连接</h4><p>首先进入到<code>/usr/local</code>目录下，使用<code>ll</code>命令查看该目录下相关文件的信息:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112201930187.png" alt="image-20211220193006140" /></p><p>可以看到，cuda目录链接到了<code>cuda-10.1</code>目录下，说明当前版本为10.1(可通过<code>nvcc -V</code>查看，注意大写).如果想切换为cuda-11.0，可使用如下命令<code>sudo ln -snf ./cuda-11.0 ./cuda</code>。执行后相关目录信息变为:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112201933553.png" alt="image-20211220193329508" /></p><p>从上述图片可以看出，成功切换了cuda版本，但是这个方法的弊端就是这个修改是全局生效的，就是可能会影响到其他正在使用CUDA的同学。</p><h4 id="设置临时环境变量---只在当前终端生效">设置临时环境变量---只在当前终端生效</h4><p>我们可以通过设置临时的环境变量使得CUDA版本的修改只在当前终端生效。原理就是利用<code>export</code>声明的环境变量只在当前终端生效。因此我们可以使用export来声明和CUDA相关的环境变量来实现修改CUDA版本的目的。</p><p>具体步骤如下图所示:</p><ol type="1"><li><p>首先使用sudo创建shell脚本.(因为/usr/local目录下普通用户没有写权限，当然也不一定需要在/usr/local目录下，其他目录也是可以的).根据需要修改的CUDA的版本，在下面的shell脚本中填写对应的路径.例如cuda-11.0。如果是其他版本，可以修改为cuda-11.1等。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">export PATH=/usr/local/cuda-11.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda-11.0</span><br></pre></td></tr></table></figure></li><li><p>修改shell脚本的own.因为当前脚本的所有者为root用户，普通用户没有权限执行。使用命名<code>chown lyc chang_cuda_to_110.sh</code>。</p></li><li><p>执行脚本<code>. chang_cuda_to_110.sh</code>或者<code>source change_cuda_to_110.sh</code>。不要使用<code>sh</code>和<code>bash</code>.具体区别可参考<a href="https://www.cnblogs.com/pcat/p/5467188.html">参考链接</a></p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112201952363.png" alt="image-20211220195225313" /></p><p>上图就是使用shell脚本来修改CUDA版本的例子。上述方法有以下两个需要注意的地方:</p><ol type="1"><li>该方法只在当前终端有效，<strong>换句话说就是，你断开服务器，在重连，这个修改就失效了</strong>。可以配合screen,tmux等命令使用。其优点是不会影响到其他同学。</li><li><strong>不要去修改<code>~/.bashrc</code>文件中关于CUDA的环境变量，不要去修改<code>~/.bashrc</code>文件中关于CUDA的环境变量，不要去修改<code>~/.bashrc</code>文件中关于CUDA的环境变量，</strong></li></ol><p>此外，关于CUDA版本，当服务器多用户时，不同用户间使用的CUDA版本是可以互不影响的。</p><h3 id="cuda的安装">CUDA的安装</h3><p>当服务器上的没有自己所需要的CUDA版本，需要自行安装时。可以按照以下步骤:</p><ol type="1"><li><p>首先查看显卡驱动能否支持相关CUDA版本的安装.</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112202003373.png" alt="image-20211220200357316" /></p><p>前往<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#title-resolved-issues">链接</a>查看相关信息:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112202006223.png" alt="image-20211220200626157" /></p><p>举个例子，你想装CUDA10.2,那么显卡驱动需要大于等于440.33。如果显卡驱动小于这个版本，则不能安装，可能需要更新显卡驱动。</p></li><li><p>在确定显卡驱动可以安装之后，就可以去网上找安装CUDA的教程了。但是，在安装过程中，<strong>会让你选择是否安装显卡驱动, 你需要选择否， 否，否，就是no</strong>，这个选择有的是在装的过程中问你，有的是需要自己打开option来自己选，这一步非常重要，如果选择错了，将会影响其他版本的正常使用。比如你装CUDA10.2, 在装的过程中选了顺带安装显卡驱动，那么可能CUDA11.5就不能正常用了，因为CUAD10.2装的驱动版本可能不能满足CUDA11.5的要求。剩下的选项然后差不多就是一路回车了。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 知识分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技巧 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attentional PointNet for 3D-Object Detection in Point Clouds论文解读</title>
      <link href="/2021/12/15/new/"/>
      <url>/2021/12/15/new/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112152134754.png" alt="image-20211215213449664" /><br /><a href="https://github.com/anshulpaigwar/Attentional-PointNet">代码链接</a></p><p><a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.html">paper链接</a></p><h2 id="论文总结">论文总结</h2><p>这是2019年CVPR的一篇文章，本文提出了一种新的利用循环神经网络来做三维目标检测的方法。并且使用了类似于BERT中讲图像分割成patch的方法来处理大型的点云场景。本文的实验效果一般，速度在kitti排行榜上也很一般，不知道为什么能发CVPR. 下面简单介绍一下本文的主要思想。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112152139849.png" alt="image-20211215213956713" /></p><p>作者首先讲点云场景划分为<span class="math inline">\(12 \times 12\)</span>大小的patch，并将其在<span class="math inline">\(z\)</span>轴上投影得到一个深度图。使用PointNet来处理点云，使用卷积神经网络来处理深度图，然后讲这两个特征进行相加，这样的话就可以得到关于这个patch的上下文信息。然后就来到了循环定位网络（Recurrent Localization Net）。这个网络的输入包含两个部分，一个是Context Vector，上一个循环神经网络的隐含向量的输出。每个GRU一个分支是输出目标的置信度，另一个一个分支是输出一个目标可能所处的位置以及方向。然后在对应的patch中，仅从坐标变化以及重采样。在经过变化得到的感兴趣区域上计算目标边界框的大小以及位置。</p><p>关于GRU迭代的次数，作者在文中是这么解释的，根据kitti数据集上的统计，将kitti点云场景中划分为<span class="math inline">\(12 \times 12\)</span>大小的patch时，每个patch中最多含有三个目标，因此，GRU迭代的次数就设置为3.每次迭代计算一个目标边界框。当patch中包含的目标数小于3时，网络会倾向于预测一个位于patch之外的目标边界框。(这句话可以理解为: 当patch中不足3个目标是，会使用一个位于patch之外的边界框来和网络预测的三维边界框来绑定预测)。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112152152947.png" alt="image-20211215215228895" /></p><p>此外，作者还提出了一个更加轻量的模型，就是省略一个坐标变化，直接使用GRU的输出来做预测。</p><p>流程讲完之后来看一下作者的实验效果.</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112152150827.png" alt="image-20211215215053764" /></p><p>上表中的实验结果可以看出，这个结果并不怎么样，而且作者给出的结果还是在kitti验证集上的结果。拿自己验证集上的结果于别人测试集比较都比不过。。。。。。。</p><p>相比于其他文章，可能作者在速度和精度上达到了一个平衡吧。在时使用Lidar的方法中，速度比本文快的可能精度比不上作者，精度比本文高度速度又慢。但是本文的精度时建立在验证集的基础上，如果放在测试集撒谎给你的话，应该是比不过Complex-YOLO的。</p>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MLCVNet解读</title>
      <link href="/2021/12/14/MLCVNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>/2021/12/14/MLCVNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112141017992.png" alt="image-20211214101721902" /></p><p><a href="https://github.com/NUAAXQ/MLCVNet">代码链接</a></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_MLCVNet_Multi-Level_Context_VoteNet_for_3D_Object_Detection_CVPR_2020_paper.html">paper链接</a></p><h2 id="论文总结">论文总结</h2><p>本文首先提出了三维目标检测中对于一些含有点云数量很少的目标，人类都难以辨别，此外，大部分的网络都是单独的考虑每一个proposal，这也大大加大了网络来辨别的难度。此外，作者通过观察发现，如果能够结合上下文信息的话，我们可能能够更加简单的辨别出所需要的目标。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112141021403.png" alt="上下文信息的重要性" /></p><p>上图是作者用来展示上下文信息的重要性的。如果单独拿一个目标出来，人类肉眼都难以辨别这个是什么类型的目标，但是如果知道这是一个餐厅的话，有60%的概率能够辨别出来是椅子，如果能够知道这个目标周围有社么的话，有85%的把握知道这是一个椅子，如果既知道是餐厅，又知道是厨房的话，就有90%的把握能够预测出是椅子。</p><p>因此，针对上面的现象，作者提出了多个层级的上下文信息提取模块。首先是patch2patch的上下文信息。patch应该指的是原始的点云场景中的一个局部区域。作者文中的解释是，通过相似的patch之间的互补来弥补一些目标点很少的问题。此外，由于votenet中仅单独的考虑每一个proposal，这没有充分的利用proposl中的上下文信息。因此作者还提出了一个objec2object的提取上下文信息的模块。此外，全局的上下文信息也能够再一定程度上为目标检测提供潜在的信息，作者还在网络中加入了全局的特征。</p><p>其具体流程如下:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112141032897.png" alt="网络流程图" /></p><p>从上图可以看出，相比于传统的votenet网络，作者在voting之前加入了patch2patch的上下文信息提取网络，此外，作者在voting之后，也在各个proposal2proposal的上下文特征提取网络。此外，还增加了一个提取全局特征的分支。</p><p>其中第一层级的上下文信息patch的基本原理如下，首先使用PointNet++提取全局的特征。经过PointNet++提取特征后输出的点的特征表示的是该点所处的局部区域的所有点的特征。我们将这个局部区域称为点云空间的一个patch。然后作者针对这些patch使用self-attention结构来提取特征，使得每一个patch的特征都能受到场景中其他patch的影响，从而减轻因点云数据缺失带来的影响。</p><p>然后对于目标层级的上下文信息的提取，作者基本也是采取的同样的处理方式。</p><p>全局上下文信息的提取作者使用了未经过self-attention的Patch特征和Clusters的特征，将这些特征最大池化之后生成一个一维的向量，然后拼接起来经过一个MLP就生成了全局特征。将生成的全局特征和Object2Object Context模块的数据拼接用于三维边界框的计算。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112141044354.png" alt="image-20211214104459299" /></p><p>从上述表格中可以看出，相比于VoteNet，引入上下文特征之后大大提高了模型的检测性能。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112141045744.png" alt="image-20211214104533696" /></p><p>上述表格展示了不同的子模块对于模型性能的影响。</p>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Context information, self-Attenion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BANet论文解读</title>
      <link href="/2021/12/11/BANet(Boundary%20aware%20Net)/"/>
      <url>/2021/12/11/BANet(Boundary%20aware%20Net)/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112101724500.png" alt="image-20211210172456421" /></p><p><a href="">代码链接</a></p><p><a href="https://arxiv.org/abs/2104.10330">paper链接</a></p><h2 id="论文总结">论文总结</h2><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112111442291.png" alt="image-20211211144247213" /></p><p>本文首先提出了一个目前二阶段目标检测网络存在的问题，就是第一阶段提出的proposal通常与真实的边界框存在一定的偏移，特别是在距离较远，包含点很少的目标。例如上图中的(a),(c),(d)中提出的proposal.由于偏移的存在，这样的话这些边界框就很难获的目标的边缘信息。此外，现存的refine的网络也没有聚合或者补偿这些边界信息的机制。此外，现存的网络在优化这些proposal的时候，通常是独立的来优化这些proposal，本文中提出了一种将这些proposal建立图神经网络来同时调整以及特征的传播。作者的解释是通过这个图神经网络，使得当前的proposal能够扩大感受野，获得与其相邻的其他proposal的特征，这样的话就能更好的获得目标的边界信息。</p><p>此外，由于BEV的表达方式存在着一些问题，比如特征模糊或者放弃了3D结构的上下文信息，因此作者提出了一个新的局部特征聚合的网络来弥补这些损失。</p><p>本文中图神经网络的机制:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112111504241.png" alt="image-20211211150436187" /></p><p>以两个proposal的中心点为衡量标准，半径r内的proposal之间存在一条边。所有的proposal构成了集合V,所有的边构成了集合E。上面算法中的K表达的是网络迭代的次数。每次迭代，遍历每个节点，将与当前节点有边相连的节点的特征全部拼接起来，输入到一个线性层中，然后再做一个通道层面的pooling操作(这个和pointnet中是一样的)，然后再将计算得到的特征经过一个线性层和非线性操作，然后再将其与上一阶段的特征相加，得到本次遍历的特征。最后这个proposal的特征采用最后一次遍历的输出。通过一次次遍历，可以不断的扩大当前节点的感受野，从而不断完善当前proposal对于目标边界的了解。</p><p>然后是本文作者提出的一个关于局部区域特征聚合的网络，</p><p>这部分感觉作者说的不是很清楚，也可能是我读得不太仔细。大概按照作者的意思翻译一下:</p><p>关于体素层面的encoding网络，为了能够后的体素层面的特征，作者设计了一个体素和点融合的模块，以充分利用基于点和基于体素的方法的优点。首先作者再稀疏卷积中保留了每个体素的位置坐标，这样的话每个体素的特征的表示就是<span class="math inline">\(\left \{(f_j,p_j:J=1,...M) \right \}\)</span>.为了能够关联多尺度的不同阶段的特征，作者根据pointnet++中的FP层将稀疏卷积的特征传递到了原始点云中。然后我们从原始的点云中插值3D稀疏卷积特征以弥补直接将3D稀疏卷积特征映射到2DBEV特征带来的缺少三维上下文信息的问题。</p><p>我的理解: 大概意思就是没有直接把三维稀疏体素的特征直接转化到2D的BEV特征下，因为这样的话将会带来三维信息的确实。于是作者将三维体素信息使用FP传播到原始点云空间，然后在利用原始点云空间来插值这个BEV的特征。【但是看pipline又不太对，迷惑了】</p><p>pixel-wise的特征部分也说的不是很明白；</p><p>point-wise部分的特征是作者通过使用一个小的pointnet，以原始点云作为输入，将整个点云场景的特征聚合到一些关键点上。</p><p>然后最后来做refine的特征就是上面描述的三部分的特征拼接起来。</p><p>实验结果:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112111548634.png" alt="image-20211211154817563" /></p><p>上表展示的是BANet在KITTI测试集上的表现。其在BEV这个指标上好像获得了排名第一的成绩，其3D的检测精度也达到了一个较好的水平。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112111550532.png" alt="image-20211211155055476" /></p><p>上述两个表格展示了聚合的不同特征以及图神经网络迭代次数对于性能的影响。从不同特征的消融实验中可以看出，基于点的特征对于实验性能的影响非常的巨大。加入了基于点的对于实验性能的影响非常巨大，但是只是用基于点的特征的网络的性能却不是很好。作者分析其原因是在直接转化BEV表示时，从3D特征逐步进行8x下采样，可能会降低定位精度且缺少3D结构上下文信息。</p><p>从图神经网络的迭代次数对于网络性能的影响可以看出，在迭代0次和迭代4次模型性能的差异，这也体现了作者所提方法的有效性。</p><hr /><h2 id="常用句式">常用句式</h2><ol type="1"><li>to fully exploit the merits of both sth. and sth. 为了充分利用什么和什么的优点</li><li>Specifically, to compensate for sth, we devise sth. 特别地，为了弥补什么，我们提出了什么</li></ol><h2 id="代码解读">代码解读</h2><p>[挖坑]作者还没有公布代码</p>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Object Detect, Graph Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>霍夫丁不等式证明学习的可行性</title>
      <link href="/2021/12/11/%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7/"/>
      <url>/2021/12/11/%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<h1 id="学习的可行性">学习的可行性</h1><p><a href="https://wulc.me/2017/02/28/机器学习基石--学习理论/">机器学习基石--学习的可行性 | 吴良超的学习笔记 (wulc.me)</a></p><h2 id="霍夫丁不等式">霍夫丁不等式</h2><p>霍夫丁不等式描述的抽样的数目与模型的误差之间的关系。假如橙色弹珠的真实比例为<span class="math inline">\(\nu\)</span>,模型估计的比率为<span class="math inline">\(\mu\)</span>,样本大小为<span class="math inline">\(N\)</span>，则对应的霍夫丁不等式为:<br /><span class="math display">\[P(|\nu - \mu| &lt; \epsilon) \le 2exp(-2 \epsilon^2N)\]</span><br /><span class="math inline">\(\epsilon\)</span>表示误差允许的范围。</p><h2 id="从霍夫丁不等式到机器学习">从霍夫丁不等式到机器学习</h2><p>考虑一个二分类问题，绿色弹珠表示样本标签与我们模型<span class="math inline">\(h\)</span>的预测值一致，橙色弹珠表示样本标签与模型<span class="math inline">\(h\)</span>的预测不一致，同时将模型<span class="math inline">\(h\)</span>在<strong><em>全部弹珠中的分类错误率</em></strong>记为<span class="math inline">\(E_{out}(h)\)</span>，在训练样本中的<strong><em>分类错误率</em></strong>记为<span class="math inline">\(E_{in}(h)\)</span>。</p><p>霍夫丁不等式描述的就是当模型<span class="math inline">\(h\)</span>在训练样本上的分类错误率足够小的话，<span class="math inline">\(E_{in}(h)\)</span>将足够接近<span class="math inline">\(E_{out}(h)\)</span>。即<br /><span class="math display">\[P(|E_{in}(h)-P_{out}(h)| &lt; \epsilon) \le 2exp(-2 \epsilon^2N)\]</span><br />其中<span class="math inline">\(N\)</span>表示样本的数量。也就是说，样本数量越多，<span class="math inline">\(E_{in}(h)\)</span>和<span class="math inline">\(E_{out}(h)\)</span>也就是训练误差和泛化误差越接近。</p><p>Hoeffeding不等式只是描述了训练误差和泛化误差可以分接近，凡是这一接近是建立在训练误差<span class="math inline">\(E_{in}(h)\)</span>很小的情况下才有意义。</p><h2 id="使用霍夫丁不等式解释学习是可行的">使用霍夫丁不等式解释学习是可行的</h2><p>对于单个hypothesis, 霍夫丁不等式告诉哦我们呢的是<span class="math inline">\(h\)</span>在抽取的样本上的误差（也就是训练误差）跟<span class="math inline">\(h\)</span>在所有数据上的误差(也就是泛化误差)很接近。</p><p>也就是说，对于抽取出的样本，还是有很小的概率使得<span class="math inline">\(h\)</span>在样本上得到的误差与总体上的误差很大。我们将这部分样本称为Bad data.就是使得<span class="math inline">\(E_{in}(h)\)</span>很小，<span class="math inline">\(E_{out}(h)\)</span>很大的样本。如果进行多次抽样，那么一定会有一些样本导致两者的差距很大。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111171953466.png" alt="image-20211117195333393" /></p><p>当有多个hypothesis时，上表可以转化为下表所示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111171955706.png" alt="image-20211117195510375" /></p><p>对于每个hypothsis有着不同的bad data.只有当样本对所有的hypothesis都不是bad data时，才不会出现泛化误差和训练误差很大的情况，在有<span class="math inline">\(M\)</span>个hypotheis的时候，用霍夫丁不等式表示选择了bad data的概率。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111171957900.png" alt="image-20211117195738595" /></p><p>上面的不等式表明，在对有限多个hypothesis而言，<span class="math inline">\(E_{in}(h) \approx E_{out}(h)\)</span>还是PAC的，只是两者误差的上限变大了，但是数据量的增大能够抵消这个影响。因此，在有多个hypothesis的情况下，只需要选择<span class="math inline">\(E_{in}(h)\)</span>小的，就能保证<span class="math inline">\(E_{out}(h)\)</span>也是小的，也就是学习是可行的。</p><p>上述过程可以使用以下图片表示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111172005227.png" alt="image-20211117200453898" /></p>]]></content>
      
      
      <categories>
          
          <category> 知识分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Point cloud transformer论文解读</title>
      <link href="/2021/12/08/PCT(Meng%E2%80%94Hao)/"/>
      <url>/2021/12/08/PCT(Meng%E2%80%94Hao)/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081119407.png" alt="1" /></p><p><a href="https://github.com/MenghaoGuo/PCT">代码链接</a></p><p><a href="https://doi.org/10.1007/s41095-021-0229-5">paper链接</a></p><h2 id="论文总结">论文总结</h2><p>本文提出了一种在适用于点云的Transformer结构。根据点云数据的特点进一步改善了Transformer的结构。其主要做了三点改进:</p><ol type="1"><li>基于坐标的输入嵌入方法</li><li>改进的offset-attention方法(想法主要来源于图神经网络)</li><li>邻近点嵌入方法</li></ol><p>下面依次介绍上述三种改进点以及改进的原因。</p><p>首先介绍原始的Transformer结构在点云中的使用。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081123918.png" alt="2" /></p><p>使用点云的Encoder结构来提取点云的特征。首先使用一个输入嵌入层来转化点云的坐标，就是将三维的点云坐标映射到一个更高维的空间。这一步的目的是使得具有相似语义信息的点云能够在高维空间中更加靠近。文中这一步是使用线性层来完成的。然后将经过转化后的坐标输入到级联的Attention网络中。然后将各个层级的Attention网络的输出拼接起来，再经过一个线性层以得到一个逐点的特征，然后使用一个Max Pooling或者Mean Pooling操作来获得一个全局的特征。这样我们就可以使用全局的特征来进行点云的分类了。如果是其他任务的话，可以把全局特征和前面逐点的特征进行拼接，在来进行下一步的任务(这个和PointNet++中的操作是一致的)。</p><p>上述过程描述的就是使用基于坐标的输入嵌入方法和原始的Transformer结构结合起来处理点云的网络。</p><p>在介绍offset-Attention方法之前，需要先介绍一下原始的self-Attenion的机制:<br /><span class="math display">\[\hat{A}=(\hat{a})_{i,j}=Q \cdot K^T \\F_{sa} = A \cdot V \\F_{out} = SA(F_{in}) = LBR(F_{sa}) + F_{in}\]</span><br />上述公式中<span class="math inline">\(A\)</span>是<span class="math inline">\(\hat{A}\)</span>的归一化后的结果，具体的归一化的方法可以参考原论文。offset-Attention的方法就是将<span class="math inline">\(LBR\)</span>中的<span class="math inline">\(F_{sa}\)</span>变化为<span class="math inline">\(F_{sa}\)</span>和<span class="math inline">\(F_{in}\)</span>的差。这样的话就可以有:<br /><span class="math display">\[F_{out}=OA(F_{in})=LRB(F_{in} - F_{sa}) + F_{in}\]</span><br /><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081239610.png" alt="image-20211208123926572" /></p><p>作者把这样改进之后的PCT记为SPCT.</p><p>此外，作者还注意到，PCT在提取点云特征的时候，只关注到了点云的全局特征，而忽略了局部特征，因此，受到PointNet++的影响，作者引入了一个局部特征聚合的机制。和PointNet++一样，进行了一个Sampling和Grouping。然后使用PointNet来提取局部点云的特征，然后将这些采样的点及其对应的聚合的特征作为Attention网络的输入.此举大大提高了模型的性能。具体可见下表:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081244793.png" alt="image-20211208124405743" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081245640.png" alt="image-20211208124541575" /></p><p>从上述表格中可以看出，SPCT和那个局部特征聚合都对模型的性能有所提升。此外，作者还做了计算资源方面的分析。资源消耗对比如下表所示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081259234.png" alt="image-20211208125922176" /></p><p>从上表可以看出，NPCT和SPCT的资源消耗相差不是很大，但是精度有一定的提升。PCT相比其余两个的参数量和计算量要多很多，但是所需的浮点运算的次数仍比PointNet++(MSG)要少很多。精度却有很大的提升。</p><h2 id="可用知识点">可用知识点</h2><ol type="1"><li>在点云中使用Transformer时，因为点云自带位置信息，因此可以考虑将位置嵌入和输入嵌入结合起来。</li><li>在点云中使用Transformer时，可以考虑使用offset-Attenion方法来替换self-Attention方法。</li><li>在点云中使用Transformer时，可以考虑从使用类似于PointNet2中的局部特征聚合的方法来聚合局部信息。</li></ol><h2 id="常用句式">常用句式</h2><ol type="1"><li>Therefore, the whole self-attention process is permutation-invariant, making it well-suited to the disordered, irregular domain presented by point clouds. 解释了self-attention适合于point cloud处理的原因。</li></ol><h2 id="代码解读">代码解读</h2><p>[挖坑]</p>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Segment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D ObJect Detection with Pointformer</title>
      <link href="/2021/12/07/Pointformer/"/>
      <url>/2021/12/07/Pointformer/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071043420.png" alt="image-20211207104343339" /></p><p><a href="https://github.com/Vladimir2506/Pointformer">代码链接</a></p><p><a href="https://arxiv.org/abs/2012.11409">paper链接</a></p><hr /><h2 id="论文总结">论文总结</h2><p>作者提出了一种在3D目标检测任务中的一种基于纯Transformer结构的骨干网络。并在室内，室外的数据集上验证了所提方法的有效性。感觉作者主要是改进了PointNet++的Set Abstraction层。其网络的总体流程如下:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071054867.png" alt="image-20211207105443791" /></p><center>图1-Pointformer结构</center><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071104267.png" alt="image-20211207110458199" /></p><center>图2-PointNet2结构</center><p>作者主要改进的是Pointformers部分.作者使用了类似PointNet2中的结构设计主要改进了Set Abstraction层。对于Pointformer中的Local Transformer部分的结构如下图所示:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071104392.png" alt="image-20211207110400332" /></p><center>图3-Pointformer结构</center><p>和Pointnet2一样，首先对输入点云进行采样和聚合。采样使用的是FPS采样方法，聚合使用的是球形聚合(即聚合距离点距离r以内的点)。这样就形成了以采样点为中心的局部区域。由于Transformer中还需要一个位置映射。因此，相比于PointNet2，Pointfromer增加了一个位置操作。然后讲聚合后的特征以及位置嵌入送入到Transformer结构中进行处理，处理之后进行一个Max Pooling，这是为了保证点云的一些顺序不变性，旋转不变性所必须的操作，这也和PointNet2中一致。此外，作者分析了FPS采样的缺陷，提出了一种坐标调整的网络。其具体做法就是利用最后一层的attention map来计算采样点的偏移，使其向目标目标中心靠(这个感觉和votenet有点像)。然后再经过一个FPN就就生成了这个阶段的特征。</p><p>此外，为了能够更好的融合全局特征，作者还设计了一个Local-Global Transformer结构。其使用的是Cross-Attention的结构。即使用Local Transformer的输出作为query，使用Global Trnansformer的输出作为key和value.这样的话我们就利用整个全局的中心点通过注意力机制来整合全局的信息，这使得两者的特征学习更加有效。</p><p>此外，作者还分析了Transformer没有在目标检测领域得到广泛应用的原因是因为其计算需要消耗大量的资源。其计算复杂度是<span class="math inline">\(n^2\)</span>的。因此，可以考虑将其替换为<a href="https://arxiv.org/abs/2004.05679">MLCVNet</a>中的Linformer结构。<strong><em>在网上看了一下这个Linformer，好像不是很靠谱的感觉。本文作者也没有公布关于该方法的版本</em></strong>。其可在保证精度损失较小的情况下，将计算复杂度下降到<span class="math inline">\(kN\)</span>.</p><p>其基本原理如下:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081419940.png" alt="image-20211208141911901" /></p><p><span class="math inline">\(E_{i}\)</span>和<span class="math inline">\(F_{i}\)</span>都是一个<span class="math inline">\(k\times N\)</span>的矩阵。<strong>关于减少计算量这部分不是很懂</strong></p><p>下面为作者所作的实验结果:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071357116.png" alt="image-20211207135749059" /></p><center>Pointfromer在户外数据集的Backbone上的表现</center><p>从上表中可以看出，无论是在KITTI数据集还是较大的nuScenes数据集上，Pointfromer均在一定程度上提高了模型的检测性能。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071400043.png" alt="image-20211207140039986" /></p><center>上图展示的是消融实验部分的结果</center><p>从上表中可以看出，作者提出的Pointfromer的各个部分均对实验性能有所提升。此外，还展示了位置嵌入对于实验性能的影响。</p><hr /><h2 id="可用知识点">可用知识点</h2><ol type="1"><li>作者在文中阐述了图神经网络和transformer的关系。其提出当有足够多的FFN网络的时候，基于图的特征提取网络能够获得一层Transformer网络提取的特征的表达能力。因此，叠加多层Transformer能够获得更强表达能力的特征。</li><li>关于FPS的缺点。FPS采样方法存在两个缺点，1) 是对于离群值很敏感，特别是在处理真实世界中中的点云数据的时候。2) FPS采样的点都是原始点云中的子集，这使得在物体被遮挡或者没有足够点的情况下，很难判断除物体原始的几何信息。</li><li>作者分析了为什么Transformer没有在目标检测领域大规模应用的原因，因为相比分类和分割任务，目标检测任务需要耕读的点云数据，这就使得基于Transformer的网络很难大规模应用。</li></ol><hr /><h2 id="tips">tips</h2><ol type="1"><li>作者还没有公布在kitti数据集和nuScenes数据集上的代码</li><li>作者还没有公布关于Linformer部分的代码</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPN网络总结</title>
      <link href="/2021/12/02/RPN%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"/>
      <url>/2021/12/02/RPN%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="常见的rpn网络">常见的RPN网络</h1><h2 id="faster-rcnn中的rpn网络">Faster-RCNN中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111281938250.png" alt="image-20211128193818170" /></p><p>在Backbone生成的特征图中，使用大小为<span class="math inline">\(3\times 3\)</span>的卷积处理特征图，针对每一个中心点生成一个256维的向量。特征图可以理解为原图的一种缩小版。 使用<span class="math inline">\(3\times3\)</span>的卷积处理特征图可以对应到原图中的一个区域。然后RPN网络的目的就是在原图中的各个区域放置anchors。然后根据提取的特征判断这些anchor是否合理以及之后对这些anchor进行调整。</p><p>针对特征图中的每一个中心点，使用<span class="math inline">\(3\times3\)</span>的卷积来提取特征图的特征，生成一个256-D的向量。这个256-D的向量可以理解为对应于原始图像中的某一个区域。然后使用一些全连接层来判断这些anchor是背景还是前景，以及这些anchor距离目标中心点的偏移。</p><p>上述步骤处理完成之后，我们就在提取的特征图上的每一个点都获得了一组anchor,及其是否是前景点还是背景点，以及其相对Ground truth的偏移。然后我们需要proposal layer网络来生成proposal了。其基本步骤如下：</p><ol type="1"><li>生成anchors. 根据前面的RPN网络计算出的偏移量和原始的anchors,生成最终anchors所在的位置</li><li>根据前面的对于anchor是前景点还是背景点的判断的输出，根据置信度排序，选取前N个作为关注的anchor.</li><li>对于超出图像边界的anchors进行处理</li><li>剔除尺寸较小的anchors</li><li>对剩余的positive anchor进行NMS</li><li>剩余的区域输出作为下个阶段的输入</li></ol><p>上述proposal生成了一系列大小不同的anchor。这些anchor都是对应原图的不同大小的区域。由于神经网络只能处理固定大小的输入，因此，在将不同大小的anchor映射回backbone生成的特征图之后，将所对应区域的不同大小的特征图划分为相同大小的网格，然后对这些网格进行pooling操作。这样就使得网络能够有固定大小的输入了。</p><p><img src="https://pic1.zhimg.com/v2-e3108dc5cdd76b871e21a4cb64001b5c_r.jpg" alt="preview" /></p><h2 id="second网络中的rpn网络">SECOND网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021017505.png" alt="Second-RPN" /></p><p>SECOND的RPN网络与Faster-RCNN中的类似，不同的是，SECOND中并没有第二阶段，这个RPN网络是用于从backbone提取的特征图中生成边界框的。</p><p>由于三维的卷积神经网络太过耗时，所以作者在使用稀疏卷积神经网络提取三维点云的特征之后，将其压缩到了俯视图的特征图上，然后再特征图的各个位置计算该区域所属的类别以及回归分支计算的值。</p><p>神经网络总是要找到一个目标去优化的，但是就目前来说，我们还不知道我们优化的目标是什么，所以我们需要找到一个基准值去优化。所以需要根据anchor找到一个基准值作为优化的目标。这里采用的是以IoU为基准来进行判断。也就是说当我的anchor与基准值的IoU大于某个阈值的时候,我就需要关注这个anchor预测的输出。小于某个阈值的时候，我们也需要关注其预测的输出，我们将其分类背景，那么我们在训练的时候也希望他是朝着背景的方向去优化的。</p><p>这个就是OpenPCDet中的TargetAssign. 将anchor与gt绑定之后，我们还需要gt的其他信息来帮助我们优化我们的目标。</p><h2 id="pointrcnn网络中的rpn网络">PointRCNN网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112020959428.png" alt="image-20211202095917332" /></p><p><strong>该方法生成的区域提议达到了很高的recall。</strong>其实原因很好理解，根据每一个前景点生成，只要能够找到目标的前景点，大概率也能计算出其对应的三维边界框。使用点云分割网络对采样的点云进行分割。然后根据分割出的前景点生成边界框。文中的意思的是针对每一个前景点都预测一个三维边界框。这样的话就会产生大量的重复的三维边界框，因此作者使用了NMS去除边界框。作者选择在俯视图上IoU阈值为0.85,来选择300个高质量的porposal来进行第二阶段的refine。选择在俯视图上的原因是二维IoU的计算要简单快速的多，此外，因为重复的很多，所以作者选择了一个很高的阈值来减少proposal的数量。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021029478.png" alt="image-20211202102931430" /></p><p>从上图中可以看出，在IoU为0.5时,其recall能达到98.21。这已经达到了相当高的水准。但是当IoU为0.7时，其下降到了82.29，这也在一定程度上反映了针对其实各个前景点生成的三维边界框的质量并不是很高。分析其原因是因为不同的前景点通常位于目标不同的位置，根据其预测回归三维边界框可能比较困难。</p><p>上述方法存在的弊端就一方面需要消耗大量的资源来计算三维边界框。该网络是一个二阶段的网络，但是作者并没有同时训练这两个网络，分析其原因是因为计算资源消耗太大了。一方面是分割网络的资源消耗，另一方面的proposal生成的消耗。</p><h2 id="votenet网络中的rpn网络">VoteNet网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021000221.png" alt="VoteNet-RPN" /></p><p>VoteNet的基本流程就是首先在原始场景中进行采样，提取特征。然后根据提取的特征对目标的中心点进行投票。这里投票的意思就是根据原始点云的位置和特征，估算目标中心点的位置，计算该点云到中心点的偏移。原始点云加上这个偏移就可以向目标中心点靠拢。然后在根据投票结果做一个聚类。文中描述的聚类的话<strong>是在Votes中进行FPS采样</strong>，采样<span class="math inline">\(K\)</span>个点，然后将这<span class="math inline">\(K\)</span>个点周围的其他点聚合到一个集合中就形成了一个聚类。因为投票之后目标的点靠的更近了。聚类后的结果即为网络的proposal。然后根据这些proposal生成三维边界框。</p><p>VoteNet网络的RPN感觉比PointRCNN的要好一些，同样是根据点来生成proposal。VoteNet中借用了投票和聚类的方法来生成proposal。此外，再该方法中，不仅利用了前景点云，还在一定程度上利用了周围的背景点云。可能还在一定程度上减少了计算开销。</p><h2 id="dssd网络中的rpn网络">3DSSD网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021050611.png" alt="3DSSD-RPN" /></p><p>3DSSD中的RPN网络部分和VoteNet中的基本一致。不同的是3DSSD中引入了一种混合采样的方法。传统的FPS采样是在欧几里得空间进行的。作者在文中分析了这个方法的弊端。因为点云场景中大部分的点云都是背景点云。FPS采样虽然可以让采样的点云均匀的近似均匀的分布在整个点云空间，但是其采样到的点云大部分都是背景点云。这不利于我们的特征提取。因此作者引入了一种混合采样的方式，就是既在欧几里得空间应用FPS采样，又在语义特征空间应用FPS采样。然后将两种采样的距离结合起来。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021046903.png" alt="image-20211202104607856" /></p><p>上表中，D-FPS表示的是欧几里得空间的FPS,F-FPS表示的是语义特征空间的FPS。从上表中可以看出，引入F-FPS确实能在一定程度上提高算法采样到前景点的比例。</p><p>此外，3DSSD采用的是anchor-free的方法生成三维框，其根据每一个候选人点生成一个三维框，</p><p>一个疑问<strong>高维空间中的距离度量将会失效，那再高维的语义特征空间中，FPS为什么还会有效呢？？？,难道是这里的语义特征的维度很小，这里需要到代码中求证</strong></p><p><a href="https://blog.csdn.net/t949500898/article/details/107433419">高维空间欧氏距离与余弦相似度失效</a></p>]]></content>
      
      
      <categories>
          
          <category> 知识总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CT3D论文</title>
      <link href="/2021/11/29/CT3D%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
      <url>/2021/11/29/CT3D%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292051455.png" alt="image-20211129205132392" /></p><p><a href="https://github.com/hlsheng1/CT3D">代码链接</a></p><p><a href="https://arxiv.org/abs/2108.10723">paper链接</a></p><h2 id="论文总结">论文总结</h2><p>本文提出了一种目前二阶段的目标检测算法不能很好的提取proposal中的特征。本文提出了一种基于通道层面的self-attention结构来提高网络对于proposal中点的特征的提取能力。</p><p>下面简单介绍一下网络的处理流程:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292056595.png" alt="image-20211129205620537" /></p><p>与传统的二阶段目标检测器一样，首先使用一个backbone提取点样场景的特征，然后使用RPN网络生成proposal。注意，这里生成的proposal一个三维的边界框。文中为了能够更好的提取其周围点云的特征，将这个三维边界框转化成了一个不限制高度的圆柱体。圆柱体的直径是底边对角线长度的一定倍数。然后在生成的圆柱体中采样256个点，根据这些点来提取proposal的特征。文中使用了各个点点对原始的三维边界框(就是proposal提出的)的八个角点来计算相对位置作为点的特征的一部分。proposal中的原始特征可以表示为<span class="math inline">\([\bigtriangleup p_i^c, \bigtriangleup p_i^1, ..., \bigtriangleup p_i^8, f_i^r]\)</span>,其中<span class="math inline">\(\bigtriangleup p_i^c\)</span>表示的相对三维边界框中心点的相对位置，<span class="math inline">\(\bigtriangleup p_i^1\)</span>表示的相对边界框角点的位置。 <span class="math inline">\(f_i^r\)</span>表示的是点的原始特征。</p><p>然后作者使用了一个self-attention网络来提取proposal中的点的特征。</p><p>但是在decoder部分，作者并没有使用transformer中的标准的decoder。不使用标准的decoder主要有以下两点原因:</p><ol type="1"><li>如果使用transformer中的decoder网络，需要<span class="math inline">\(M\)</span>个quert嵌入,这将消耗大量的内存</li><li>另一方面，标准的transformer中的decoder网络是需要生成<span class="math inline">\(M\)</span>个单词或者句子，而我们只需要一个proposal的特征的表示。</li></ol><p>此外，作者还分析了使用一个decoder来处理上述问题时还存在一个原始的transformer中的decoder仅关注了全局的信息，而忽略了各个通道的信息，作者的想法是在3D检测中，各个通道都是非常重要的，不同的通道之间通常意味着不同的几何关系。因此我们要重视不同通道的。所以设计了一个通道层面的re-weigthing网络。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292126841.png" alt="image-20211129212652791" /></p><p>a)是原始的transformer中的decoder计算权重的方式。b)是作者提出的在通道层面赋予每个点不同权重的方式。其中a)主要关注的是全局信息，b)主要关注的是局部信息，因此c)将两者结合了起来。</p><p>然后将生成的特征用于计算置信度和进一步调整三维边界框。</p><p>实验结果:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292132776.png" alt="image-20211129213223726" /></p><p>从表中可以看出，作者的网络在KITTI的test数据集上还是取得了不错的效果。超越了PV-RCNN和Voxel-RCNN。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292137720.png" alt="image-20211129213725664" /></p><p>该表展示了作者设计的第二阶段网络对于模型性能提升的影响。从表中可以看出，作者设计的二阶段的网络对模型的性能的提升非常大。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292137177.png" alt="image-20211129213743112" /></p><p>第二三行展示了作者提出的proposal-to-point的效果的对比，从表中结果可以看出，这部分其实对于网络性能的影响非常小。仅0.17%左右。第1行和第3行展示了self-attention中的encoder的作用，该方法提升了2.2%，是一个非常大的提升。此外，作者设计的channel-wise的decoder也在一定程度上提高了模型的精度。</p><h2 id="可用知识点">可用知识点</h2><p>可以使用self-attention中的decoder来提取点云的特征。从文中的结果看出，该部分网络对于模型性能的提升是巨大的。此外，在提取proposal的时候，可以在一定程度上扩大一点proposal的范围。还有就是关于文中的这个decoder结构，我们在使用一些经典的模型的时候，可能需要根据使用环境多一些思考。</p><h2 id="常用句式">常用句式</h2><p>[挖坑]</p><h2 id="代码解读">代码解读</h2>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPVNet解读</title>
      <link href="/2021/11/27/RPVNet/"/>
      <url>/2021/11/27/RPVNet/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161253759.png" alt="image-20211116125355498" /></p><p><a href="">代码链接</a></p><p><a href="https://arxiv.org/abs/2103.12978">RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation (arxiv.org)</a></p><h2 id="论文总结">论文总结</h2><p>本文实现了一种大场景下的点云语义分割网络，并在SemanticKITTI数据集上实现了1st的结果。其主要通过一种融合的方法来实现。不同于以往的使用基于点，基于体素和基于深度图(Range Image)的方法.但是这些方法都有一定的缺点。之前融合的方法也仅是融合其中的两类。本文提出了一种融合三类数据的网络。网络的主要结构如下:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161300855.png" alt="image-20211116130031567" /></p><p>网络主要分为三个分支，使用了经典的Encoder和Decoder的结构。三个分支分别是基于体素的，基于点云的和基于深度图的。分别使用对应的网络提取各个分支的特征，然后作者使用了点这个分支来作为中间节点，来融合不同来源的数据。不同特征进行融合的算法如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161329277.png" alt="image-20211116132950759" /></p><p>将其他特征映射到基于点的表示的话，可以使用最近邻插值或者三线性插值或者双线性插值的方法。以基于深度图到基于点的表示为例:通过计算该点在深度图上的周围的四个</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161348473.png" alt="image-20211116134822778" style="zoom:80%;" /></p><p>其中，<span class="math inline">\(F_R（u）\)</span>表示下标为<span class="math inline">\(u\)</span>的深度图的特征。<span class="math inline">\(\phi(u,j)\)</span>表示的是对应的线性权重的计算方法。<span class="math inline">\(\delta(j)\)</span>表示的是<span class="math inline">\(j\)</span>周围的四个邻居。</p><p>在讲其他表示的特征投影到点上之后，作者提出了一个GFM(Gated Fusion Module)来对这些点表示的特征进行融合。GFM的结构如图2中的蓝色部分所示，然后就得到了一个基于点的融合后的特征，然后讲融合后的特征在转换到其他表示的形式下。</p><p>从点到其他两种表达形式的话，作者使用的是平均的方法。以体素为例子，将属于一个体素的所有点的特征进行平均，然后将平均后的特征作为这个体素的特征。具体做法图下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427140.png" alt="image-20211116142742978" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427766.png" alt="image-20211116142730453" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161423649.png" alt="image-20211116142351294" style="zoom:80%;" /></p><p>消融实验部分</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161441821.png" alt="image-20211116144154823" /></p><p>Table4 表示了不同的视角对于实验结果的影响。R表示深度图，P表示点，V表示体素以及不同分辨率的体素。</p><p>Table5 展示了不同融合方法的性能差异。其中Addition表示将特征进行相加，Concatnation表示进行拼接，Gated Fusion表示使用了作者设计的网络来进行融合。</p><p>Table6 展示了集成模型与本文模型的性能差异</p><p>Table7 展示了不同分支对于模型性能的影响。</p><p><span class="math display">\[d = \sqrt{a^2 + b^2}\]</span><br /><span class="math inline">\(d\)</span>是欧式距离</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">s = <span class="string">&quot;hello, world&quot;</span>;</span><br><span class="line">cout&lt;&lt;s&lt;&lt;endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 点云分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯学习</title>
      <link href="/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="贝叶斯理论">贝叶斯理论</h2><h3 id="计算后验概率">计算后验概率</h3><p><span class="math display">\[P(h|D)=\frac{P(D|h)p(h)}{p(D)}\]</span></p><p>其中，<span class="math inline">\(P(D|h)\)</span>是根据数据观察到的条件概率，<span class="math inline">\(p(h)\)</span>是先验概率。我们要做的就是在假设空间中找到具有最大后验概率的假设。那么就有:</p><p><span class="math display">\[h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\\=arg max_{h \in H}P(D|h)p(h)\]</span></p><p>如果每个假设出现的概率相等，等上述问题退化为<span class="math inline">\(h_{ML}=argmax_{h \in H}P(D|h)\)</span>,这个就是极大似然假设。上面那个是极大后验假设。</p><p>使用极大似然假设进行癌症诊断的例子：</p><ol type="1"><li><p>两个假设：1) 病人有癌症，2) 病人没有癌症</p></li><li><p>两种可能的测试输出: 1) positive,记为+ 2) negative,记为-</p></li><li><p>先验知识：P(cancer) = 0.008 P(not cancer)=0.992 P(+|cancer)=0.98 P(- |cancer)=0.02</p><p>P(- |not cancer) = 0.97 P(+|not cancer) = 0.03</p></li><li><p>分别计算病人检测结果为阳性，其患癌症和不患癌症哪个概率大<br /><span class="math display">\[P(cancer|+)=\frac{P(+|cancer)*P(cancer)}{P(+)} \\P(not cancer | +) = \frac{P(+ | not cancer)*P(not cancer)}{P(+)}\]</span><br />从上述式子可以看出，只用比较两个式子的分子部分就可以对比谁的概率大。</p></li></ol><h2 id="极大似然假设和极大后验概率假设的区别">极大似然假设和极大后验概率假设的区别</h2><ol type="1"><li><p>极大后验概率假设：<br /><span class="math display">\[h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\\=arg max_{h \in H}P(D|h)p(h)\]</span></p></li><li><p>极大似然假设</p><p>当假设空间中每个假设出现的概率都相同时，我们就可以不再考虑极大后验概率中的<span class="math inline">\(P(h)\)</span>，仅考虑数据<span class="math inline">\(D\)</span>对于假设<span class="math inline">\(h\)</span>的似然值<span class="math inline">\(P(D|h)\)</span>。则上述问题退化为:<br /><span class="math display">\[h_{ML}=argmax_{h\in H}P(D|h)\]</span></p></li></ol><h2 id="一般概率准则">一般概率准则</h2><ol type="1"><li><p>求和准则： 如果两个事件不是互相独立的，则A或B发生的概率可以使用一下方法进行计算:</p><p><span class="math inline">\(P(A \or B) = P(A) + P(B) - P(A \and B)\)</span></p></li><li><p>相乘准则：</p><ol type="1"><li><p>如果两个事件不是独立的，则：<span class="math inline">\(P(A \and B)=P(A | B)*P(B) =P(B|A)*P(A)\)</span></p></li><li><p>如果两个事件是独立的，则: <span class="math inline">\(P(A \and B)=P(A)*P(B)\)</span></p></li><li><p>当且仅当:<br /><span class="math display">\[P(A|B)=P(A) or \\P(B|A) = P(B) or \\P(AB)=P(A)P(B)\]</span><br />我们可以说事件A和事件B是独立的</p></li><li><p>全概率准则:</p><p>如果<span class="math inline">\(A_1...A_n\)</span>是互斥的，独立的事件，即<span class="math inline">\(P(B) = \sum_{1}^{n}P(B|A_i)P(A_i)\)</span></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211006281.png" alt="image-20211121100603229" /></p><p>上表中，第三列是各个年龄段患病的概率，最后一列是各个年龄段所占的比例。则求整个疾病的患病概率<span class="math inline">\(P(D)\)</span>。</p></li></ol></li></ol><h2 id="最小描述长度">最小描述长度</h2><p>最小描述长度(Minimum Description Length Principle MDL)</p><ol type="1"><li><p>基本思想：</p><ol type="1"><li>对于一个给定的假设集合和数据集，我们需要在假设集中找到或者合并一些使得数据被压缩的最小的假设。</li></ol></li><li><p>最小描述长度原理:</p><p><span class="math inline">\(h_{MDL}=arg \min_{h \in H} L_{C_1}(h)+L_{C_{2}}(D|h)\)</span>,其中<span class="math inline">\(L_{c_1}(h)\)</span>表示的是模型的复杂度，其具体表示描述整个模型所需的最小比特, <span class="math inline">\(L_{C_2}(D|h)\)</span>表示的是描述使用假设编码后的最小长度。在这里可以代表模型的错误率。所以最小描述长度就是在模型复杂度和模型性能之间做一个权衡。</p></li></ol><h2 id="贝叶斯最优分类器">贝叶斯最优分类器</h2><ol type="1"><li><p>核心思想：</p><p><a href="https://blog.csdn.net/lsjseu/article/details/12285407">贝叶斯最优分类器</a></p><p><strong>一般来说，新实例的最可能的分类可以通过合并所有假设的预测得到。用后验概率来加权</strong>。如果新样例可能分类可取集合<span class="math inline">\(V\)</span>的任一值<span class="math inline">\(v_j\)</span>，那么概率<span class="math inline">\(P(v_j|D)\)</span>表示新实例<span class="math inline">\(v_j\)</span>被正确分类的概率，其值为:<br /><span class="math display">\[P(v_i|D)=\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)\]</span><br />新实例的最优分类为使<span class="math inline">\(P(v_i|D)\)</span>最大的<span class="math inline">\(v_i\)</span>的值。因此贝叶斯最优分类器有：<br /><span class="math display">\[arg\max_{v_i \in V}\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)\]</span></p></li><li><p>贝叶斯最优分类器:</p><ol type="1"><li>该方法最大化了新实例被分类正确的可能性</li><li>在使用相同的假设空间和先验知识的条件下，没有其他的模型能够超过该方法</li><li>该方法所作的决策不能对应到假设空间中某个具体的假设上</li></ol></li><li><p>贝叶斯最优分类器存在的问题：需要计算所有可能的模型和假设，当假设空间很大时，这个计算是耗时的。因此可以使用Gibbs算法来解决</p></li></ol><h2 id="吉布斯算法">吉布斯算法</h2><p>为了解决贝叶斯最优分类器难以计算的问题，提出了吉布斯算法。吉布斯算法的基本思想就是:</p><p>根据后验概率在假设空间的分布，随机的从假设空间选取一个假设来对新实例进行分类，在具体的条件下，该方法的分类错误率小于两倍的贝叶斯最优分类器的错误率。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211323660.png" alt="image-20211121132357605" /></p><p>但是，通常从假设空间采样是困难的，因为：</p><ol type="1"><li><span class="math inline">\(P(h|D)\)</span>难以计算</li><li>对于没有参数的分类器例如SVM，<span class="math inline">\(P(h|D)\)</span>不可能计算</li><li><span class="math inline">\(P(h|D)\)</span>在假设空间很大时非常的小。</li></ol><p>为了解决上述问题，提出了Bagging 分类器算法。通过将从假设空间采样转化为从训练数据中进行采样。</p><h2 id="bagging-分类器算法">Bagging 分类器算法</h2><ol type="1"><li><p>Boostrap采样</p><p>对于给定的训练数据D,其中包含m个训练样本，从D中有放回的随机采样n个样本构成<span class="math inline">\(D^i\)</span>数据集。<span class="math inline">\(D^i\)</span>理论上有0.37的数据没有被采样到。</p></li><li><p>Bagging算法</p><p>构建<span class="math inline">\(k\)</span>个<span class="math inline">\(D^i\)</span>，对灭一个<span class="math inline">\(D^i\)</span>分别构建一个分类器<span class="math inline">\(h^i\)</span>,在对新实例分类时，所有的分类器以相同的权重进行投票:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211337106.png" alt="image-20211121133725061" /></p></li></ol><h2 id="偏差方差分解"><strong>偏差方差分解</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/38853908">参考链接</a></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212130947.png" alt="image-20211121213036863" style="zoom:50%;" /></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212141086.png" alt="image-20211121214117003" /></p><p>注意借助预测值的期望这个值来使得方差和偏差能够联系起来。此外，还要知道偏差和方差的计算方法，这样才能刚好的往这个方向去凑:<br /><span class="math display">\[var=E_{D}[g^{(D)}(x) - E_{D}[g^{(D)}(x)]] \\bias = E_{D}[y - E_{D}[g^{(D)}(x)]]  \\\sigma^2 = (y - y_D)^2\]</span><br />注意上述公式中的第二个关于偏差的计算中，式子中的<span class="math inline">\(y\)</span>不是表示数据标签，而是训练数据的真实标签，因为训练所用的标签和真实的标签可能存在一定的误差。其中<span class="math inline">\(y_D\)</span>表示的是数据在训练时的标签。</p><p>偏差方差分解的核心原理就是利用已知的方差偏差的计算公式不断的去凑。第一次用训练结果的期望去凑，第二次用训练时的标签<span class="math inline">\(y_D\)</span>去凑。</p><p>方差描述的是不同训练集训练出的模型的差距。</p><p>偏差描述的是所有可能的训练数据集训练的所有模型的输出的平均值与真实模型之间输出值之间的差距。</p><h2 id="朴素贝叶斯分类器--可能会出计算题">朴素贝叶斯分类器--可能会出计算题</h2><p>假设训练集中的一个实例可以划分为有n个属性的组合。<span class="math inline">\(&lt;a_1,a_2,...a_n&gt;\)</span>，同时，类别空间<span class="math inline">\(V\)</span>是一个有限的集合。</p><p>使用贝叶斯方法对新实例进行分类:<br /><span class="math display">\[v_{MAP}=arg \max_{v_J \in V}P(v_j|a_1,a_2,...a_n) \\v_{MAP}=arg \max_{v_J \in V]}\frac{P(a_1,a_2,..a_n|v_j)P(v_j)}{P(a_1,a_2,..a_n)}\]</span><br />如何计算<span class="math inline">\(P(v_j)\)</span>和<span class="math inline">\(P(a_1,a_2,...a_n|v_j)\)</span>：</p><ol type="1"><li><span class="math inline">\(P(v_j)\)</span>是这一类别在训练数据中出现的概率</li><li><strong><span class="math inline">\(P(a_1, a_2, ...,a_n|v_j)\)</span>不太可信，除非我们有一个非常非常大的训练样本。</strong></li></ol><p>为了解决<span class="math inline">\(P(a_1,a_2, ..a_n|v_j)\)</span>难以直接计算的问题，提出了贝叶斯假设。其核心内容就是所有的属性值都是相互独立的。所以有:<br /><span class="math display">\[P(a_1,a_2,...a_n|v_j)=\prod_{i}P(a_i|v_i)\]</span><br /><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211352341.png" alt="image-20211121135228260" /></p><p>计算的例子。</p><h2 id="贝叶斯信念网络----计算题">贝叶斯信念网络----计算题</h2><p>贝叶斯最优分类器难以计算，朴素贝叶斯要求各个属性相互独立，条件太过严格。贝叶斯信念网络提出了一种折衷的方法。</p><h3 id="条件独立">条件独立</h3><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211708912.png" alt="image-20211121170821847" /></p><p>在给定<span class="math inline">\(z\)</span>的请款下，对于任意的<span class="math inline">\(x\)</span>，都有<span class="math inline">\(P(X=x_i|Y=y_j,Z=z_k)=P(x=x_i|Z=z_k)\)</span>或者说<span class="math inline">\(P(X|Y,Z)=P(X|Z)\)</span>。我们就称<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>关于<span class="math inline">\(Z\)</span>是条件独立的。</p><p>这样的话，使用条件独立可以将朴素贝叶斯调整为:<br /><span class="math display">\[P(X,Y|Z)=P(X|Y,Z)*P(Y|Z)=P(X|Z)*P(Y|Z) \]</span><br />上述式子在另一个角度证明了朴素贝叶斯。因为朴素贝叶斯中<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>是相互独立的。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211900632.png" alt="image-20211121190026571" /></p><p>计算下列事件发生的概率:</p><ol type="1"><li><span class="math inline">\(P(W|S)\)</span></li><li><span class="math inline">\(P(S|W)\)</span></li><li><span class="math inline">\(P(S|R,W)\)</span></li></ol><p><span class="math display">\[\begin{equation}\begin{aligned}P(W|S) &amp;= \frac{P(W,S)}{P(S)} \\&amp;= \frac{P(W,S|R)*P(R) +P(W,S|\neg R)*P(\neg R)}{P(S)} \\&amp;= \frac{P(W,S,R) + P(W,S,\neg R)}{P(S)} \\&amp;= \frac{P(W,S,R)}{P(R,S)}*\frac{P(R,S)}{R(S)} + \frac{P(W,\neg R,S)}{P(\neg R, S)}*\frac{P(\neg R,S)}{P(S)} \\  &amp;= P(W|R,S)*P(R|S) + P(W | \neg R,S)*P(\neg R | S) \\&amp;= P(W|R,S)*P(R) + P(W|\neg R, S)*P(\neg R) \\\end{aligned}\end{equation}\]</span></p><p>第三行变换的依据是根据已知条件凑出来的。最后一行的变化是因为<span class="math inline">\(R\)</span>和<span class="math inline">\(S\)</span>是独立的。<br /><span class="math display">\[P(S|W) = \frac{P(W|S)*P(S)}{P(W)} \\\begin{equation}\begin{aligned}P(W) &amp;= P(W|R,S)*P(R , S) + P(W|R, \neg S) *P(R, \neg S) \\&amp;+ P(W|\neg R, S) * P(\neg R. S) + P(W|\neg R, \neg S) *P(\neg R, \neg S) \end{aligned}\end{equation} \\\\P(R,S) = P(R)*P(S)\]</span></p><p><span class="math display">\[\begin{equation}\begin{aligned}P(S|R,W) &amp;= \frac{P(R,S,W)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(R,S)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(R)*P(S)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(S)}{P(W|R)}\end{aligned}\end{equation} \\\]</span></p><p><span class="math display">\[\begin{equation}\begin{aligned}P(W|R) &amp;= \frac{P(W,R)}{P(R)} \\&amp;= \frac{P(W.R|S) * P(S) + P(W,R|\neg S)*P(\neg S)}{P(R)} \\&amp;= \frac{P(W,R,S) + P(W,R,\neg S)}{P(R)} \\&amp;= \frac{P(W, R,S)}{P(R,S)}*\frac{P(R,S)}{P(R)} + \frac{P(W, R,\neg S)}{P(R,\neg S)}*\frac{P(R,\neg S)}{P(R)} \\&amp;= P(W|R,S)*P(S|R) + P(W|R, \neg S)*P(\neg S|R) \\&amp;= P(W|R,S)*P(S) + P(W|R, \neg S)*P(\neg S)\end{aligned}\end{equation}\]</span></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212023894.png" alt="image-20211121202358797" /></p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 知识分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

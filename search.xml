<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>RPVNet解读</title>
      <link href="/2021/11/27/RPVNet/"/>
      <url>/2021/11/27/RPVNet/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161253759.png" alt="image-20211116125355498"></p><p><a href="">代码链接</a></p><p><a href="https://arxiv.org/abs/2103.12978">RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation (arxiv.org)</a></p><h2 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h2><p>本文实现了一种大场景下的点云语义分割网络，并在SemanticKITTI数据集上实现了1st的结果。其主要通过一种融合的方法来实现。不同于以往的使用基于点，基于体素和基于深度图(Range Image)的方法.但是这些方法都有一定的缺点。之前融合的方法也仅是融合其中的两类。本文提出了一种融合三类数据的网络。网络的主要结构如下:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161300855.png" alt="image-20211116130031567"></p><p>网络主要分为三个分支，使用了经典的Encoder和Decoder的结构。三个分支分别是基于体素的，基于点云的和基于深度图的。分别使用对应的网络提取各个分支的特征，然后作者使用了点这个分支来作为中间节点，来融合不同来源的数据。不同特征进行融合的算法如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161329277.png" alt="image-20211116132950759"></p><p>将其他特征映射到基于点的表示的话，可以使用最近邻插值或者三线性插值或者双线性插值的方法。以基于深度图到基于点的表示为例:通过计算该点在深度图上的周围的四个</p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161348473.png" alt="image-20211116134822778" style="zoom:80%;" /><p>其中，$F_R（u）$表示下标为$u$的深度图的特征。$\phi(u,j)$表示的是对应的线性权重的计算方法。$\delta(j)$表示的是$j$周围的四个邻居。</p><p>在讲其他表示的特征投影到点上之后，作者提出了一个GFM(Gated Fusion Module)来对这些点表示的特征进行融合。GFM的结构如图2中的蓝色部分所示，然后就得到了一个基于点的融合后的特征，然后讲融合后的特征在转换到其他表示的形式下。</p><p>从点到其他两种表达形式的话，作者使用的是平均的方法。以体素为例子，将属于一个体素的所有点的特征进行平均，然后将平均后的特征作为这个体素的特征。具体做法图下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427140.png" alt="image-20211116142742978"></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427766.png" alt="image-20211116142730453"></p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161423649.png" alt="image-20211116142351294" style="zoom:80%;" /><p>消融实验部分</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161441821.png" alt="image-20211116144154823"></p><p>Table4 表示了不同的视角对于实验结果的影响。R表示深度图，P表示点，V表示体素以及不同分辨率的体素。</p><p>Table5 展示了不同融合方法的性能差异。其中Addition表示将特征进行相加，Concatnation表示进行拼接，Gated Fusion表示使用了作者设计的网络来进行融合。</p><p>Table6 展示了集成模型与本文模型的性能差异</p><p>Table7 展示了不同分支对于模型性能的影响。</p><p>$$<br>d = \sqrt{a^2 + b^2}<br>$$<br>$d$是欧式距离</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">s = <span class="string">&quot;hello, world&quot;</span>;</span><br><span class="line">cout&lt;&lt;s&lt;&lt;endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Segment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯学习</title>
      <link href="/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="贝叶斯理论"><a href="#贝叶斯理论" class="headerlink" title="贝叶斯理论"></a>贝叶斯理论</h2><h3 id="计算后验概率"><a href="#计算后验概率" class="headerlink" title="计算后验概率"></a>计算后验概率</h3><p>$$<br>P(h|D)=\frac{P(D|h)p(h)}{p(D)}<br>$$</p><p>其中，$P(D|h)$是根据数据观察到的条件概率，$p(h)$是先验概率。我们要做的就是在假设空间中找到具有最大后验概率的假设。那么就有:</p><p>$$<br>h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\=arg max_{h \in H}P(D|h)p(h)<br>$$</p><p>如果每个假设出现的概率相等，等上述问题退化为$h_{ML}=argmax_{h \in H}P(D|h)$,这个就是极大似然假设。上面那个是极大后验假设。</p><p>使用极大似然假设进行癌症诊断的例子：</p><ol><li><p>两个假设：1) 病人有癌症，2) 病人没有癌症</p></li><li><p>两种可能的测试输出: 1) positive,记为+   2) negative,记为-</p></li><li><p>先验知识：P(cancer) = 0.008   P(not cancer)=0.992  P(+|cancer)=0.98   P(- |cancer)=0.02</p><p>P(- |not cancer) = 0.97 P(+|not cancer) = 0.03</p></li><li><p>分别计算病人检测结果为阳性，其患癌症和不患癌症哪个概率大<br>$$<br>P(cancer|+)=\frac{P(+|cancer)*P(cancer)}{P(+)} \<br>P(not cancer | +) = \frac{P(+ | not cancer)*P(not cancer)}{P(+)}<br>$$<br>从上述式子可以看出，只用比较两个式子的分子部分就可以对比谁的概率大。</p></li></ol><h2 id="极大似然假设和极大后验概率假设的区别"><a href="#极大似然假设和极大后验概率假设的区别" class="headerlink" title="极大似然假设和极大后验概率假设的区别"></a>极大似然假设和极大后验概率假设的区别</h2><ol><li><p>极大后验概率假设：<br>$$<br>h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\=arg max_{h \in H}P(D|h)p(h)<br>$$</p></li><li><p>极大似然假设</p><p>当假设空间中每个假设出现的概率都相同时，我们就可以不再考虑极大后验概率中的$P(h)$，仅考虑数据$D$对于假设$h$的似然值$P(D|h)$。则上述问题退化为:<br>$$<br>h_{ML}=argmax_{h\in H}P(D|h)<br>$$</p></li></ol><h2 id="一般概率准则"><a href="#一般概率准则" class="headerlink" title="一般概率准则"></a>一般概率准则</h2><ol><li><p>求和准则： 如果两个事件不是互相独立的，则A或B发生的概率可以使用一下方法进行计算:</p><p>$P(A \or B) = P(A) + P(B) - P(A \and B)$</p></li><li><p>相乘准则：</p><ol><li><p>如果两个事件不是独立的，则：$P(A \and B)=P(A | B)*P(B) =P(B|A)*P(A)$</p></li><li><p>如果两个事件是独立的，则: $P(A \and B)=P(A)*P(B)$</p></li><li><p>当且仅当:<br>$$<br>P(A|B)=P(A) or \<br>P(B|A) = P(B) or \<br>P(AB)=P(A)P(B)<br>$$<br>我们可以说事件A和事件B是独立的</p></li><li><p>全概率准则: </p><p>如果$A_1…A_n$是互斥的，独立的事件，即$P(B) = \sum_{1}^{n}P(B|A_i)P(A_i)$</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211006281.png" alt="image-20211121100603229"></p><p>上表中，第三列是各个年龄段患病的概率，最后一列是各个年龄段所占的比例。则求整个疾病的患病概率$P(D)$。</p></li></ol></li></ol><h2 id="最小描述长度"><a href="#最小描述长度" class="headerlink" title="最小描述长度"></a>最小描述长度</h2><p>最小描述长度(Minimum Description Length Principle MDL)</p><ol><li><p>基本思想：</p><ol><li>对于一个给定的假设集合和数据集，我们需要在假设集中找到或者合并一些使得数据被压缩的最小的假设。</li></ol></li><li><p>最小描述长度原理:</p><p>$h_{MDL}=arg \min_{h \in H} L_{C_1}(h)+L_{C_{2}}(D|h)$,其中$L_{c_1}(h)$表示的是模型的复杂度，其具体表示描述整个模型所需的最小比特, $L_{C_2}(D|h)$表示的是描述使用假设编码后的最小长度。在这里可以代表模型的错误率。所以最小描述长度就是在模型复杂度和模型性能之间做一个权衡。</p></li></ol><h2 id="贝叶斯最优分类器"><a href="#贝叶斯最优分类器" class="headerlink" title="贝叶斯最优分类器"></a>贝叶斯最优分类器</h2><ol><li><p>核心思想：</p><p><a href="https://blog.csdn.net/lsjseu/article/details/12285407">贝叶斯最优分类器</a></p><p><strong>一般来说，新实例的最可能的分类可以通过合并所有假设的预测得到。用后验概率来加权</strong>。如果新样例可能分类可取集合$V$的任一值$v_j$，那么概率$P(v_j|D)$表示新实例$v_j$被正确分类的概率，其值为:<br>$$<br>P(v_i|D)=\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)<br>$$<br>新实例的最优分类为使$P(v_i|D)$最大的$v_i$的值。因此贝叶斯最优分类器有：<br>$$<br>arg\max_{v_i \in V}\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)<br>$$</p></li><li><p>贝叶斯最优分类器:</p><ol><li>该方法最大化了新实例被分类正确的可能性</li><li>在使用相同的假设空间和先验知识的条件下，没有其他的模型能够超过该方法</li><li>该方法所作的决策不能对应到假设空间中某个具体的假设上</li></ol></li><li><p>贝叶斯最优分类器存在的问题：需要计算所有可能的模型和假设，当假设空间很大时，这个计算是耗时的。因此可以使用Gibbs算法来解决 </p></li></ol><h2 id="吉布斯算法"><a href="#吉布斯算法" class="headerlink" title="吉布斯算法"></a>吉布斯算法</h2><p>为了解决贝叶斯最优分类器难以计算的问题，提出了吉布斯算法。吉布斯算法的基本思想就是:</p><p>根据后验概率在假设空间的分布，随机的从假设空间选取一个假设来对新实例进行分类，在具体的条件下，该方法的分类错误率小于两倍的贝叶斯最优分类器的错误率。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211323660.png" alt="image-20211121132357605"></p><p>但是，通常从假设空间采样是困难的，因为：</p><ol><li>$P(h|D)$难以计算</li><li>对于没有参数的分类器例如SVM，$P(h|D)$不可能计算</li><li>$P(h|D)$在假设空间很大时非常的小。</li></ol><p>为了解决上述问题，提出了Bagging 分类器算法。通过将从假设空间采样转化为从训练数据中进行采样。</p><h2 id="Bagging-分类器算法"><a href="#Bagging-分类器算法" class="headerlink" title="Bagging 分类器算法"></a>Bagging 分类器算法</h2><ol><li><p>Boostrap采样</p><p>对于给定的训练数据D,其中包含m个训练样本，从D中有放回的随机采样n个样本构成$D^i$数据集。$D^i$理论上有0.37的数据没有被采样到。</p></li><li><p>Bagging算法</p><p>构建$k$个$D^i$，对灭一个$D^i$分别构建一个分类器$h^i$,在对新实例分类时，所有的分类器以相同的权重进行投票:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211337106.png" alt="image-20211121133725061"></p></li></ol><h2 id="偏差方差分解"><a href="#偏差方差分解" class="headerlink" title="偏差方差分解"></a><strong>偏差方差分解</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/38853908">参考链接</a></p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212130947.png" alt="image-20211121213036863" style="zoom:50%;" /><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212141086.png" alt="image-20211121214117003"></p><p>注意借助预测值的期望这个值来使得方差和偏差能够联系起来。此外，还要知道偏差和方差的计算方法，这样才能刚好的往这个方向去凑:<br>$$<br>var=E_{D}[g^{(D)}(x) - E_{D}[g^{(D)}(x)]] \<br>bias = E_{D}[y - E_{D}[g^{(D)}(x)]]  \<br>\sigma^2 = (y - y_D)^2<br>$$<br>注意上述公式中的第二个关于偏差的计算中，式子中的$y$不是表示数据标签，而是训练数据的真实标签，因为训练所用的标签和真实的标签可能存在一定的误差。其中$y_D$表示的是数据在训练时的标签。</p><p>偏差方差分解的核心原理就是利用已知的方差偏差的计算公式不断的去凑。第一次用训练结果的期望去凑，第二次用训练时的标签$y_D$去凑。</p><p>方差描述的是不同训练集训练出的模型的差距。</p><p>偏差描述的是所有可能的训练数据集训练的所有模型的输出的平均值与真实模型之间输出值之间的差距。</p><h2 id="朴素贝叶斯分类器–可能会出计算题"><a href="#朴素贝叶斯分类器–可能会出计算题" class="headerlink" title="朴素贝叶斯分类器–可能会出计算题"></a>朴素贝叶斯分类器–可能会出计算题</h2><p>假设训练集中的一个实例可以划分为有n个属性的组合。$&lt;a_1,a_2,…a_n&gt;$，同时，类别空间$V$是一个有限的集合。</p><p>使用贝叶斯方法对新实例进行分类:<br>$$<br>v_{MAP}=arg \max_{v_J \in V}P(v_j|a_1,a_2,…a_n) \<br>v_{MAP}=arg \max_{v_J \in V]}\frac{P(a_1,a_2,..a_n|v_j)P(v_j)}{P(a_1,a_2,..a_n)}<br>$$<br>如何计算$P(v_j)$和$P(a_1,a_2,…a_n|v_j)$：</p><ol><li>$P(v_j)$是这一类别在训练数据中出现的概率</li><li><strong>$P(a_1, a_2, …,a_n|v_j)$不太可信，除非我们有一个非常非常大的训练样本。</strong></li></ol><p>为了解决$P(a_1,a_2, ..a_n|v_j)$难以直接计算的问题，提出了贝叶斯假设。其核心内容就是所有的属性值都是相互独立的。所以有:<br>$$<br>P(a_1,a_2,…a_n|v_j)=\prod_{i}P(a_i|v_i)<br>$$<br><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211352341.png" alt="image-20211121135228260"></p><p>计算的例子。</p><h2 id="贝叶斯信念网络—-计算题"><a href="#贝叶斯信念网络—-计算题" class="headerlink" title="贝叶斯信念网络—-计算题"></a>贝叶斯信念网络—-计算题</h2><p>贝叶斯最优分类器难以计算，朴素贝叶斯要求各个属性相互独立，条件太过严格。贝叶斯信念网络提出了一种折衷的方法。</p><h3 id="条件独立"><a href="#条件独立" class="headerlink" title="条件独立"></a>条件独立</h3><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211708912.png" alt="image-20211121170821847"></p><p>在给定$z$的请款下，对于任意的$x$，都有$P(X=x_i|Y=y_j,Z=z_k)=P(x=x_i|Z=z_k)$或者说$P(X|Y,Z)=P(X|Z)$。我们就称$X$和$Y$关于$Z$是条件独立的。</p><p>这样的话，使用条件独立可以将朴素贝叶斯调整为:<br>$$<br>P(X,Y|Z)=P(X|Y,Z)*P(Y|Z)=P(X|Z)*P(Y|Z)<br>$$<br>上述式子在另一个角度证明了朴素贝叶斯。因为朴素贝叶斯中$X$和$Y$是相互独立的。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211900632.png" alt="image-20211121190026571"></p><p>计算下列事件发生的概率:</p><ol><li>$P(W|S)$</li><li>$P(S|W)$</li><li>$P(S|R,W)$</li></ol><p>$$<br>\begin{equation}<br>\begin{aligned}<br>P(W|S) &amp;= \frac{P(W,S)}{P(S)} \<br>&amp;= \frac{P(W,S|R)*P(R) +P(W,S|\neg R)*P(\neg R)}{P(S)} \<br>&amp;= \frac{P(W,S,R) + P(W,S,\neg R)}{P(S)} \<br>&amp;= \frac{P(W,S,R)}{P(R,S)}*\frac{P(R,S)}{R(S)} + \frac{P(W,\neg R,S)}{P(\neg R, S)}*\frac{P(\neg R,S)}{P(S)} \<br>&amp;= P(W|R,S)*P(R|S) + P(W | \neg R,S)*P(\neg R | S) \<br>&amp;= P(W|R,S)*P(R) + P(W|\neg R, S)*P(\neg R) \<br>\end{aligned}<br>\end{equation}<br>$$</p><p>第三行变换的依据是根据已知条件凑出来的。最后一行的变化是因为$R$和$S$是独立的。<br>$$<br>P(S|W) = \frac{P(W|S)*P(S)}{P(W)} \<br>\begin{equation}<br>\begin{aligned}<br>P(W) &amp;= P(W|R,S)*P(R , S) + P(W|R, \neg S) *P(R, \neg S) \<br>&amp;+ P(W|\neg R, S) * P(\neg R. S) + P(W|\neg R, \neg S) *P(\neg R, \neg S)<br>\end{aligned}<br>\end{equation} \<br>\<br>P(R,S) = P(R)*P(S)<br>$$</p><p>$$<br>\begin{equation}<br>\begin{aligned}<br>P(S|R,W) &amp;= \frac{P(R,S,W)}{P(R,W)} \<br>&amp;= \frac{P(W|R,S)*P(R,S)}{P(R,W)} \<br>&amp;= \frac{P(W|R,S)*P(R)*P(S)}{P(R,W)} \<br>&amp;= \frac{P(W|R,S)*P(S)}{P(W|R)}<br>\end{aligned}<br>\end{equation} \<br>$$</p><p>$$<br>\begin{equation}<br>\begin{aligned}<br>P(W|R) &amp;= \frac{P(W,R)}{P(R)} \<br>&amp;= \frac{P(W.R|S) * P(S) + P(W,R|\neg S)*P(\neg S)}{P(R)} \<br>&amp;= \frac{P(W,R,S) + P(W,R,\neg S)}{P(R)} \<br>&amp;= \frac{P(W, R,S)}{P(R,S)}*\frac{P(R,S)}{P(R)} + \frac{P(W, R,\neg S)}{P(R,\neg S)}*\frac{P(R,\neg S)}{P(R)} \<br>&amp;= P(W|R,S)*P(S|R) + P(W|R, \neg S)*P(\neg S|R) \<br>&amp;= P(W|R,S)*P(S) + P(W|R, \neg S)*P(\neg S)<br>\end{aligned}<br>\end{equation}<br>$$</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212023894.png" alt="image-20211121202358797"></p><p>​                                                                                                                                                                       </p>]]></content>
      
      
      <categories>
          
          <category> 知识分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/11/18/hello-world/"/>
      <url>/2021/11/18/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>

<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Point cloud transformer论文解读</title>
      <link href="/2021/12/08/PCT(Meng%E2%80%94Hao)/"/>
      <url>/2021/12/08/PCT(Meng%E2%80%94Hao)/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081119407.png" alt="1" /></p><p><a href="https://github.com/MenghaoGuo/PCT">代码链接</a></p><p><a href="https://doi.org/10.1007/s41095-021-0229-5">paper链接</a></p><h2 id="论文总结">论文总结</h2><p>本文提出了一种在适用于点云的Transformer结构。根据点云数据的特点进一步改善了Transformer的结构。其主要做了三点改进:</p><ol type="1"><li>基于坐标的输入嵌入方法</li><li>改进的offset-attention方法(想法主要来源于图神经网络)</li><li>邻近点嵌入方法</li></ol><p>下面依次介绍上述三种改进点以及改进的原因。</p><p>首先介绍原始的Transformer结构在点云中的使用。</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081123918.png" alt="2" /></p><p>使用点云的Encoder结构来提取点云的特征。首先使用一个输入嵌入层来转化点云的坐标，就是将三维的点云坐标映射到一个更高维的空间。这一步的目的是使得具有相似语义信息的点云能够在高维空间中更加靠近。文中这一步是使用线性层来完成的。然后将经过转化后的坐标输入到级联的Attention网络中。然后将各个层级的Attention网络的输出拼接起来，再经过一个线性层以得到一个逐点的特征，然后使用一个Max Pooling或者Mean Pooling操作来获得一个全局的特征。这样我们就可以使用全局的特征来进行点云的分类了。如果是其他任务的话，可以把全局特征和前面逐点的特征进行拼接，在来进行下一步的任务(这个和PointNet++中的操作是一致的)。</p><p>上述过程描述的就是使用基于坐标的输入嵌入方法和原始的Transformer结构结合起来处理点云的网络。</p><p>在介绍offset-Attention方法之前，需要先介绍一下原始的self-Attenion的机制:<br /><span class="math display">\[\hat{A}=(\hat{a})_{i,j}=Q \cdot K^T \\F_{sa} = A \cdot V \\F_{out} = SA(F_{in}) = LBR(F_{sa}) + F_{in}\]</span><br />上述公式中<span class="math inline">\(A\)</span>是<span class="math inline">\(\hat{A}\)</span>的归一化后的结果，具体的归一化的方法可以参考原论文。offset-Attention的方法就是将<span class="math inline">\(LBR\)</span>中的<span class="math inline">\(F_{sa}\)</span>变化为<span class="math inline">\(F_{sa}\)</span>和<span class="math inline">\(F_{in}\)</span>的差。这样的话就可以有:<br /><span class="math display">\[F_{out}=OA(F_{in})=LRB(F_{in} - F_{sa}) + F_{in}\]</span><br /><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081239610.png" alt="image-20211208123926572" /></p><p>作者把这样改进之后的PCT记为SPCT.</p><p>此外，作者还注意到，PCT在提取点云特征的时候，只关注到了点云的全局特征，而忽略了局部特征，因此，受到PointNet++的影响，作者引入了一个局部特征聚合的机制。和PointNet++一样，进行了一个Sampling和Grouping。然后使用PointNet来提取局部点云的特征，然后将这些采样的点及其对应的聚合的特征作为Attention网络的输入.此举大大提高了模型的性能。具体可见下表:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081244793.png" alt="image-20211208124405743" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081245640.png" alt="image-20211208124541575" /></p><p>从上述表格中可以看出，SPCT和那个局部特征聚合都对模型的性能有所提升。此外，作者还做了计算资源方面的分析。资源消耗对比如下表所示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081259234.png" alt="image-20211208125922176" /></p><p>从上表可以看出，NPCT和SPCT的资源消耗相差不是很大，但是精度有一定的提升。PCT相比其余两个的参数量和计算量要多很多，但是所需的浮点运算的次数仍比PointNet++(MSG)要少很多。精度却有很大的提升。</p><h2 id="可用知识点">可用知识点</h2><ol type="1"><li>在点云中使用Transformer时，因为点云自带位置信息，因此可以考虑将位置嵌入和输入嵌入结合起来。</li><li>在点云中使用Transformer时，可以考虑使用offset-Attenion方法来替换self-Attention方法。</li><li>在点云中使用Transformer时，可以考虑从使用类似于PointNet2中的局部特征聚合的方法来聚合局部信息。</li></ol><h2 id="常用句式">常用句式</h2><ol type="1"><li>Therefore, the whole self-attention process is permutation-invariant, making it well-suited to the disordered, irregular domain presented by point clouds. 解释了self-attention适合于point cloud处理的原因。</li></ol><h2 id="代码解读">代码解读</h2><p>[挖坑]</p>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Segment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D ObJect Detection with Pointformer</title>
      <link href="/2021/12/07/Pointformer/"/>
      <url>/2021/12/07/Pointformer/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071043420.png" alt="image-20211207104343339" /></p><p><a href="https://github.com/Vladimir2506/Pointformer">代码链接</a></p><p><a href="https://arxiv.org/abs/2012.11409">paper链接</a></p><hr /><h2 id="论文总结">论文总结</h2><p>作者提出了一种在3D目标检测任务中的一种基于纯Transformer结构的骨干网络。并在室内，室外的数据集上验证了所提方法的有效性。感觉作者主要是改进了PointNet++的Set Abstraction层。其网络的总体流程如下:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071054867.png" alt="image-20211207105443791" /></p><center>图1-Pointformer结构</center><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071104267.png" alt="image-20211207110458199" /></p><center>图2-PointNet2结构</center><p>作者主要改进的是Pointformers部分.作者使用了类似PointNet2中的结构设计主要改进了Set Abstraction层。对于Pointformer中的Local Transformer部分的结构如下图所示:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071104392.png" alt="image-20211207110400332" /></p><center>图3-Pointformer结构</center><p>和Pointnet2一样，首先对输入点云进行采样和聚合。采样使用的是FPS采样方法，聚合使用的是球形聚合(即聚合距离点距离r以内的点)。这样就形成了以采样点为中心的局部区域。由于Transformer中还需要一个位置映射。因此，相比于PointNet2，Pointfromer增加了一个位置操作。然后讲聚合后的特征以及位置嵌入送入到Transformer结构中进行处理，处理之后进行一个Max Pooling，这是为了保证点云的一些顺序不变性，旋转不变性所必须的操作，这也和PointNet2中一致。此外，作者分析了FPS采样的缺陷，提出了一种坐标调整的网络。其具体做法就是利用最后一层的attention map来计算采样点的偏移，使其向目标目标中心靠(这个感觉和votenet有点像)。然后再经过一个FPN就就生成了这个阶段的特征。</p><p>此外，为了能够更好的融合全局特征，作者还设计了一个Local-Global Transformer结构。其使用的是Cross-Attention的结构。即使用Local Transformer的输出作为query，使用Global Trnansformer的输出作为key和value.这样的话我们就利用整个全局的中心点通过注意力机制来整合全局的信息，这使得两者的特征学习更加有效。</p><p>此外，作者还分析了Transformer没有在目标检测领域得到广泛应用的原因是因为其计算需要消耗大量的资源。其计算复杂度是<span class="math inline">\(n^2\)</span>的。因此，可以考虑将其替换为<a href="https://arxiv.org/abs/2004.05679">MLCVNet</a>中的Linformer结构。<strong><em>在网上看了一下这个Linformer，好像不是很靠谱的感觉。本文作者也没有公布关于该方法的版本</em></strong>。其可在保证精度损失较小的情况下，将计算复杂度下降到<span class="math inline">\(kN\)</span>.</p><p>其基本原理如下:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112081419940.png" alt="image-20211208141911901" /></p><p><span class="math inline">\(E_{i}\)</span>和<span class="math inline">\(F_{i}\)</span>都是一个<span class="math inline">\(k\times N\)</span>的矩阵。<strong>关于减少计算量这部分不是很懂</strong></p><p>下面为作者所作的实验结果:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071357116.png" alt="image-20211207135749059" /></p><center>Pointfromer在户外数据集的Backbone上的表现</center><p>从上表中可以看出，无论是在KITTI数据集还是较大的nuScenes数据集上，Pointfromer均在一定程度上提高了模型的检测性能。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112071400043.png" alt="image-20211207140039986" /></p><center>上图展示的是消融实验部分的结果</center><p>从上表中可以看出，作者提出的Pointfromer的各个部分均对实验性能有所提升。此外，还展示了位置嵌入对于实验性能的影响。</p><hr /><h2 id="可用知识点">可用知识点</h2><ol type="1"><li>作者在文中阐述了图神经网络和transformer的关系。其提出当有足够多的FFN网络的时候，基于图的特征提取网络能够获得一层Transformer网络提取的特征的表达能力。因此，叠加多层Transformer能够获得更强表达能力的特征。</li><li>关于FPS的缺点。FPS采样方法存在两个缺点，1) 是对于离群值很敏感，特别是在处理真实世界中中的点云数据的时候。2) FPS采样的点都是原始点云中的子集，这使得在物体被遮挡或者没有足够点的情况下，很难判断除物体原始的几何信息。</li><li>作者分析了为什么Transformer没有在目标检测领域大规模应用的原因，因为相比分类和分割任务，目标检测任务需要耕读的点云数据，这就使得基于Transformer的网络很难大规模应用。</li></ol><hr /><h2 id="tips">tips</h2><ol type="1"><li>作者还没有公布在kitti数据集和nuScenes数据集上的代码</li><li>作者还没有公布关于Linformer部分的代码</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPN网络总结</title>
      <link href="/2021/12/02/RPN%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"/>
      <url>/2021/12/02/RPN%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="常见的rpn网络">常见的RPN网络</h1><h2 id="faster-rcnn中的rpn网络">Faster-RCNN中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111281938250.png" alt="image-20211128193818170" /></p><p>在Backbone生成的特征图中，使用大小为<span class="math inline">\(3\times 3\)</span>的卷积处理特征图，针对每一个中心点生成一个256维的向量。特征图可以理解为原图的一种缩小版。 使用<span class="math inline">\(3\times3\)</span>的卷积处理特征图可以对应到原图中的一个区域。然后RPN网络的目的就是在原图中的各个区域放置anchors。然后根据提取的特征判断这些anchor是否合理以及之后对这些anchor进行调整。</p><p>针对特征图中的每一个中心点，使用<span class="math inline">\(3\times3\)</span>的卷积来提取特征图的特征，生成一个256-D的向量。这个256-D的向量可以理解为对应于原始图像中的某一个区域。然后使用一些全连接层来判断这些anchor是背景还是前景，以及这些anchor距离目标中心点的偏移。</p><p>上述步骤处理完成之后，我们就在提取的特征图上的每一个点都获得了一组anchor,及其是否是前景点还是背景点，以及其相对Ground truth的偏移。然后我们需要proposal layer网络来生成proposal了。其基本步骤如下：</p><ol type="1"><li>生成anchors. 根据前面的RPN网络计算出的偏移量和原始的anchors,生成最终anchors所在的位置</li><li>根据前面的对于anchor是前景点还是背景点的判断的输出，根据置信度排序，选取前N个作为关注的anchor.</li><li>对于超出图像边界的anchors进行处理</li><li>剔除尺寸较小的anchors</li><li>对剩余的positive anchor进行NMS</li><li>剩余的区域输出作为下个阶段的输入</li></ol><p>上述proposal生成了一系列大小不同的anchor。这些anchor都是对应原图的不同大小的区域。由于神经网络只能处理固定大小的输入，因此，在将不同大小的anchor映射回backbone生成的特征图之后，将所对应区域的不同大小的特征图划分为相同大小的网格，然后对这些网格进行pooling操作。这样就使得网络能够有固定大小的输入了。</p><p><img src="https://pic1.zhimg.com/v2-e3108dc5cdd76b871e21a4cb64001b5c_r.jpg" alt="preview" /></p><h2 id="second网络中的rpn网络">SECOND网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021017505.png" alt="Second-RPN" /></p><p>SECOND的RPN网络与Faster-RCNN中的类似，不同的是，SECOND中并没有第二阶段，这个RPN网络是用于从backbone提取的特征图中生成边界框的。</p><p>由于三维的卷积神经网络太过耗时，所以作者在使用稀疏卷积神经网络提取三维点云的特征之后，将其压缩到了俯视图的特征图上，然后再特征图的各个位置计算该区域所属的类别以及回归分支计算的值。</p><p>神经网络总是要找到一个目标去优化的，但是就目前来说，我们还不知道我们优化的目标是什么，所以我们需要找到一个基准值去优化。所以需要根据anchor找到一个基准值作为优化的目标。这里采用的是以IoU为基准来进行判断。也就是说当我的anchor与基准值的IoU大于某个阈值的时候,我就需要关注这个anchor预测的输出。小于某个阈值的时候，我们也需要关注其预测的输出，我们将其分类背景，那么我们在训练的时候也希望他是朝着背景的方向去优化的。</p><p>这个就是OpenPCDet中的TargetAssign. 将anchor与gt绑定之后，我们还需要gt的其他信息来帮助我们优化我们的目标。</p><h2 id="pointrcnn网络中的rpn网络">PointRCNN网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112020959428.png" alt="image-20211202095917332" /></p><p><strong>该方法生成的区域提议达到了很高的recall。</strong>其实原因很好理解，根据每一个前景点生成，只要能够找到目标的前景点，大概率也能计算出其对应的三维边界框。使用点云分割网络对采样的点云进行分割。然后根据分割出的前景点生成边界框。文中的意思的是针对每一个前景点都预测一个三维边界框。这样的话就会产生大量的重复的三维边界框，因此作者使用了NMS去除边界框。作者选择在俯视图上IoU阈值为0.85,来选择300个高质量的porposal来进行第二阶段的refine。选择在俯视图上的原因是二维IoU的计算要简单快速的多，此外，因为重复的很多，所以作者选择了一个很高的阈值来减少proposal的数量。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021029478.png" alt="image-20211202102931430" /></p><p>从上图中可以看出，在IoU为0.5时,其recall能达到98.21。这已经达到了相当高的水准。但是当IoU为0.7时，其下降到了82.29，这也在一定程度上反映了针对其实各个前景点生成的三维边界框的质量并不是很高。分析其原因是因为不同的前景点通常位于目标不同的位置，根据其预测回归三维边界框可能比较困难。</p><p>上述方法存在的弊端就一方面需要消耗大量的资源来计算三维边界框。该网络是一个二阶段的网络，但是作者并没有同时训练这两个网络，分析其原因是因为计算资源消耗太大了。一方面是分割网络的资源消耗，另一方面的proposal生成的消耗。</p><h2 id="votenet网络中的rpn网络">VoteNet网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021000221.png" alt="VoteNet-RPN" /></p><p>VoteNet的基本流程就是首先在原始场景中进行采样，提取特征。然后根据提取的特征对目标的中心点进行投票。这里投票的意思就是根据原始点云的位置和特征，估算目标中心点的位置，计算该点云到中心点的偏移。原始点云加上这个偏移就可以向目标中心点靠拢。然后在根据投票结果做一个聚类。文中描述的聚类的话<strong>是在Votes中进行FPS采样</strong>，采样<span class="math inline">\(K\)</span>个点，然后将这<span class="math inline">\(K\)</span>个点周围的其他点聚合到一个集合中就形成了一个聚类。因为投票之后目标的点靠的更近了。聚类后的结果即为网络的proposal。然后根据这些proposal生成三维边界框。</p><p>VoteNet网络的RPN感觉比PointRCNN的要好一些，同样是根据点来生成proposal。VoteNet中借用了投票和聚类的方法来生成proposal。此外，再该方法中，不仅利用了前景点云，还在一定程度上利用了周围的背景点云。可能还在一定程度上减少了计算开销。</p><h2 id="dssd网络中的rpn网络">3DSSD网络中的RPN网络</h2><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021050611.png" alt="3DSSD-RPN" /></p><p>3DSSD中的RPN网络部分和VoteNet中的基本一致。不同的是3DSSD中引入了一种混合采样的方法。传统的FPS采样是在欧几里得空间进行的。作者在文中分析了这个方法的弊端。因为点云场景中大部分的点云都是背景点云。FPS采样虽然可以让采样的点云均匀的近似均匀的分布在整个点云空间，但是其采样到的点云大部分都是背景点云。这不利于我们的特征提取。因此作者引入了一种混合采样的方式，就是既在欧几里得空间应用FPS采样，又在语义特征空间应用FPS采样。然后将两种采样的距离结合起来。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202112021046903.png" alt="image-20211202104607856" /></p><p>上表中，D-FPS表示的是欧几里得空间的FPS,F-FPS表示的是语义特征空间的FPS。从上表中可以看出，引入F-FPS确实能在一定程度上提高算法采样到前景点的比例。</p><p>此外，3DSSD采用的是anchor-free的方法生成三维框，其根据每一个候选人点生成一个三维框，</p><p>一个疑问<strong>高维空间中的距离度量将会失效，那再高维的语义特征空间中，FPS为什么还会有效呢？？？,难道是这里的语义特征的维度很小，这里需要到代码中求证</strong></p><p><a href="https://blog.csdn.net/t949500898/article/details/107433419">高维空间欧氏距离与余弦相似度失效</a></p>]]></content>
      
      
      <categories>
          
          <category> 知识总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CT3D论文</title>
      <link href="/2021/11/29/CT3D%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
      <url>/2021/11/29/CT3D%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292051455.png" alt="image-20211129205132392" /></p><p><a href="https://github.com/hlsheng1/CT3D">代码链接</a></p><p><a href="https://arxiv.org/abs/2108.10723">paper链接</a></p><h2 id="论文总结">论文总结</h2><p>本文提出了一种目前二阶段的目标检测算法不能很好的提取proposal中的特征。本文提出了一种基于通道层面的self-attention结构来提高网络对于proposal中点的特征的提取能力。</p><p>下面简单介绍一下网络的处理流程:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292056595.png" alt="image-20211129205620537" /></p><p>与传统的二阶段目标检测器一样，首先使用一个backbone提取点样场景的特征，然后使用RPN网络生成proposal。注意，这里生成的proposal一个三维的边界框。文中为了能够更好的提取其周围点云的特征，将这个三维边界框转化成了一个不限制高度的圆柱体。圆柱体的直径是底边对角线长度的一定倍数。然后在生成的圆柱体中采样256个点，根据这些点来提取proposal的特征。文中使用了各个点点对原始的三维边界框(就是proposal提出的)的八个角点来计算相对位置作为点的特征的一部分。proposal中的原始特征可以表示为<span class="math inline">\([\bigtriangleup p_i^c, \bigtriangleup p_i^1, ..., \bigtriangleup p_i^8, f_i^r]\)</span>,其中<span class="math inline">\(\bigtriangleup p_i^c\)</span>表示的相对三维边界框中心点的相对位置，<span class="math inline">\(\bigtriangleup p_i^1\)</span>表示的相对边界框角点的位置。 <span class="math inline">\(f_i^r\)</span>表示的是点的原始特征。</p><p>然后作者使用了一个self-attention网络来提取proposal中的点的特征。</p><p>但是在decoder部分，作者并没有使用transformer中的标准的decoder。不使用标准的decoder主要有以下两点原因:</p><ol type="1"><li>如果使用transformer中的decoder网络，需要<span class="math inline">\(M\)</span>个quert嵌入,这将消耗大量的内存</li><li>另一方面，标准的transformer中的decoder网络是需要生成<span class="math inline">\(M\)</span>个单词或者句子，而我们只需要一个proposal的特征的表示。</li></ol><p>此外，作者还分析了使用一个decoder来处理上述问题时还存在一个原始的transformer中的decoder仅关注了全局的信息，而忽略了各个通道的信息，作者的想法是在3D检测中，各个通道都是非常重要的，不同的通道之间通常意味着不同的几何关系。因此我们要重视不同通道的。所以设计了一个通道层面的re-weigthing网络。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292126841.png" alt="image-20211129212652791" /></p><p>a)是原始的transformer中的decoder计算权重的方式。b)是作者提出的在通道层面赋予每个点不同权重的方式。其中a)主要关注的是全局信息，b)主要关注的是局部信息，因此c)将两者结合了起来。</p><p>然后将生成的特征用于计算置信度和进一步调整三维边界框。</p><p>实验结果:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292132776.png" alt="image-20211129213223726" /></p><p>从表中可以看出，作者的网络在KITTI的test数据集上还是取得了不错的效果。超越了PV-RCNN和Voxel-RCNN。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292137720.png" alt="image-20211129213725664" /></p><p>该表展示了作者设计的第二阶段网络对于模型性能提升的影响。从表中可以看出，作者设计的二阶段的网络对模型的性能的提升非常大。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111292137177.png" alt="image-20211129213743112" /></p><p>第二三行展示了作者提出的proposal-to-point的效果的对比，从表中结果可以看出，这部分其实对于网络性能的影响非常小。仅0.17%左右。第1行和第3行展示了self-attention中的encoder的作用，该方法提升了2.2%，是一个非常大的提升。此外，作者设计的channel-wise的decoder也在一定程度上提高了模型的精度。</p><h2 id="可用知识点">可用知识点</h2><p>可以使用self-attention中的decoder来提取点云的特征。从文中的结果看出，该部分网络对于模型性能的提升是巨大的。此外，在提取proposal的时候，可以在一定程度上扩大一点proposal的范围。还有就是关于文中的这个decoder结构，我们在使用一些经典的模型的时候，可能需要根据使用环境多一些思考。</p><h2 id="常用句式">常用句式</h2><p>[挖坑]</p><h2 id="代码解读">代码解读</h2>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPVNet解读</title>
      <link href="/2021/11/27/RPVNet/"/>
      <url>/2021/11/27/RPVNet/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161253759.png" alt="image-20211116125355498" /></p><p><a href="">代码链接</a></p><p><a href="https://arxiv.org/abs/2103.12978">RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation (arxiv.org)</a></p><h2 id="论文总结">论文总结</h2><p>本文实现了一种大场景下的点云语义分割网络，并在SemanticKITTI数据集上实现了1st的结果。其主要通过一种融合的方法来实现。不同于以往的使用基于点，基于体素和基于深度图(Range Image)的方法.但是这些方法都有一定的缺点。之前融合的方法也仅是融合其中的两类。本文提出了一种融合三类数据的网络。网络的主要结构如下:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161300855.png" alt="image-20211116130031567" /></p><p>网络主要分为三个分支，使用了经典的Encoder和Decoder的结构。三个分支分别是基于体素的，基于点云的和基于深度图的。分别使用对应的网络提取各个分支的特征，然后作者使用了点这个分支来作为中间节点，来融合不同来源的数据。不同特征进行融合的算法如下图所示:</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161329277.png" alt="image-20211116132950759" /></p><p>将其他特征映射到基于点的表示的话，可以使用最近邻插值或者三线性插值或者双线性插值的方法。以基于深度图到基于点的表示为例:通过计算该点在深度图上的周围的四个</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161348473.png" alt="image-20211116134822778" style="zoom:80%;" /></p><p>其中，<span class="math inline">\(F_R（u）\)</span>表示下标为<span class="math inline">\(u\)</span>的深度图的特征。<span class="math inline">\(\phi(u,j)\)</span>表示的是对应的线性权重的计算方法。<span class="math inline">\(\delta(j)\)</span>表示的是<span class="math inline">\(j\)</span>周围的四个邻居。</p><p>在讲其他表示的特征投影到点上之后，作者提出了一个GFM(Gated Fusion Module)来对这些点表示的特征进行融合。GFM的结构如图2中的蓝色部分所示，然后就得到了一个基于点的融合后的特征，然后讲融合后的特征在转换到其他表示的形式下。</p><p>从点到其他两种表达形式的话，作者使用的是平均的方法。以体素为例子，将属于一个体素的所有点的特征进行平均，然后将平均后的特征作为这个体素的特征。具体做法图下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427140.png" alt="image-20211116142742978" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161427766.png" alt="image-20211116142730453" /></p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161423649.png" alt="image-20211116142351294" style="zoom:80%;" /></p><p>消融实验部分</p><p><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202111161441821.png" alt="image-20211116144154823" /></p><p>Table4 表示了不同的视角对于实验结果的影响。R表示深度图，P表示点，V表示体素以及不同分辨率的体素。</p><p>Table5 展示了不同融合方法的性能差异。其中Addition表示将特征进行相加，Concatnation表示进行拼接，Gated Fusion表示使用了作者设计的网络来进行融合。</p><p>Table6 展示了集成模型与本文模型的性能差异</p><p>Table7 展示了不同分支对于模型性能的影响。</p><p><span class="math display">\[d = \sqrt{a^2 + b^2}\]</span><br /><span class="math inline">\(d\)</span>是欧式距离</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">s = <span class="string">&quot;hello, world&quot;</span>;</span><br><span class="line">cout&lt;&lt;s&lt;&lt;endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 论文分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 点云分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯学习</title>
      <link href="/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/11/27/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="贝叶斯理论">贝叶斯理论</h2><h3 id="计算后验概率">计算后验概率</h3><p><span class="math display">\[P(h|D)=\frac{P(D|h)p(h)}{p(D)}\]</span></p><p>其中，<span class="math inline">\(P(D|h)\)</span>是根据数据观察到的条件概率，<span class="math inline">\(p(h)\)</span>是先验概率。我们要做的就是在假设空间中找到具有最大后验概率的假设。那么就有:</p><p><span class="math display">\[h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\\=arg max_{h \in H}P(D|h)p(h)\]</span></p><p>如果每个假设出现的概率相等，等上述问题退化为<span class="math inline">\(h_{ML}=argmax_{h \in H}P(D|h)\)</span>,这个就是极大似然假设。上面那个是极大后验假设。</p><p>使用极大似然假设进行癌症诊断的例子：</p><ol type="1"><li><p>两个假设：1) 病人有癌症，2) 病人没有癌症</p></li><li><p>两种可能的测试输出: 1) positive,记为+ 2) negative,记为-</p></li><li><p>先验知识：P(cancer) = 0.008 P(not cancer)=0.992 P(+|cancer)=0.98 P(- |cancer)=0.02</p><p>P(- |not cancer) = 0.97 P(+|not cancer) = 0.03</p></li><li><p>分别计算病人检测结果为阳性，其患癌症和不患癌症哪个概率大<br /><span class="math display">\[P(cancer|+)=\frac{P(+|cancer)*P(cancer)}{P(+)} \\P(not cancer | +) = \frac{P(+ | not cancer)*P(not cancer)}{P(+)}\]</span><br />从上述式子可以看出，只用比较两个式子的分子部分就可以对比谁的概率大。</p></li></ol><h2 id="极大似然假设和极大后验概率假设的区别">极大似然假设和极大后验概率假设的区别</h2><ol type="1"><li><p>极大后验概率假设：<br /><span class="math display">\[h_{MAP}= arg max_{h \in H}P(h|D)=argmax_{h \in H} \frac{P(D|h)p(h)}{p(D)}\\=arg max_{h \in H}P(D|h)p(h)\]</span></p></li><li><p>极大似然假设</p><p>当假设空间中每个假设出现的概率都相同时，我们就可以不再考虑极大后验概率中的<span class="math inline">\(P(h)\)</span>，仅考虑数据<span class="math inline">\(D\)</span>对于假设<span class="math inline">\(h\)</span>的似然值<span class="math inline">\(P(D|h)\)</span>。则上述问题退化为:<br /><span class="math display">\[h_{ML}=argmax_{h\in H}P(D|h)\]</span></p></li></ol><h2 id="一般概率准则">一般概率准则</h2><ol type="1"><li><p>求和准则： 如果两个事件不是互相独立的，则A或B发生的概率可以使用一下方法进行计算:</p><p><span class="math inline">\(P(A \or B) = P(A) + P(B) - P(A \and B)\)</span></p></li><li><p>相乘准则：</p><ol type="1"><li><p>如果两个事件不是独立的，则：<span class="math inline">\(P(A \and B)=P(A | B)*P(B) =P(B|A)*P(A)\)</span></p></li><li><p>如果两个事件是独立的，则: <span class="math inline">\(P(A \and B)=P(A)*P(B)\)</span></p></li><li><p>当且仅当:<br /><span class="math display">\[P(A|B)=P(A) or \\P(B|A) = P(B) or \\P(AB)=P(A)P(B)\]</span><br />我们可以说事件A和事件B是独立的</p></li><li><p>全概率准则:</p><p>如果<span class="math inline">\(A_1...A_n\)</span>是互斥的，独立的事件，即<span class="math inline">\(P(B) = \sum_{1}^{n}P(B|A_i)P(A_i)\)</span></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211006281.png" alt="image-20211121100603229" /></p><p>上表中，第三列是各个年龄段患病的概率，最后一列是各个年龄段所占的比例。则求整个疾病的患病概率<span class="math inline">\(P(D)\)</span>。</p></li></ol></li></ol><h2 id="最小描述长度">最小描述长度</h2><p>最小描述长度(Minimum Description Length Principle MDL)</p><ol type="1"><li><p>基本思想：</p><ol type="1"><li>对于一个给定的假设集合和数据集，我们需要在假设集中找到或者合并一些使得数据被压缩的最小的假设。</li></ol></li><li><p>最小描述长度原理:</p><p><span class="math inline">\(h_{MDL}=arg \min_{h \in H} L_{C_1}(h)+L_{C_{2}}(D|h)\)</span>,其中<span class="math inline">\(L_{c_1}(h)\)</span>表示的是模型的复杂度，其具体表示描述整个模型所需的最小比特, <span class="math inline">\(L_{C_2}(D|h)\)</span>表示的是描述使用假设编码后的最小长度。在这里可以代表模型的错误率。所以最小描述长度就是在模型复杂度和模型性能之间做一个权衡。</p></li></ol><h2 id="贝叶斯最优分类器">贝叶斯最优分类器</h2><ol type="1"><li><p>核心思想：</p><p><a href="https://blog.csdn.net/lsjseu/article/details/12285407">贝叶斯最优分类器</a></p><p><strong>一般来说，新实例的最可能的分类可以通过合并所有假设的预测得到。用后验概率来加权</strong>。如果新样例可能分类可取集合<span class="math inline">\(V\)</span>的任一值<span class="math inline">\(v_j\)</span>，那么概率<span class="math inline">\(P(v_j|D)\)</span>表示新实例<span class="math inline">\(v_j\)</span>被正确分类的概率，其值为:<br /><span class="math display">\[P(v_i|D)=\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)\]</span><br />新实例的最优分类为使<span class="math inline">\(P(v_i|D)\)</span>最大的<span class="math inline">\(v_i\)</span>的值。因此贝叶斯最优分类器有：<br /><span class="math display">\[arg\max_{v_i \in V}\sum_{h_j \in H}P(v_i|h_j)P(h_j|D)\]</span></p></li><li><p>贝叶斯最优分类器:</p><ol type="1"><li>该方法最大化了新实例被分类正确的可能性</li><li>在使用相同的假设空间和先验知识的条件下，没有其他的模型能够超过该方法</li><li>该方法所作的决策不能对应到假设空间中某个具体的假设上</li></ol></li><li><p>贝叶斯最优分类器存在的问题：需要计算所有可能的模型和假设，当假设空间很大时，这个计算是耗时的。因此可以使用Gibbs算法来解决</p></li></ol><h2 id="吉布斯算法">吉布斯算法</h2><p>为了解决贝叶斯最优分类器难以计算的问题，提出了吉布斯算法。吉布斯算法的基本思想就是:</p><p>根据后验概率在假设空间的分布，随机的从假设空间选取一个假设来对新实例进行分类，在具体的条件下，该方法的分类错误率小于两倍的贝叶斯最优分类器的错误率。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211323660.png" alt="image-20211121132357605" /></p><p>但是，通常从假设空间采样是困难的，因为：</p><ol type="1"><li><span class="math inline">\(P(h|D)\)</span>难以计算</li><li>对于没有参数的分类器例如SVM，<span class="math inline">\(P(h|D)\)</span>不可能计算</li><li><span class="math inline">\(P(h|D)\)</span>在假设空间很大时非常的小。</li></ol><p>为了解决上述问题，提出了Bagging 分类器算法。通过将从假设空间采样转化为从训练数据中进行采样。</p><h2 id="bagging-分类器算法">Bagging 分类器算法</h2><ol type="1"><li><p>Boostrap采样</p><p>对于给定的训练数据D,其中包含m个训练样本，从D中有放回的随机采样n个样本构成<span class="math inline">\(D^i\)</span>数据集。<span class="math inline">\(D^i\)</span>理论上有0.37的数据没有被采样到。</p></li><li><p>Bagging算法</p><p>构建<span class="math inline">\(k\)</span>个<span class="math inline">\(D^i\)</span>，对灭一个<span class="math inline">\(D^i\)</span>分别构建一个分类器<span class="math inline">\(h^i\)</span>,在对新实例分类时，所有的分类器以相同的权重进行投票:</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211337106.png" alt="image-20211121133725061" /></p></li></ol><h2 id="偏差方差分解"><strong>偏差方差分解</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/38853908">参考链接</a></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212130947.png" alt="image-20211121213036863" style="zoom:50%;" /></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212141086.png" alt="image-20211121214117003" /></p><p>注意借助预测值的期望这个值来使得方差和偏差能够联系起来。此外，还要知道偏差和方差的计算方法，这样才能刚好的往这个方向去凑:<br /><span class="math display">\[var=E_{D}[g^{(D)}(x) - E_{D}[g^{(D)}(x)]] \\bias = E_{D}[y - E_{D}[g^{(D)}(x)]]  \\\sigma^2 = (y - y_D)^2\]</span><br />注意上述公式中的第二个关于偏差的计算中，式子中的<span class="math inline">\(y\)</span>不是表示数据标签，而是训练数据的真实标签，因为训练所用的标签和真实的标签可能存在一定的误差。其中<span class="math inline">\(y_D\)</span>表示的是数据在训练时的标签。</p><p>偏差方差分解的核心原理就是利用已知的方差偏差的计算公式不断的去凑。第一次用训练结果的期望去凑，第二次用训练时的标签<span class="math inline">\(y_D\)</span>去凑。</p><p>方差描述的是不同训练集训练出的模型的差距。</p><p>偏差描述的是所有可能的训练数据集训练的所有模型的输出的平均值与真实模型之间输出值之间的差距。</p><h2 id="朴素贝叶斯分类器--可能会出计算题">朴素贝叶斯分类器--可能会出计算题</h2><p>假设训练集中的一个实例可以划分为有n个属性的组合。<span class="math inline">\(&lt;a_1,a_2,...a_n&gt;\)</span>，同时，类别空间<span class="math inline">\(V\)</span>是一个有限的集合。</p><p>使用贝叶斯方法对新实例进行分类:<br /><span class="math display">\[v_{MAP}=arg \max_{v_J \in V}P(v_j|a_1,a_2,...a_n) \\v_{MAP}=arg \max_{v_J \in V]}\frac{P(a_1,a_2,..a_n|v_j)P(v_j)}{P(a_1,a_2,..a_n)}\]</span><br />如何计算<span class="math inline">\(P(v_j)\)</span>和<span class="math inline">\(P(a_1,a_2,...a_n|v_j)\)</span>：</p><ol type="1"><li><span class="math inline">\(P(v_j)\)</span>是这一类别在训练数据中出现的概率</li><li><strong><span class="math inline">\(P(a_1, a_2, ...,a_n|v_j)\)</span>不太可信，除非我们有一个非常非常大的训练样本。</strong></li></ol><p>为了解决<span class="math inline">\(P(a_1,a_2, ..a_n|v_j)\)</span>难以直接计算的问题，提出了贝叶斯假设。其核心内容就是所有的属性值都是相互独立的。所以有:<br /><span class="math display">\[P(a_1,a_2,...a_n|v_j)=\prod_{i}P(a_i|v_i)\]</span><br /><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211352341.png" alt="image-20211121135228260" /></p><p>计算的例子。</p><h2 id="贝叶斯信念网络----计算题">贝叶斯信念网络----计算题</h2><p>贝叶斯最优分类器难以计算，朴素贝叶斯要求各个属性相互独立，条件太过严格。贝叶斯信念网络提出了一种折衷的方法。</p><h3 id="条件独立">条件独立</h3><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211708912.png" alt="image-20211121170821847" /></p><p>在给定<span class="math inline">\(z\)</span>的请款下，对于任意的<span class="math inline">\(x\)</span>，都有<span class="math inline">\(P(X=x_i|Y=y_j,Z=z_k)=P(x=x_i|Z=z_k)\)</span>或者说<span class="math inline">\(P(X|Y,Z)=P(X|Z)\)</span>。我们就称<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>关于<span class="math inline">\(Z\)</span>是条件独立的。</p><p>这样的话，使用条件独立可以将朴素贝叶斯调整为:<br /><span class="math display">\[P(X,Y|Z)=P(X|Y,Z)*P(Y|Z)=P(X|Z)*P(Y|Z) \]</span><br />上述式子在另一个角度证明了朴素贝叶斯。因为朴素贝叶斯中<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>是相互独立的。</p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111211900632.png" alt="image-20211121190026571" /></p><p>计算下列事件发生的概率:</p><ol type="1"><li><span class="math inline">\(P(W|S)\)</span></li><li><span class="math inline">\(P(S|W)\)</span></li><li><span class="math inline">\(P(S|R,W)\)</span></li></ol><p><span class="math display">\[\begin{equation}\begin{aligned}P(W|S) &amp;= \frac{P(W,S)}{P(S)} \\&amp;= \frac{P(W,S|R)*P(R) +P(W,S|\neg R)*P(\neg R)}{P(S)} \\&amp;= \frac{P(W,S,R) + P(W,S,\neg R)}{P(S)} \\&amp;= \frac{P(W,S,R)}{P(R,S)}*\frac{P(R,S)}{R(S)} + \frac{P(W,\neg R,S)}{P(\neg R, S)}*\frac{P(\neg R,S)}{P(S)} \\  &amp;= P(W|R,S)*P(R|S) + P(W | \neg R,S)*P(\neg R | S) \\&amp;= P(W|R,S)*P(R) + P(W|\neg R, S)*P(\neg R) \\\end{aligned}\end{equation}\]</span></p><p>第三行变换的依据是根据已知条件凑出来的。最后一行的变化是因为<span class="math inline">\(R\)</span>和<span class="math inline">\(S\)</span>是独立的。<br /><span class="math display">\[P(S|W) = \frac{P(W|S)*P(S)}{P(W)} \\\begin{equation}\begin{aligned}P(W) &amp;= P(W|R,S)*P(R , S) + P(W|R, \neg S) *P(R, \neg S) \\&amp;+ P(W|\neg R, S) * P(\neg R. S) + P(W|\neg R, \neg S) *P(\neg R, \neg S) \end{aligned}\end{equation} \\\\P(R,S) = P(R)*P(S)\]</span></p><p><span class="math display">\[\begin{equation}\begin{aligned}P(S|R,W) &amp;= \frac{P(R,S,W)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(R,S)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(R)*P(S)}{P(R,W)} \\&amp;= \frac{P(W|R,S)*P(S)}{P(W|R)}\end{aligned}\end{equation} \\\]</span></p><p><span class="math display">\[\begin{equation}\begin{aligned}P(W|R) &amp;= \frac{P(W,R)}{P(R)} \\&amp;= \frac{P(W.R|S) * P(S) + P(W,R|\neg S)*P(\neg S)}{P(R)} \\&amp;= \frac{P(W,R,S) + P(W,R,\neg S)}{P(R)} \\&amp;= \frac{P(W, R,S)}{P(R,S)}*\frac{P(R,S)}{P(R)} + \frac{P(W, R,\neg S)}{P(R,\neg S)}*\frac{P(R,\neg S)}{P(R)} \\&amp;= P(W|R,S)*P(S|R) + P(W|R, \neg S)*P(\neg S|R) \\&amp;= P(W|R,S)*P(S) + P(W|R, \neg S)*P(\neg S)\end{aligned}\end{equation}\]</span></p><p><img src="https://gitee.com/twn29004/myimg/raw/master/imgs/202111212023894.png" alt="image-20211121202358797" /></p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 知识分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

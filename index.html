<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>twn29004's Blog</title><meta name="author" content="twn29004"><meta name="copyright" content="twn29004"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="twn29004&#39;s Blog">
<meta property="og:url" content="http://twn29004.top/index.html">
<meta property="og:site_name" content="twn29004&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112082242974.jpeg">
<meta property="article:author" content="twn29004">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112082242974.jpeg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://twn29004.top/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?da945d048bc901ec4ee07f4643119577";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: twn29004","link":"链接: ","source":"来源: twn29004's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'twn29004\'s Blog',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-04-24 19:41:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112082242974.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-video"></i><span> 相册</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('https://picture.zwc365.com/getbing.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">twn29004's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-video"></i><span> 相册</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">twn29004's Blog</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/twn29004" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:twn29004@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/02/27/mmvirtualpioint/" title="Multimodal Virtual Point 3D Detection论文解读">Multimodal Virtual Point 3D Detection论文解读</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-02-26T16:00:00.000Z" title="发表于 2022-02-27 00:00:00">2022-02-27</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></span></div><div class="content">
代码链接
paper链接
个人博客
问题
本文提出了一种针对二维和三维信息融合的方法以解决三维点云稀疏难以检测远处的目标的问题。
解决方法
本文的主要步骤是首先对与点云对应的二维RGB图像进行语义分割。然后讲三维点云投影到二维RGB相机坐标内。仅考虑位于前景点实体分割中的点。根据位于前景实体区域内的点云生成虚拟点。其生成虚拟点的方法是在二维前景实体分割区域中进行随机采样\(K\)个点，然后根据这\(K\)个点周围的最近的几个原始点云的深度插值出虚拟点的深度，然后讲这些虚拟点反向投影回三维空间中，同时这些虚拟点包含实体分割中的类别信息。然后对含有虚拟点的点云使用3D backbone进行处理。
上述生成虚拟点的依据可能是对于属于同一个前景目标中的点，其前景深度不会相差很大，所以可以用其周围点的深度信息来对虚拟点的深度信息进行补全。从而到达缓解点云稀疏性的目的。


解决效果

相比于baseline CenterPoint,从上述表格中可以看出，使用生成的修点确实能够在一定程度上提高模型的检测性能。此外，相比于同样采用分割结果来进行多模态融合的PointPai ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/02/26/2-17%E8%B0%83%E7%A0%94/" title="无题">无题</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-02-26T00:49:09.966Z" title="发表于 2022-02-26 08:49:09">2022-02-26</time></span></div><div class="content">论文连接未录取论文arxiv上的

现有的RGB-D的语义分割大都是分别处理深度信息和RGB信息，但是通常联合推理RGBD的颜色信息和几何信息能够取得更好的分割效果。因此，本文提出了一种针对RGBD数据语义分割的基于注意力的双监督编码器架构。本文的基础架构如下:

蓝色和灰色分别代表RGB数据和深度数据。在联合学习中，不同任务特定的编码器探索不同任务之间的相关性。这些方法都是以编码器特定的感受野固定比例执行多任务蒸馏。但是，这些任务的影响并不只固定在某一特定的比例。因此，传统的简单的聚合方式例如跳连接(skip connection)，多尺度特征融合对于语义分割这类复杂任务的作用非常有限。
因此本文设计了一种简单高效的的对称网络以高效的利用多尺度的跨模态信息。我们首先提出一种基于注意力的多模态融合模块来处理两分支解码器中的多级配对互补信息，用于融合来自RGB数据和深度数据的特征。同时为了能在编码过程中学习跨模态信息，本文提出了一种新的双分支解码结构，该结构主要用于由另一个任务引导分支监督的语义分割。这样的设计能够使得我们能够通过主分支末尾的ASPP模块合并多尺度上下文特征，其 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/14/Faster-RCNN/" title="Faster-RCNN论文解读">Faster-RCNN论文解读</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-01-13T16:00:00.000Z" title="发表于 2022-01-14 00:00:00">2022-01-14</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/">参考链接</a></span></div><div class="content">知乎-参考链接
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/13/3D_IoU_Net/" title="3D IoU-Net论文解读">3D IoU-Net论文解读</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-01-12T16:00:00.000Z" title="发表于 2022-01-13 00:00:00">2022-01-13</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></span></div><div class="content">
代码链接
paper链接
个人博客
问题 test
本文首先说明了一个问题就是NMS是一个非常重要的去除预测结果中重复的后处理过程。一些工作已经发现在NMS中使用IOU来作为排序的标准能够取得更好的效果。这里作者还用一个实验证明了上述说法的正确性。

上表中的Ground truth IoU表示的是通过计算预测的边界框与基准值之间的IoU作为NMS中的分数的标准。从上表的结果可以看出，在使用IoU来知道NMS之后，模型的精度得到了非常大的提高。此外，之前的一些工作已经做出了使用预测IoU而不是预测class score来作为NMS的评分标准。大部分方法是通过增加一个IoU分支来直接简单的预测IoU，但是这些方法存在着两个问题:

直接增加一个IoU预测分支，而没有提取一些对于IoU预测很重要的特征。
另一个问题是IoU的预测存在着不对齐的问题。


具体情况如上图所示，上述结果展示的是经过refine之后的IoU的分布和没有经过Refine的IoU的分布。在训练的时候，IoU预测分支是使用的proposal的特征和基准框之间IoU,但是在测试时候，这个预测值 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/12/YOLOF/" title="YOLOF论文解读">YOLOF论文解读</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-01-11T16:00:00.000Z" title="发表于 2022-01-12 00:00:00">2022-01-12</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></span></div><div class="content">image-20220111210952738
代码链接
paper链接
个人博客
问题
本文首先分析了FPN网络的必要性。并通过实验证明，FPN有效的原因并不是因为融合了不用尺度的特征，而是因为其采取的分治的处理方法。其实验结果如图所示:


其中MiMo(Multi-Input-Multi-Output)表示的是不同尺度的特征图作为输入，同时不同尺度的特征图对应多个输出，SiMo(Single-Input-Multi-Output)表示仅使用单个尺度的特征图作为输入，然后分别对特征图进行下采样或者上采样最后生成不同尺度的输出。其余的方法类似。从作者的实验结果可以看出，其中SiMo网络获得的性能和MiMo网络获得的性能非常相近，其差别&lt;1AP。这在一定程度上说明了FPN网络的成功并不是因为其融合了多尺度的特征，而是因为他分别处理了不同尺度的对象。同时也说明，一定尺度的特征图其实已经包含了丰富的不同尺度的上下文信息。
在了解了FPN网络成功的主要因素后，作者又分析到，虽然分治能够提高模型的检测性能，但是其在一定程度上带来了额外的计算开销，减慢了模型的检测速度。因 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/09/GSPN/" title="GSPN论文解读">GSPN论文解读</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-01-08T16:00:00.000Z" title="发表于 2022-01-09 00:00:00">2022-01-09</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></span></div><div class="content">代码链接
paper链接
个人博客
问题
目前的一些点云实体分割算法的一些工作是首先生成目标的近似几何估计，例如三维边界框，虽然这些估计很简单，但是由于大多数估计的生成缺乏对于目标几何信息的理解，导致这些生成的近似都是很满盲目的，其中要么包含很多个目标，要么仅包含部分目标，这大大影响了网络的实例分割的性能。除此之外，我们很难理解这些生成提议的网络对于三维几何信息理解的好坏。
为了能够更好的了解提出提议的网络对于三维几何信息的理解的好坏，本文作者提出了一种新的三维实体分割的方法。该方法在生成提议之前，首先重建这些目标。本文使用生成模型显示的生成自然目标的分布，并通过从目标对象的分布中采样生成提议。
该方法在三维的实体分割中比二维的更具有吸引力，原因是三维目标具有更多完成的三维几何形状，具有更好的物理尺度，这在一定程度上保证了我们能够获取三维目标的分布规律。
解决方法


GCPN网络结构

GSPN网络将目标的提议问题看作是一个目标生成问题。其中原始的目标分布符合条件概率\(P_{\theta}(x|c)\)，其中\(c\)是点云\(P\)和采样点集合\(s\)的 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/09/pytorchwithcuda/" title="Pytorch,CUDA联合编程">Pytorch,CUDA联合编程</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-01-08T16:00:00.000Z" title="发表于 2022-01-09 00:00:00">2022-01-09</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/">知识分享</a></span></div><div class="content">Pytorch和CUDA联合编程的基本步骤
参考链接-w3school
参考链接-CSDN
本文代码-github
背景
目前PyTorch已经提供了丰富的接口可以直接调用，但是仍存在一些高度自定义的操作无法使用PyToch或者Python高效的完成，因此PyTorch还提供了使用C++和CUDA编程的扩展接口。C++扩展主要有两种形式，一种是使用setuptools提前构建，也可以通过torch.utils.cpp_extension.load()在运行时构建。下面仅介绍第一种方法，第二种方法之后再学习。
基本步骤
Pytorch,CUDA,C++联合编程的一般步骤如下:

首先需要定义一个C++文件，该文件声明了CUDA文件中定义的函数，还需要进行一些检查，并最终将其调用转发给.cu文件。此外，该文件还需要声明将在Python中调用的函数，并使用pybind11绑定到python。OpenPCDet将上述步骤划分为以下几个步骤:

首先定义一个头文件，该头文件.h中包含了.cu文件中定义的函数和.cpp文件中定义的函数.
然后定义.cpp文件，其中函数的作用 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/07/RelationNetworkforObjectDetection/" title="Relation Network for Object Network解读">Relation Network for Object Network解读</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-01-06T16:00:00.000Z" title="发表于 2022-01-07 00:00:00">2022-01-07</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></span></div><div class="content">
代码链接
paper链接
个人博客
问题
本文首先提出了一个目前在目标检测领域的一个共识就是: 丰富的上下文信息和目标之间的关系能够更好的帮助网络去检测目标。但是现有的网络都没有很好的利用这个信息，或者说现有的卷积神经网络都不能很好的处理这些问题。原因是一个场景中往往有很多类别的目标，且类别的数目是不一定的，而且目标的数目也是不一定的。因此，按照之前的CNN网络很难对这些目标之间的关系进行建模。作者受到在NLP领域中非常成功的Transformer的影响，提出了一种自适应的注意力网络用于目标检测领域。作者把这个模块称为目标关系模块。作者将这个关系模块应用于两个任务，一个是\(instance-recognition\)和\(Duplicate-remova\)阶段。其中前一个阶段就是在提取proposal的特征之后，在对RoI进行调整和rescore之前。后者应用于消除重复的预测阶段，是取代了NMS，因为NMS不是一个端到端的结构。具体应用阶段如下图所示:

解决方法
Relation Module的结构

Relation Module的结构如上图所示。下面分别 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/02/PartA2/" title="PartA2论文解读">PartA2论文解读</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-01-01T16:00:00.000Z" title="发表于 2022-01-02 00:00:00">2022-01-02</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/">论文分享</a></span></div><div class="content">
代码链接
paper链接
个人博客
问题

不同于二维目标检测，三维目标检测中的基准框包含着大量的信息，其能够很自然的提供前景点的标记，甚至提供位于基准框中各个点的内部相对位置。而这些信息对于三维目标检测是非常重要的。三维目标网络的内部相对位置包含了三维基准框内点的分布，这类信息能够很轻易的从点云数据中获得，且包含大量的信息，但是之前从来没有三维边界网络使用这些信息。基于这个观察，本文作者提出了Part-Aware and Part-Aggregation网络.

内部相对位置的图示。通过这些相对位置，可以帮助网络更好的预测三维边界框。
解决方法


网络的总体结构如上图所示，本文作者设计了一个使用稀疏卷积的U-Net网络。其中这部分Encode-Decode部分的网络如下图所示:

注意，这个与Second中的还是不太一样的，其增加了一个反卷积部分的网络。作者使用这些特征做了两个方面的工作，一个分支是用来预测点的类别，即属于前景点还是属于背景点，另一个分支是用来预测前景点相对边界框的内部相对位置。
关于前景点内部相对位置的计算，其计算公式如下:

 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/01/01/view_and_reshape/" title="Pytorch中View和Reshape的区别">Pytorch中View和Reshape的区别</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2021-12-31T16:00:00.000Z" title="发表于 2022-01-01 00:00:00">2022-01-01</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></span></div><div class="content">CSDN 参考链接
</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/twn2333/my_imgs@latest//img/202112082242974.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">twn29004</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/twn29004"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/twn29004" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:twn29004@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is twn29004's Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/02/27/mmvirtualpioint/" title="Multimodal Virtual Point 3D Detection论文解读">Multimodal Virtual Point 3D Detection论文解读</a><time datetime="2022-02-26T16:00:00.000Z" title="发表于 2022-02-27 00:00:00">2022-02-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/02/26/2-17%E8%B0%83%E7%A0%94/" title="无题">无题</a><time datetime="2022-02-26T00:49:09.966Z" title="发表于 2022-02-26 08:49:09">2022-02-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/14/Faster-RCNN/" title="Faster-RCNN论文解读">Faster-RCNN论文解读</a><time datetime="2022-01-13T16:00:00.000Z" title="发表于 2022-01-14 00:00:00">2022-01-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/13/3D_IoU_Net/" title="3D IoU-Net论文解读">3D IoU-Net论文解读</a><time datetime="2022-01-12T16:00:00.000Z" title="发表于 2022-01-13 00:00:00">2022-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/01/12/YOLOF/" title="YOLOF论文解读">YOLOF论文解读</a><time datetime="2022-01-11T16:00:00.000Z" title="发表于 2022-01-12 00:00:00">2022-01-12</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/"><span class="card-category-list-name">参考链接</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/"><span class="card-category-list-name">知识分享</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"><span class="card-category-list-name">知识总结</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"><span class="card-category-list-name">论文分享</span><span class="card-category-list-count">16</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/2D-Object-Detection/" style="font-size: 1.3em; color: #99a1ac">2D Object Detection</a> <a href="/tags/2D-Object-Detection/" style="font-size: 1.1em; color: #999">2D Object-Detection</a> <a href="/tags/3D-Object-Detect-Graph-Network/" style="font-size: 1.1em; color: #999">3D Object Detect, Graph Network</a> <a href="/tags/3D-Object-Detection/" style="font-size: 1.5em; color: #99a9bf">3D Object Detection</a> <a href="/tags/3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 1.3em; color: #99a1ac">3D目标检测</a> <a href="/tags/Context-information-self-Attenion/" style="font-size: 1.1em; color: #999">Context information, self-Attenion</a> <a href="/tags/Fusion/" style="font-size: 1.1em; color: #999">Fusion</a> <a href="/tags/Instance-Segment/" style="font-size: 1.1em; color: #999">Instance Segment</a> <a href="/tags/Object-Detection/" style="font-size: 1.5em; color: #99a9bf">Object Detection</a> <a href="/tags/Object-Detection-2D-Object-Detection/" style="font-size: 1.1em; color: #999">Object Detection, 2D Object Detection</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/02/"><span class="card-archive-list-date">二月 2022</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/01/"><span class="card-archive-list-date">一月 2022</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/12/"><span class="card-archive-list-date">十二月 2021</span><span class="card-archive-list-count">11</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/11/"><span class="card-archive-list-date">十一月 2021</span><span class="card-archive-list-count">3</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">24</div></div><div class="webinfo-item"><div class="item-name">已运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2021-11-26T16:00:00.000Z"></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-04-24T11:41:34.555Z"></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By twn29004</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">保持热爱，奔赴山海</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function subtitleType () {
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      if (true) {
        var from = '出自 ' + data.from
        var sub = "保持热爱，奔赴山海".length == 0 ? new Array() : "保持热爱，奔赴山海".split(',')
        var both = sub.unshift(data.hitokoto, from)
        var typed = new Typed('#subtitle', {
          strings: sub,
          startDelay: 300,
          typeSpeed: 150,
          loop: false,
          backSpeed: 50,
        })
      } else {
        document.getElementById('subtitle').innerHTML = data.hitokoto
      }
    })
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}
</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>